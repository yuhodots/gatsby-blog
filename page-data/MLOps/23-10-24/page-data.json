{"componentChunkName":"component---src-templates-post-js","path":"/MLOps/23-10-24/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>NVIDIA Triton Inference Server 사용을 위한 기본 개념들과 샘플 코드를 기록합니다. </p>\n</blockquote>\n<p>Triton은 NVIDIA가 직접 만든 AI inference용 서버입니다. Triton 내부에서는 추론 최적화 및 GPU 활용률을 높일 수 있는 여러 기능들을 제공하고 있으며, 학습된 모델을 model repository에 저장하기만 하면 간단히 inference API를 만들 수 있습니다.</p>\n<h3 id=\"ai-inference-pipeline\" style=\"position:relative;\"><a href=\"#ai-inference-pipeline\" aria-label=\"ai inference pipeline permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AI Inference Pipeline</h3>\n<p>AI inference 파이프라인을 만들고 inference 요청을 주고 받는 과정은 일반적으로 다음과 같습니다.</p>\n<ol>\n<li>AI model 학습 후 model weights 저장 (e.g., onnx/jit model을 cloud/local storage에 저장)</li>\n<li>Inference server 실행: 필요한 모든 model weights을 inference server로 import(copy/download)하고, 각 model을 config에 따라 세팅</li>\n<li>Client에서 inference server로 요청 (gRPC request)</li>\n<li>Inference server에서 내부적으로 pre-processing / model inference / post-processing를 실시 (물론 client에서 전후처리 해도 됨)</li>\n<li>Inference server의 결과를 client로 응답 (response)</li>\n</ol>\n<p>전체 파이프라인 구축을 위해서 data storage, client server, inference server가 기본적으로 필요하며, client와 inference server를 한 번에 실행하기 위해서 docker compose와 같은 멀티 컨테이너 툴을 활용하는 것이 좋습니다.</p>\n<p>위 과정에서 inference server의 역할은 '필요한 AI model 관리', 'config에 따른 model 세팅', 'client의 요청에 대한 적절한 inference 결과 응답'정도로 생각해볼 수 있습니다. NVIDIA Triton에서는 이를 <code class=\"language-text\">Model Repository</code>,  <code class=\"language-text\">Model Configuration</code>, <code class=\"language-text\">Protocol</code> 등의 feature를 통해 제공하고 있는데, 아래에서 이 feature들을 익히고 직접 간단한 예제까지 구현해보도록 하겠습니다. </p>\n<h3 id=\"nvidia-triton\" style=\"position:relative;\"><a href=\"#nvidia-triton\" aria-label=\"nvidia triton permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>NVIDIA Triton</h3>\n<p>아래의 내용들은 아직 작성중에 있습니다.</p>\n<h5 id=\"basic-archtecture\" style=\"position:relative;\"><a href=\"#basic-archtecture\" aria-label=\"basic archtecture permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Basic Archtecture</h5>\n<ul>\n<li><a href=\"https://github.com/triton-inference-server/python_backend#usage\">https://github.com/triton-inference-server/python_backend#usage</a></li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">model-registry\n\t- first_model\n\t\t- 1\n\t\t  - model.pt\n\t\t\t- model.py\n\t\t- config.pbtxt\n\t- second_model\n\t\t- 1\n\t\t  - model.pt\n\t\t\t- model.py\n\t\t- config.pbtxt</code></pre></div>\n<h5 id=\"python-backend\" style=\"position:relative;\"><a href=\"#python-backend\" aria-label=\"python backend permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Python BackEnd</h5>\n<h5 id=\"model-repository\" style=\"position:relative;\"><a href=\"#model-repository\" aria-label=\"model repository permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model Repository</h5>\n<ul>\n<li><a href=\"https://github.com/triton-inference-server/server/blob/main/docs/README.md#model-repository\">https://github.com/triton-inference-server/server/blob/main/docs/README.md#model-repository</a></li>\n</ul>\n<h5 id=\"model-configuration\" style=\"position:relative;\"><a href=\"#model-configuration\" aria-label=\"model configuration permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model Configuration</h5>\n<ul>\n<li><a href=\"https://github.com/triton-inference-server/server/blob/main/docs/README.md#model-configuration\">https://github.com/triton-inference-server/server/blob/main/docs/README.md#model-configuration</a></li>\n</ul>\n<h5 id=\"protocol\" style=\"position:relative;\"><a href=\"#protocol\" aria-label=\"protocol permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Protocol</h5>\n<ul>\n<li><a href=\"https://github.com/triton-inference-server/server/blob/main/docs/customization_guide/inference_protocols.md\">https://github.com/triton-inference-server/server/blob/main/docs/customization_guide/inference_protocols.md</a></li>\n</ul>\n<h5 id=\"other-features\" style=\"position:relative;\"><a href=\"#other-features\" aria-label=\"other features permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Other Features</h5>\n<ul>\n<li>Model Analyzer: <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_analyzer.html\">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_analyzer.html</a></li>\n<li>Model Management: <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_management.html\">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_management.html</a></li>\n<li>Metrics: <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/metrics.html\">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/metrics.html</a></li>\n</ul>\n<h3 id=\"example-codes\" style=\"position:relative;\"><a href=\"#example-codes\" aria-label=\"example codes permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Example Codes</h3>\n<p>아래의 순서대로 코드를 기록하기</p>\n<ol>\n<li>Model Training &#x26; JIT Export</li>\n<li>Create model repository</li>\n<li>Run Triton server</li>\n<li>Install Triton client</li>\n<li>Test inference (request)</li>\n</ol>\n<h3 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h3>\n<p>좋은 깃헙 예제들을 발견하여 아래에 공유합니다.</p>\n<ul>\n<li><a href=\"https://github.com/Curt-Park/triton-inference-server-practice\">https://github.com/Curt-Park/triton-inference-server-practice</a></li>\n<li><a href=\"https://github.com/fegler/triton_server_example\">https://github.com/fegler/triton_server_example</a></li>\n<li><a href=\"https://github.com/triton-inference-server/tutorials\">https://github.com/triton-inference-server/tutorials</a></li>\n</ul>\n<p>공부를 위한 문서들을 아래에 공유합니다.</p>\n<ul>\n<li><a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html</a></li>\n<li><a href=\"https://developer.nvidia.com/blog/nvidia-serves-deep-learning-inference/\">https://developer.nvidia.com/blog/nvidia-serves-deep-learning-inference/</a></li>\n</ul>\n<p>더 쉬운 사용을 위한 wrapper 오픈소스도 존재하네요!</p>\n<ul>\n<li><a href=\"https://blog.rtzr.ai/tritony-tiny-configuration-for-triton-inference-server/\">https://blog.rtzr.ai/tritony-tiny-configuration-for-triton-inference-server/</a></li>\n</ul>","tableOfContents":"<ul>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#ai-inference-pipeline\">AI Inference Pipeline</a></li>\n<li>\n<p><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#nvidia-triton\">NVIDIA Triton</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#basic-archtecture\">Basic Archtecture</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#python-backend\">Python BackEnd</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#model-repository\">Model Repository</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#model-configuration\">Model Configuration</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#protocol\">Protocol</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#other-features\">Other Features</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#example-codes\">Example Codes</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#references\">References</a></li>\n</ul>","frontmatter":{"path":"/MLOps/23-10-24/","title":"NVIDIA Triton Inference Server","category":"MLOps","date":"2023-10-24"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}