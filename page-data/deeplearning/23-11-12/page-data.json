{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/23-11-12/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>NeurIPS 2023에서 oral paper로 선정된 Visual Instruction Tuning 논문을 정리합니다. 해당 논문은 LLaVa라는 이름으로도 알려져 있는데, multimodal learning 분야에서 image와 text pair를 가지는 intruction tuning dataset을 만들고 이를 활용하여 multi-task multimodal LLM을 만드는 시도를 한 첫 논문입니다.</p>\n</blockquote>\n<h3 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h3>\n<p>최근에 NLP 분야에서는 Alpaca, Vicuna, GPT-4-LLM 등, machine-generated instruction-following data를 사용해서 zero-shot 성능을 높이는 LLM 모델들이 많이 등장하고 있습니다. 하지만 이런 연구들은 language model에만 치중되어 있기 때문에 본 논문의 저자들은 multimodal space에서 machine-generated instruction-following data을 만들고 모델을 학습시키는 방법을 제안합니다.</p>\n<p>들어가기 앞서 본 논문의 contribution을 먼저 살펴보면 다음과 같습니다.</p>\n<ol>\n<li>Multimodal space에서 instruction-following data를 만들었습니다.</li>\n<li>해당 데이터를 활용하여 large multimodal model을 학습하였습니다.</li>\n<li>평가를 위한 multimodal instruction-following benchmark를 제공합니다.</li>\n<li>이 모든 것을 오픈소스로 제공하고 있습니다.</li>\n</ol>\n<h3 id=\"related-works\" style=\"position:relative;\"><a href=\"#related-works\" aria-label=\"related works permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Works</h3>\n<p>관련 연구로는 먼저 instruction tuning이 있습니다. 특정 task에서 pre-trained LLM을 활용하기 위해서 초기에는 pretrain-finetune 방식을 사용하였습니다. 먼저 masked language modeling (MLM)으로 backbone을 학습하고, 그 뒤에 task에 맞게 fine-tuning을 하는 방식이었습니다. 이후 연구들에서는 모델에 적절한 few-shot prompt를 제공하는 incontext-learning 방식을 사용하기도 하였습니다.</p>\n<p>최근에는 여러 task에 대한 instruct-response pair 형태의 데이터를 제공하여 튜닝하는 instruction tuning 방식을 많이 사용하고 있습니다. 이러한 instruction tuning 방식들이 좋은 zero-shot 성능을 내고 있습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/cf128ea0968817431169340c8f2af7cf/99f37/23-11-12-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 38.94736842105263%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABn0lEQVQoz5WSW2/aQBSE/f9/RKX2F/QpPLSKCCThYjAXg+NLbbANvmGDGxqDAfN1CS2R+taRjlY62p0zM2cl1/WoPzSwLJvG0ERWLXTHYxpknPl/SGEYocgKURzzuSPzTZtiLlZ810IqwVhVJ5IkwZm5zF0ffxEQhDGbzYY8z3n9ea0sy9hut0hFsWNfHAjjENUZkuTxbdr5fNF45lRVTOwl7bFNZzKjP3Uoy/L9jh+tsbyPN9LF8vNTC03XaQweMX0TN8kZuKkgupo+lgfU5x7y/RN6Z4ilvvAq1FRi0KA7pF6rY8gjUqFcCqOIvjJgla7Y/MpYZUusIKU9z26E5X6P1hkwanaxZBXXsEUEjqgZs6nB+KGN2pCJvQDJ83yaj01Gis3d1wn3NYMXJ0GPcy6Oq9PpPR9v6aKZU6I0pCjfKHYFu92OH55JX+2iTGTCdImUxCmWsGsaaz59GVO781GsNbWxJyxdM7zA8S16k5bIr808sG+ZGTONntpiqAvCbIH0928cjie2wlpxKIWqitOx+rMUbufHsq69f/sX/AZbQVLGN6ym7QAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/cf128ea0968817431169340c8f2af7cf/15813/23-11-12-1.webp 190w,\n/static/cf128ea0968817431169340c8f2af7cf/1cdb2/23-11-12-1.webp 380w,\n/static/cf128ea0968817431169340c8f2af7cf/9046c/23-11-12-1.webp 760w,\n/static/cf128ea0968817431169340c8f2af7cf/abb12/23-11-12-1.webp 1100w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/cf128ea0968817431169340c8f2af7cf/a2d4f/23-11-12-1.png 190w,\n/static/cf128ea0968817431169340c8f2af7cf/3f520/23-11-12-1.png 380w,\n/static/cf128ea0968817431169340c8f2af7cf/3c051/23-11-12-1.png 760w,\n/static/cf128ea0968817431169340c8f2af7cf/99f37/23-11-12-1.png 1100w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/cf128ea0968817431169340c8f2af7cf/3c051/23-11-12-1.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>또 다른 관련 연구들로는 Multimodal instruction-following agent나 Multimodal LLMs 등이 있습니다. (Habitat, InstructPix2Pix, VisualChatGPT, OpenFlamingo, LLaMA-Adapter, etc.) 하지만 이들은 오직 하나의 task만을 위한 모델/에이전트이거나 vision-language instruction-following data로 튜닝된 모델들이 아니기 때문에 성능도 충분하지 않습니다. 따라서 이러한 점들이 LLaVa와 차이를 가집니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/063a39f99079cc4f77ca893cd37823b1/d30ee/23-11-12-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 30.526315789473685%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABk0lEQVQY002NXWvTYABG8/vEKy/0J8gEwaGXwkAYu3AbCt5sSh26zsGgTDYY2+pGabv1+yPt+mK+mjRpTdK0S1YXxdUevRDcc3XgHHgkQz6mvrtMWDtElmVky0fYPooXMv11Q1dVUTWF4dCnazo0ZIHj9PHcAXZPZzgaEZkW/VKJa99H0mo71Hee4edTVIplzksd2o0Gju0QRzHbc0vsPV1GFyqLL1e5c+8+mWyedqvK/ud3mKaOUyhS30wS/T2XdrtNtmsncNHiXDQ5aVe58gaEkylx/ANrv87gQMbp2Rwdp1lbT6BpOq7r8VV0CIKAkRD4uTxxf4CU0RX2zgpMmhqO2UMYMkpPcHkdEYYjFuYe8vzxPIqmcppL83rjDReqxtrSAk8e3KVcrXDZauEeHvHd6CLlvhTJbqTIPHqBWP1IZT1J/tU8+tUZ4XhMYmWF5NsEnudxkMvwKbWF2TXInqbZ+vAey7KIdINhqUz8zUUKgjE9zcAuyAwaCk5HxWk1mPwcc3uz2ewfQBSG3Ex//3e3uj+p6Z3Mx55n0gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/063a39f99079cc4f77ca893cd37823b1/15813/23-11-12-2.webp 190w,\n/static/063a39f99079cc4f77ca893cd37823b1/1cdb2/23-11-12-2.webp 380w,\n/static/063a39f99079cc4f77ca893cd37823b1/9046c/23-11-12-2.webp 760w,\n/static/063a39f99079cc4f77ca893cd37823b1/58b01/23-11-12-2.webp 980w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/063a39f99079cc4f77ca893cd37823b1/a2d4f/23-11-12-2.png 190w,\n/static/063a39f99079cc4f77ca893cd37823b1/3f520/23-11-12-2.png 380w,\n/static/063a39f99079cc4f77ca893cd37823b1/3c051/23-11-12-2.png 760w,\n/static/063a39f99079cc4f77ca893cd37823b1/d30ee/23-11-12-2.png 980w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/063a39f99079cc4f77ca893cd37823b1/3c051/23-11-12-2.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h3 id=\"multi-modal-instruction-following-data\" style=\"position:relative;\"><a href=\"#multi-modal-instruction-following-data\" aria-label=\"multi modal instruction following data permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multi-Modal Instruction-Following Data</h3>\n<h5 id=\"textual-description\" style=\"position:relative;\"><a href=\"#textual-description\" aria-label=\"textual description permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Textual Description</h5>\n<p>본 논문에서는 multimodal instruction-following data를 만들기 위해서 GPT-4/ChatGPT를 활용하는 방식을 제안합니다. 즉, GPT-4/ChatGPT를 strong teacher로 여기고 이들의 응답을 ground truth로 활용하는 방식입니다. 하지만 GPT는 기본적으로 text-only model이기 때문에 image input을 넣어줄 수가 없습니다.</p>\n<p>따라서 저자들은 이미지 입력 없이 multimodal instruction-following data을 만들어내기 위해서 caption과 bounding box라는 두 가지 정보를 GPT에 추가로 제공합니다. 이를 통해 GPT는 text 만으로도 마치 image를 보고있는 것 처럼 대답할 수 있게 됩니다. 참고로, caption과 bounding box 정보 제공 위한 데이터셋으로는 COCO dataset을 사용합니다.</p>\n<ul>\n<li>Caption: 다양한 관점에서 visual scene을 묘사하는 text입니다.</li>\n<li>Bounding box: scene 내의 물체의 위치를 [x<em>min, y</em>min, x<em>max, y</em>max] 형태로 제공(localize)합니다.</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 728px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/75817b458a7e5030b072aabac8095297/cecac/23-11-12-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 34.73684210526316%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABjklEQVQoz12RPU8CQRCG+S3GP2AotDMR7Ywh1hT+ARtiQklpYqGNhZ2JhQWFNtYGODkO7o7j7gigfHp8J0dgAYHudXYIFhZvZndn99l5ZwLr9Rq1mkC1KtBqCdTrApXKFELMMJvNMJ/POQohWJPJBOPxmGOv16P7NQw6HfiDzTqwXC6RTGosVTWQyRhIpXIwDIOVz+dhmiaKxSJs20az2WSNRiNEr6LY291B6CCI8NE+LsOHCCwWC2SzCgoFCckSIANdlx8koWka0uk0g8vlMrmokosWPO8bvj/GdTyGyHEQkfAJzk9DiF2cSeAPFOWLAJ8ErhOgCsuSD5toNBrUjhqrVCpxlJUNhwOyPMVL4gl3N3G8vyXw+vyIh/vbTYWaZpGtElVnE7QA1y3DcWyqusBWJcx1XV7LFkjrMmeSdMMkR9QaXUe/398AVfWDDvPI5bIkjaWqKktRFLbsOA7d0RlqWRb3VUItkszrW+BqteIme16Hp9Zut9GhqXmeh263yznf9/+mvZXcy6n/P/sF/Urd+WpdbkEAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/75817b458a7e5030b072aabac8095297/15813/23-11-12-3.webp 190w,\n/static/75817b458a7e5030b072aabac8095297/1cdb2/23-11-12-3.webp 380w,\n/static/75817b458a7e5030b072aabac8095297/8cb3e/23-11-12-3.webp 728w\"\n              sizes=\"(max-width: 728px) 100vw, 728px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/75817b458a7e5030b072aabac8095297/a2d4f/23-11-12-3.png 190w,\n/static/75817b458a7e5030b072aabac8095297/3f520/23-11-12-3.png 380w,\n/static/75817b458a7e5030b072aabac8095297/cecac/23-11-12-3.png 728w\"\n            sizes=\"(max-width: 728px) 100vw, 728px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/75817b458a7e5030b072aabac8095297/cecac/23-11-12-3.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h5 id=\"response-type\" style=\"position:relative;\"><a href=\"#response-type\" aria-label=\"response type permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Response Type</h5>\n<p>Textual description 기반으로 아래 세 종류의 instruct-response pair를 만들어냅니다.</p>\n<ul>\n<li>Conversation: 이미지 내에 어떤 물체가 있는지, 몇 개가 있는지, 위치는 어떤지 등에 대한 질문과 답변을 제공합니다.</li>\n<li>Detailed description: 이미지에 대한 자세한 설명과 묘사를 제공합니다. (e.g., Describe the following image in detail 형태로 질문)</li>\n<li>Complex reasoning: conversation, detailed description은 visual contect에 대한 질문과 답변이었다면, complex reasoning에서는 더 깊이있는 생각이나 추리를 요구하는 질문과 답변을 제공합니다. </li>\n</ul>\n<p>Response type에 대한 예시와 데이터 생성을 위해 사용한 프롬프트 예시는 아래 사진의 오른쪽에서 확인 가능합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/139031c2dc08227e5b0aa0c5afa0787f/8698d/23-11-12-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40.526315789473685%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABlElEQVQozz2S2ZKCMBRE+f+vmhIFBdl3UJRdllEfXEpqrJ6bOzU+dCWQcOjuRBqGC3w/R5qWpAJNU6NtO1yvVzweD9btdsP7/UNrPYIgoX0JkiRBHMfIsgyHw4E1DAOksmywWGywWulYyBo0zaCPAhRFgaqqUJYlpmki4JufXdfl9d1ux7A0TXkudDqdINX0V0VxsNm4UFQXth3A81yEYUij9wG8Xi+Ga5oOXdf53T9QuBXzvu8hdV1DmxQCGbAsneJ7pIBhAur7PhzHwfP5RE112LbFawIYxyJ2RMCY93ZdB6koWnx9baCqFpZLk+AmfWRju93yKHra73N22DYnuE7CPUZRyrAwFC5z6rfhaiRR9HrtwDA8bI2AIB7HECXneY7j8chR5/mFrr3QekVAipke+EdJUlHcgdwNf4ciil4uF9Thmg5Gpj4VlqqqH8myjPv9TvACtrVD4LfkdA+bZFkmVRLANEX0DNI0fZPtiEuNopjcZXyaIkJdiyvU8jjPMzkY+Up9U7RpHDGOE7v6U4/z+YxfoNY69tMtEaQAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/139031c2dc08227e5b0aa0c5afa0787f/15813/23-11-12-4.webp 190w,\n/static/139031c2dc08227e5b0aa0c5afa0787f/1cdb2/23-11-12-4.webp 380w,\n/static/139031c2dc08227e5b0aa0c5afa0787f/9046c/23-11-12-4.webp 760w,\n/static/139031c2dc08227e5b0aa0c5afa0787f/c89f9/23-11-12-4.webp 1140w,\n/static/139031c2dc08227e5b0aa0c5afa0787f/62b6b/23-11-12-4.webp 1239w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/139031c2dc08227e5b0aa0c5afa0787f/a2d4f/23-11-12-4.png 190w,\n/static/139031c2dc08227e5b0aa0c5afa0787f/3f520/23-11-12-4.png 380w,\n/static/139031c2dc08227e5b0aa0c5afa0787f/3c051/23-11-12-4.png 760w,\n/static/139031c2dc08227e5b0aa0c5afa0787f/b5cea/23-11-12-4.png 1140w,\n/static/139031c2dc08227e5b0aa0c5afa0787f/8698d/23-11-12-4.png 1239w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/139031c2dc08227e5b0aa0c5afa0787f/3c051/23-11-12-4.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h3 id=\"visual-instruction-tuning\" style=\"position:relative;\"><a href=\"#visual-instruction-tuning\" aria-label=\"visual instruction tuning permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Visual Instruction Tuning</h3>\n<h5 id=\"architecture\" style=\"position:relative;\"><a href=\"#architecture\" aria-label=\"architecture permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Architecture</h5>\n<p>모델의 구조는 Vicuna model <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>f</mi><mi>ϕ</mi></msub></mrow><annotation encoding=\"application/x-tex\">f_\\phi</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.980548em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3361079999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">ϕ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>과 CLIP visual encoder, 그리고 projection layer <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"bold\">W</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68611em;vertical-align:0em;\"></span><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">W</span></span></span></span>의 조합으로 이루어집니다. Vicuna는 당시 language task에 대해 가장 좋은 instruction following 성능을 보이는 모델이라서 사용하였고, projection layer는 CLIP image feature를 word embedding space에 맞게 변환시키기 위한 목적으로 사용하였습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/c1cb48752dbc46f81927e75ea1122b42/7161f/23-11-12-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.47368421052632%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABK0lEQVQY032R207CQBRF+f/f8UkfTLwmBKMgQrm0CGk7w7T0NthSCnQ5VPGSGE+y5iQz++yc7GnVdc1/tav2CF/iewLP9QiDkCiIscWIy+EZ19Y5d5MLdvtdo28dj7IqWGXKEBBpM6ADsk1CXmm06SoWDcvIJ0glYaqw/RHdWYf+4hFbWVT77bdhViT0nS59u8tg9ow1f2GpXVa5RGmP4M0Yrj2Dj8xc/HTRvKXlimwbkZThb0PMuodtYdhQV6W5qMmLEhWmrGJNGGVkOmcyfWUwnOLMXA6Hwxen2I69MYxDRfv2is79Db2HNpNBn+UyQEj5IfzM05cK1+T4V51MG0OVFjzZgp5jmEnGUrPOS/MRHuPxmCSJG7EQgvl8jmVZTKdmU8fBM5qfG74DI4HF9SosP3wAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/c1cb48752dbc46f81927e75ea1122b42/15813/23-11-12-5.webp 190w,\n/static/c1cb48752dbc46f81927e75ea1122b42/1cdb2/23-11-12-5.webp 380w,\n/static/c1cb48752dbc46f81927e75ea1122b42/9046c/23-11-12-5.webp 760w,\n/static/c1cb48752dbc46f81927e75ea1122b42/c89f9/23-11-12-5.webp 1140w,\n/static/c1cb48752dbc46f81927e75ea1122b42/575d1/23-11-12-5.webp 1182w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/c1cb48752dbc46f81927e75ea1122b42/a2d4f/23-11-12-5.png 190w,\n/static/c1cb48752dbc46f81927e75ea1122b42/3f520/23-11-12-5.png 380w,\n/static/c1cb48752dbc46f81927e75ea1122b42/3c051/23-11-12-5.png 760w,\n/static/c1cb48752dbc46f81927e75ea1122b42/b5cea/23-11-12-5.png 1140w,\n/static/c1cb48752dbc46f81927e75ea1122b42/7161f/23-11-12-5.png 1182w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/c1cb48752dbc46f81927e75ea1122b42/3c051/23-11-12-5.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>만들어진 multimodal instruction-following data를 실제 학습에 사용할 때는 multi-turn conversation data의 형태로 제공되는데 자세한 방법은 다음과 같습니다.</p>\n<ol>\n<li>Prompt engineering을 위해 system message를 가장 처음으로 제공합니다.</li>\n<li>Human instruct와 assistant answer가 반복되는 multi-turn 형태의 데이터를 구성합니다.</li>\n<li>가장 첫 instruct에 대해서는 text와 image를 함께 제공하는데, text와 image의 앞 뒤 순서는 랜덤하게 선정합니다.</li>\n<li>Stop token과 assistant answer에 대해서만 loss를 계산합니다.</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/cc9873fd8770b9988171cf978a850410/105d8/23-11-12-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.89473684210527%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABhElEQVQoz3VSa5OiMBD0//+y3b31VCAQCBSCIIqsEHkoj7V3Mtbdt01V17y7kp6syrJEmqYoigJZnmGfJTiRfz6fcTqdUBwLxvF4/BV5nsPwPJ9PrKqqguu6sCwLURRBlSHiYo90nyLNUhyuGWMYBkbf9//9f7jf77her1iWBSvDvtlsYNs2wws92J6Nv+s1LNuCG3kQSiCOY3pJgsMhRZLsGSZOkoRrhodvaBpcQUNCwvMC2JYL4Uj4fgg/UJAygOu8aoL6TLzbOdyvVISAerbbLctmDhFmsJw/kOoDKt5A+G8IwjUc5w0y+ISQHzT8TrK8c+7lv3JSfiIMfULIGjLhMDyg7xluc4BmVNCTQvuI0DTka4XLRbKta6o3Aduqkmy/viSmqWUi81wmnKYJ34vJAMsEtHrApaxpoCWCFm07EEFN4k+0kJHIOzweM8Zxwe3WEzpe1DzPL0KtNa0+p4KGJrDoJHbfd5S7sW2aGl3XEcnI38QsTwiHtNyRFfy9zLbN+QFMfk6U4+DXUAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/cc9873fd8770b9988171cf978a850410/15813/23-11-12-6.webp 190w,\n/static/cc9873fd8770b9988171cf978a850410/1cdb2/23-11-12-6.webp 380w,\n/static/cc9873fd8770b9988171cf978a850410/9046c/23-11-12-6.webp 760w,\n/static/cc9873fd8770b9988171cf978a850410/c89f9/23-11-12-6.webp 1140w,\n/static/cc9873fd8770b9988171cf978a850410/446b5/23-11-12-6.webp 1170w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/cc9873fd8770b9988171cf978a850410/a2d4f/23-11-12-6.png 190w,\n/static/cc9873fd8770b9988171cf978a850410/3f520/23-11-12-6.png 380w,\n/static/cc9873fd8770b9988171cf978a850410/3c051/23-11-12-6.png 760w,\n/static/cc9873fd8770b9988171cf978a850410/b5cea/23-11-12-6.png 1140w,\n/static/cc9873fd8770b9988171cf978a850410/105d8/23-11-12-6.png 1170w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/cc9873fd8770b9988171cf978a850410/3c051/23-11-12-6.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h5 id=\"two-stage-training\" style=\"position:relative;\"><a href=\"#two-stage-training\" aria-label=\"two stage training permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Two-Stage Training</h5>\n<p>학습은 two-stage로 수행됩니다. 먼저 feature alignment를 위한 pre-training을 진행합니다. 이 단계는 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"bold\">H</mi><mi>v</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\mathbf H_v</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.83611em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">H</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">v</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>를 word embedding과 잘 align 시키기 위한 과정입니다. 학습을 위해서 전체 데이터셋을 사용하지는 않고 일부만 필터링해서 사용하였고, single-turn conversation data만 사용하였습니다. Projection matrix <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"bold\">W</mi></mrow><annotation encoding=\"application/x-tex\">\\mathbf W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68611em;vertical-align:0em;\"></span><span class=\"mord mathbf\" style=\"margin-right:0.01597em;\">W</span></span></span></span>만 튜닝하고 나머지 파라미터는 freeze 합니다. 이 이후에는 end-to-end fine-tuning을 수행합니다. CLIP visual encoder는 freeze하고 나머지 파라미터들을 튜닝합니다. </p>\n<h3 id=\"experiments\" style=\"position:relative;\"><a href=\"#experiments\" aria-label=\"experiments permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments</h3>\n<h5 id=\"multimodal-chatbot\" style=\"position:relative;\"><a href=\"#multimodal-chatbot\" aria-label=\"multimodal chatbot permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multimodal Chatbot</h5>\n<p>본 논문에서는 multimodal LLM 성능 평가를 위해 multimodal chatbot과 SienceQA task를 벤치마크로 제공합니다. Multimodal chatbot task에 대해서는 답변에 대한 성능을 정량적으로 평가한다는 것이 애매한데, 이 때 저자들은 GPT-4를 기반으로 정량적 평가를 수행하는 방법을 제안합니다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/36a2a0cdad69d6141374524d1b8c5910/67fe0/23-11-12-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 29.47368421052632%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAAAsTAAALEwEAmpwYAAABbklEQVQY022RMW/aUBSFPfA3upL+hW5RlkhlY2XqlAWJKc3ET2iDFIWlY6MMGVAk2JDIxoBUB6mq1KEkLjQm4Ajj92xsnu331bxGydIrneU7V0c691phovgzX+AulqxlxG46nQ7NZpN6vU673UYVbB0lyG2G1pr/zY7vZCUqYzwec3trI8KNMY8/nrC39xbLstg/OEAqzd29w8KXxg/DkOFwyGAwoN/v43neS7ClsgwpAkSwZquUgTfF4vn5GZ9aLbrdrmFxmhlpnWPbNtVqlXK5TKlUotfrsVqtcF0XK0s3LJdzHh9d1PZf5dn0N9dHH7honTJxHEgDNmKJip6MP5lMaDQa1Go1KpUKo9HI1JVSYulcIaO4qBuTpdtiPeLHr59cvH/H1eEbvn23iddzHOcesXJNoBDChE6nU2azGb7vv1ZGJwS+h7d4gDwxMBAhV5df+XL2mQd3XpCsuHpx3zxGPz8gz3OjNE1fHrLTXwQGpWYTVZAhAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/36a2a0cdad69d6141374524d1b8c5910/15813/23-11-12-7.webp 190w,\n/static/36a2a0cdad69d6141374524d1b8c5910/1cdb2/23-11-12-7.webp 380w,\n/static/36a2a0cdad69d6141374524d1b8c5910/9046c/23-11-12-7.webp 760w,\n/static/36a2a0cdad69d6141374524d1b8c5910/41823/23-11-12-7.webp 1101w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/36a2a0cdad69d6141374524d1b8c5910/a2d4f/23-11-12-7.png 190w,\n/static/36a2a0cdad69d6141374524d1b8c5910/3f520/23-11-12-7.png 380w,\n/static/36a2a0cdad69d6141374524d1b8c5910/3c051/23-11-12-7.png 760w,\n/static/36a2a0cdad69d6141374524d1b8c5910/67fe0/23-11-12-7.png 1101w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/36a2a0cdad69d6141374524d1b8c5910/3c051/23-11-12-7.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>자세한 방법은 다음과 같습니다. GPT-4에는 textual description과 question을 입력하여 response를 얻고, LLaVa에는 image와 question을 입력하여 response를 얻어냅니다. 그리고 이 두 response를 다시 GPT-4에 입력하여, LLaVa의 결과가 얼마나 GPT-4와 유사한지에 대해 점수를 출력합니다. 그 결과를 아래의 table에서 확인하실 수 있습니다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/783d5140d3fa9dee34efac73bca3aa23/d6a46/23-11-12-8.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 42.10526315789473%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABK0lEQVQoz3VRCW6DQAzM/3+HgISj3HeIwpFAAalMPZtSbVvVkuXFux7PDKd938FkLMuC+/2OaZowz7OqZVmirmsUUnmuqgp5UahaNw2WdVWzB87p+GCM4wjTNJGmKZ6PB0bJKsvwIYuuAsboug7T7abODXvrAh3jpDNs2xaGYcCyLERRhFTA3MsFaRjCsW3kSYJQzpHrIs9zvHkemiLH/kJ8MdQBC5Hi+z7O57NKW0AcGb4IqCvVlEWq5zhKCSuXHvN/ALdtwzAMyrPm2iARRpQfx7FKsmLNBIR+9n2PI74l/270XQ8/8DC9P9XwTfwiAL3jMva4WAfUSf1gyAf0iFLIhvJoASUTiHful3+e+Mc7+v6vZDLhAAfpIYGOSgAmfxYXBkGg3tISHfATtO5aHLTG2/cAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/783d5140d3fa9dee34efac73bca3aa23/15813/23-11-12-8.webp 190w,\n/static/783d5140d3fa9dee34efac73bca3aa23/1cdb2/23-11-12-8.webp 380w,\n/static/783d5140d3fa9dee34efac73bca3aa23/9046c/23-11-12-8.webp 760w,\n/static/783d5140d3fa9dee34efac73bca3aa23/6e1e4/23-11-12-8.webp 1008w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/783d5140d3fa9dee34efac73bca3aa23/a2d4f/23-11-12-8.png 190w,\n/static/783d5140d3fa9dee34efac73bca3aa23/3f520/23-11-12-8.png 380w,\n/static/783d5140d3fa9dee34efac73bca3aa23/3c051/23-11-12-8.png 760w,\n/static/783d5140d3fa9dee34efac73bca3aa23/d6a46/23-11-12-8.png 1008w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/783d5140d3fa9dee34efac73bca3aa23/3c051/23-11-12-8.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h5 id=\"scienceqa\" style=\"position:relative;\"><a href=\"#scienceqa\" aria-label=\"scienceqa permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ScienceQA</h5>\n<p>ScienceQA 데이터셋에 대해서도 평가를 수행합니다. ScienceQA 데이터셋은 과학 토픽에 대해서 multimodal question과 multiple choice를 제공하여 모델이 정답을 얼마나 잘 맞추는지를 측정하는 데이터셋입니다. </p>\n<p>여기서는 GPT-4와 LLaVa의 ensemble을 두 가지 버전을 제공하고 있고, 이 중에서 GPT-4 judge는 ScienceQA SoTA인 MM-CoT 보다 더 높은 성능을 보이고 있습니다.</p>\n<ul>\n<li>GPT-4 complement: GPT-4가 답변에 실패하면 LLaVa가 답변 제공</li>\n<li>GPT-4 judge: GPT-4와 LLaVa의 답변이 서로 다르면 둘의 답변을 모아 GPT-4에 다시 프롬프트로 제공</li>\n</ul>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/5eedd6a12644d82fbe77e1251b3ff6cd/78612/23-11-12-9.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 26.31578947368421%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAABFklEQVQY0z1Q2W6DMBDk/z+oX1CpfUlJQhJEApjTgLGNw9kkbaeLe6w00u4+zOW0UqMREtr0eHx8YZ2s5hDG4P3+c69j6JaqxTxNqKoKZVlaFEUBpRSaRmAYRjiNNOBNBzPc0I0PaL3gLfDg+x7qKEZWFSQEtCTqBSdM94WIOMIwRBRFYIwhz3NkWYK+13CkvqLkDTivIbsZLK5RlAKBdyLSEJvAhbh2kG2B4LxBVTMkCSOCjJBawjRN7U9rAScvKpwvZ7iei1YZG+95eyGk8GOO1+MOiRDkMMJ2+0QiLzgcDtjv9/84Hk/Y7VyKz+Asyw3TvFiiz9/KdD/DEOZlxDCP9qdURzFjcpLZ7v565JzbfY299vwNQ4dyNw/KD40AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/5eedd6a12644d82fbe77e1251b3ff6cd/15813/23-11-12-9.webp 190w,\n/static/5eedd6a12644d82fbe77e1251b3ff6cd/1cdb2/23-11-12-9.webp 380w,\n/static/5eedd6a12644d82fbe77e1251b3ff6cd/9046c/23-11-12-9.webp 760w,\n/static/5eedd6a12644d82fbe77e1251b3ff6cd/c89f9/23-11-12-9.webp 1140w,\n/static/5eedd6a12644d82fbe77e1251b3ff6cd/4ef06/23-11-12-9.webp 1260w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/5eedd6a12644d82fbe77e1251b3ff6cd/a2d4f/23-11-12-9.png 190w,\n/static/5eedd6a12644d82fbe77e1251b3ff6cd/3f520/23-11-12-9.png 380w,\n/static/5eedd6a12644d82fbe77e1251b3ff6cd/3c051/23-11-12-9.png 760w,\n/static/5eedd6a12644d82fbe77e1251b3ff6cd/b5cea/23-11-12-9.png 1140w,\n/static/5eedd6a12644d82fbe77e1251b3ff6cd/78612/23-11-12-9.png 1260w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/5eedd6a12644d82fbe77e1251b3ff6cd/3c051/23-11-12-9.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h3 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h3>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\">\n<p>Jason Wei, et al. \"Finetuned Language Models are Zero-Shot Learners.\" <em>International Conference on Learning Representations</em>. 2022.</p>\n<a href=\"#fnref-1\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-2\">\n<p>Anas Awadalla, et al. \"Openflamingo: An open-source framework for training large autoregressive vision-language models.\" <em>arXiv preprint arXiv:2308.01390</em> (2023).</p>\n<a href=\"#fnref-2\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-3\">\n<p>Haotian Liu, et al. \"Visual Instruction Tuning.\" NeurIPS 2023.</p>\n<a href=\"#fnref-3\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-4\">\n<p>Pan Lu, et al., \"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,\" The 36th Conference on Neural Information Processing Systems (NeurIPS), 2022.</p>\n<a href=\"#fnref-4\" class=\"footnote-backref\">↩</a>\n</li>\n</ol>\n</div>","tableOfContents":"<ul>\n<li><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#introduction\">Introduction</a></li>\n<li><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#related-works\">Related Works</a></li>\n<li>\n<p><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#multi-modal-instruction-following-data\">Multi-Modal Instruction-Following Data</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#textual-description\">Textual Description</a></li>\n<li><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#response-type\">Response Type</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#visual-instruction-tuning\">Visual Instruction Tuning</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#architecture\">Architecture</a></li>\n<li><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#two-stage-training\">Two-Stage Training</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#experiments\">Experiments</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#multimodal-chatbot\">Multimodal Chatbot</a></li>\n<li><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#scienceqa\">ScienceQA</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/23-11-12-Visual%20Instruction%20Tuning/#references\">References</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/23-11-12/","title":"LLaVa: Visual Instruction Tuning","category":"Deep Learning","date":"2023-11-12"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}