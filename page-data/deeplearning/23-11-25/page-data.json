{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/23-11-25/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>2023 가짜연구소 7기: '최신연구동향 쉽고 재미있게 작성하기' 그룹에 참여하여 작성한 글을 이곳에 공유합니다.  Weakly-Supervised Instance Segmentation을 주제로 선정하여 최신 연구 동향에 대한 글을 작성하였습니다. 인공지능에 대한 기본적인 이해, 특히 CV 분야에 대한 이해도가 있는 분들을 독자층으로 하고 있습니다.</p>\n</blockquote>\n<h3 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h3>\n<p>컴퓨터 비전 분야의 대표적인 문제들로는 classification, object detection, image segmentation가 존재합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6f302dfdd054d8957a9dda68936fe6c4/6871f/23-11-25-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 36.31578947368421%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAABq0lEQVQoz02RzW8SURTFx3/I/8K9exe6dOdG40a7MyYuXKiJBlMTbaVlQRjBlo50WmoT2xBsSNs0gTIMMJ0hw9CBKAWGr59z38qXnJz7Pu65596nXbUa7G7rFH/k2NY3qZSPKB3/4qDwnaKRJZ9NEV4HyBqPRkwmE4XxeEwURYziM4kF8/kcrXpRIfnpDXpqVfHhXp4tPUPqywfSmx9JJt7jd1yVaFkWzWaTdruNbdt4nketVqPT6eC6riqk2XULc8tgf0dgcnZS4fjgkEJWZ8/4RjFn0O/1lJtut0u/31cYDAaEYah4sViwXC7VGy2KbAI/j981GPRNZtM6ntOicVnFqldxPYfZbKYSW62WcuI4jorFmbC4Fohj7fTcZD17J57QrRhajLs8edZkbT3NfmaD3OcEYc9XM2o0GkpAWv5fWIrJvSzt51GRd4+e8zsWK+/eJii/5PH9EqsrL3hqPuRB4jVXvh87j+jFrUuysEBaFpfD4VDFquU/f3c4z7yiFtzjrLDCdT3Pab7IydcUG6Ukb800w5sb9Sl+LCxCwjJPEQ+CQJ3Jfjqd8g9xiunw4z1UoAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/6f302dfdd054d8957a9dda68936fe6c4/15813/23-11-25-1.webp 190w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/1cdb2/23-11-25-1.webp 380w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/9046c/23-11-25-1.webp 760w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/c89f9/23-11-25-1.webp 1140w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/7afe4/23-11-25-1.webp 1520w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/58c0b/23-11-25-1.webp 2688w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/6f302dfdd054d8957a9dda68936fe6c4/a2d4f/23-11-25-1.png 190w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/3f520/23-11-25-1.png 380w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/3c051/23-11-25-1.png 760w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/b5cea/23-11-25-1.png 1140w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/891d5/23-11-25-1.png 1520w,\n/static/6f302dfdd054d8957a9dda68936fe6c4/6871f/23-11-25-1.png 2688w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/6f302dfdd054d8957a9dda68936fe6c4/3c051/23-11-25-1.png\"\n            alt=\"23-11-25-1\"\n            title=\"23-11-25-1\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>이 중에서 객체별 mask(이미지 내에서 물체가 존재하는 영역)를 예측하는 image segmentation에도 세부적으로 여러 종류가 존재합니다.</p>\n<ul>\n<li>Semantic segmentation: 이미지 내에서 물체들의 종류 별 mask를 예측하는 문제를 말합니다. 예를 들면, 컵이 존재하는 영역의 mask, 큐브가 존재하는 영역의 mask, 페트병이 존재하는 영역의 mask를 각각 예측하면 됩니다.</li>\n<li>Instance segmentation: 동일한 종류의 물체에 대해서도 따로 분할하여 mask를 예측하는 문제를 말합니다. 예를 들어, 첫번째 큐브가 존재하는 영역의 mask, 두번째 큐브가 존재하는 영역의 mask, 세번째 큐브가 존재하는 영역의 mask를 같은 종류라고 하더라도 모두 따로 예측할 수 있어야 합니다.</li>\n<li>Panoptic segmentation: 물체 뿐만 아니라 배경(stuff)에 대한 segmentation을 수행하면서도, 물체 하나하나 또한 mask를 예측하는 문제를 말합니다.</li>\n</ul>\n<p>딥러닝 모델을 학습시키기 위해서는 모델이 보고 배울 정답값(GT)이 필요합니다. Segmentation의 GT는 물체가 존재하는 영역의 mask, 그리고 물체에 대한 종류(class) 정보로 구성되어 있습니다. 하지만 이러한 정답 값 말고도, 덜 자세한 형태의 정보를 정답 값으로 사용하는 경우도 있는데 이를 ‘weak-supervision’이라고 말합니다. Bounding boxes, Labeled points, Scribbles 등의 weak-supervision이 존재합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/668042b197a20db5ea87aa5ed6210bdc/39148/23-11-25-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 24.736842105263158%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAABYlAAAWJQFJUiTwAAABFklEQVQY0z1Qu07EMBD0/1DQ8kmU9PTXIUokJDpaGsRJIEF10lWp4AcSiPNw3k5ySS6PYTY8LFlaz4xnd1Yty4KmaWDrGnK6vocOAkRxjHEcUZQlPM+D7/voug41dVIH1PTUDsMArTWiKIJ4KaO/8PT4jA9nD2e/g60qtHmOlkbWWuRpiozisih+3uQK1mJsjEFAM0M+CkMkSQL1ut3i9OQMF+eXuL+9QUmhiFKKy7ZFRoMjJ5nmGQUN28MBf6diczEBuSNvzFTKcd6x2dzh+uoBby+71SChMONnw+kCdrfEck6l+SEhtq6IfMo6JCZTf3IFwqllmf87DkO/RpHY1W9k6So7c113rQWTyGIiE4pWMNFP04RvcKlyv/LkNRkAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/668042b197a20db5ea87aa5ed6210bdc/15813/23-11-25-2.webp 190w,\n/static/668042b197a20db5ea87aa5ed6210bdc/1cdb2/23-11-25-2.webp 380w,\n/static/668042b197a20db5ea87aa5ed6210bdc/9046c/23-11-25-2.webp 760w,\n/static/668042b197a20db5ea87aa5ed6210bdc/c89f9/23-11-25-2.webp 1140w,\n/static/668042b197a20db5ea87aa5ed6210bdc/7afe4/23-11-25-2.webp 1520w,\n/static/668042b197a20db5ea87aa5ed6210bdc/00642/23-11-25-2.webp 2876w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/668042b197a20db5ea87aa5ed6210bdc/a2d4f/23-11-25-2.png 190w,\n/static/668042b197a20db5ea87aa5ed6210bdc/3f520/23-11-25-2.png 380w,\n/static/668042b197a20db5ea87aa5ed6210bdc/3c051/23-11-25-2.png 760w,\n/static/668042b197a20db5ea87aa5ed6210bdc/b5cea/23-11-25-2.png 1140w,\n/static/668042b197a20db5ea87aa5ed6210bdc/891d5/23-11-25-2.png 1520w,\n/static/668042b197a20db5ea87aa5ed6210bdc/39148/23-11-25-2.png 2876w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/668042b197a20db5ea87aa5ed6210bdc/3c051/23-11-25-2.png\"\n            alt=\"23-11-25-2\"\n            title=\"23-11-25-2\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h3 id=\"instance-segmentation\" style=\"position:relative;\"><a href=\"#instance-segmentation\" aria-label=\"instance segmentation permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Instance Segmentation</h3>\n<h5 id=\"unsupervised-methods\" style=\"position:relative;\"><a href=\"#unsupervised-methods\" aria-label=\"unsupervised methods permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Unsupervised Methods</h5>\n<p>‘이미지 내에 물체의 mask(contour)를 코드 기반으로 얻어내려면 어떻게 해야할까?’라는 생각을 기반으로 1990년대부터 관련 연구는 계속 진행되어 왔습니다.</p>\n<p>딥러닝 이전에는 mask GT 값 기반으로 모델을 업데이트 하는 방식보다는, 이미지 내의 색상, 밝기, 질감 등의 정보를 가지고 영역을 분리하고 mask 예측을 수행하는 unsupervised learning 방식을 많이 사용하곤 했습니다.</p>\n<ul>\n<li>K-means clustering: pixel(or feature)에 대해 k-means clustering 알고리즘을 수행합니다. 같은 cluster에 속하는 image pixel(or feature)를 하나의 더 큰 pixel 단위인 super-pixel로 여기고, super-pixel끼리 영역 분할을 수행합니다.</li>\n<li>Level-set methods: urface function을 어떤 특정 수식(energy function) 기반으로 업데이트 하고, 최종 업데이트의 결과물을 활용하여 segmentation을 수행합니다.</li>\n<li>Graph Cuts (Boykov and Jolly, 2001): 이미지를 하나의 graph로 여기고, pixel은 graph node로 여긴 뒤에, 유사하다고 판단되는 graph node (즉, pixel) 끼리 연결지어 영역을 분할합니다. 후속 논문으로 제일 유명한 논문은 Grabcut (Carsten Rother, et al., 2004) 입니다.</li>\n<li>Multiscale Combinatorial Grouping (MCG) (Pablo Arbela ́ez, et al., 2014): 이미지에 따라 이미지 내에 존재하는 물체의 스케일이 달라지기 때문에, 이런 Multi-scale에 대응이 가능한 방법을 제안하는 방법입니다.</li>\n</ul>\n<p>가장 최근에 제안된 Multiscale Combinatorial Grouping (MCG) 알고리즘은 아래의 순서로 작동합니다. </p>\n<ol>\n<li>Image의 크기(resolution)를 서로 다르도록 여러 개 만듭니다 (image pyramid)</li>\n<li>서로 다른 resolution의 이미지에 대해서 각각 contour map을 형성합니다. Contour map을 형성하기 위해서는 색상, 밝기, 질감 등의 정보들을 사용합니다.</li>\n<li>이들의 크기가 모두 일치하도록 다시 재조정합니다. 그러면 contour map은 서로 다르지만 크기는 같은 여러 개의 이미지를 얻어낼 수 있는데, 이들을 적당히 합쳐줘서 하나의 contour map을 갖도록 만듭니다.</li>\n<li>그리고 최종적으로 얻어진 countour map을 또 적절히 grouping 합니다. (SVM, random forest 등 활용)</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2ee8f8b6e91c6e305743ee93514be112/809bc/23-11-25-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 36.84210526315789%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAABpUlEQVQozz2Rz4vUMBTH+z9638v+CYIHUUQRr6ugIHj15lEUDzKsrroyzsrCCh4EZ6edTn9N06RNm7Zp+zFT0YQQeMn7vO/7Ps+0LR/Pz3j3/g2L1SeiNCFJEvzAJ9gG6KqibjRhHJJkCca21G2N0oqiLBjHkaos2ax/0xiDp1zC0xePOD4+4ujWDT58XqDLhrIq54+yqolywcbfEAYBvYv9W8M0zHe82/L8yQmxE+MJIXj99hX3H97lwckdwugapUoCl+z7PsZV1U5hFEekaYbVGu06kIVElnIGKiX5vvpGts/wZKG4vLrg9r2bPHv5GNtNTplGFILCndaOJKIgy1KyNMWWCrFeE0cx8T4ma9LZktPFgm24xdO6ppCCsy+n/Pz1g8mC7S15nqN1xThBrhR7V/0A7B28l5K+6zGtmRWmTv3l8iuFU+p1Xcc4jCwvztk54w/AwQ7kIqdpGqwDZg5YuiEIsadrakameRvb/PfwarWkqiu8Q2Capvmhbxxo56YqW/7GIRCGIK8xfYtxgzKu9c55N04juqu4VmsSHWOtZRgG/gCnaAc6kwH9kQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/2ee8f8b6e91c6e305743ee93514be112/15813/23-11-25-3.webp 190w,\n/static/2ee8f8b6e91c6e305743ee93514be112/1cdb2/23-11-25-3.webp 380w,\n/static/2ee8f8b6e91c6e305743ee93514be112/9046c/23-11-25-3.webp 760w,\n/static/2ee8f8b6e91c6e305743ee93514be112/c89f9/23-11-25-3.webp 1140w,\n/static/2ee8f8b6e91c6e305743ee93514be112/7afe4/23-11-25-3.webp 1520w,\n/static/2ee8f8b6e91c6e305743ee93514be112/81ab8/23-11-25-3.webp 2612w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/2ee8f8b6e91c6e305743ee93514be112/a2d4f/23-11-25-3.png 190w,\n/static/2ee8f8b6e91c6e305743ee93514be112/3f520/23-11-25-3.png 380w,\n/static/2ee8f8b6e91c6e305743ee93514be112/3c051/23-11-25-3.png 760w,\n/static/2ee8f8b6e91c6e305743ee93514be112/b5cea/23-11-25-3.png 1140w,\n/static/2ee8f8b6e91c6e305743ee93514be112/891d5/23-11-25-3.png 1520w,\n/static/2ee8f8b6e91c6e305743ee93514be112/809bc/23-11-25-3.png 2612w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/2ee8f8b6e91c6e305743ee93514be112/3c051/23-11-25-3.png\"\n            alt=\"23-11-25-3\"\n            title=\"23-11-25-3\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h5 id=\"deep-instance-segmentation\" style=\"position:relative;\"><a href=\"#deep-instance-segmentation\" aria-label=\"deep instance segmentation permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Deep Instance Segmentation</h5>\n<p>딥러닝의 발전에 따라 자연스럽게 segmentation 분야에서도 딥러닝 방식을 적용하려는 시도가 점점 늘어나게 됩니다. Instance segmentation을 딥러닝 기반으로 풀려는 시도는 2014년 Hariharan et al.의 논문 “Simultaneous detection and segmentation (SDS)”에서 찾아볼 수 있는데, 해당 논문은 object detection과 semantic segmentation을 하나의 모델로 합치는 시도를 하였습니다.</p>\n<p>이후 연구인 Mask-RCNN에서도 instance 단위로 segmentation을 수행하기 위해서 object detection과 semantic segmentation의 방식을 활용합니다. 즉, object detection 통해 검출한 객체에 대해서 mask 예측을 수행하는 방식입니다. 논문이 발표될 당시 제일 유명한 object detection 모델은 Faster-RCNN(NeurIPS 2015)이었기에, Faster-RCNN의 구조에 mask 예측을 위한 mask head를 장착하였습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6c6005a5f179f275e7bcbd4c74c22c07/772aa/23-11-25-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.89473684210527%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABgUlEQVQoz22SS0vDQBSF/XWuxN/hXhGhGxfdFF10IYq4sCAWBRVLN60IBS2Kq+JCa+IjSdNHmqaddNI0aZrkODPaYm0PTLhzw/3mnjuzBKY4jhFHEQ9hdS2x53IcB7quo9FoQNM0KIqCWq2GMAzh+z6iKMSkfqIlAftN3JbySO1uiJgQmy0qYkopDMNAn/bFvl7/wn25AFa5CPjT2XHmEJvry8gXknAHHiseYjAIEASB6JQQAsuyUKlUcHZ1gJeH7BQ2A+SfiJ2USqxhZ3sFF8UtDJwAHYsyQBe23Re2e70eDNNE/vIEe5k05NccolgQ5y1zZU8TSO+v4lnKMYsjNkvKIASm2cFbtcps6nj/+MTj3TluSkcwiDzT4YQjOuRDvi4mobWeRNK2bWGVy3VdcQHDoYvxOISslEGoNgf7BxzD851p2/wSCOmxfIS/4rNsmy143giLxKFTy/x0w2ih2WyK5yHLMlRVRbvdFg74LfO8qqiQJImNoC6eE58v/zfhfAOjpVbuNh1epQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/6c6005a5f179f275e7bcbd4c74c22c07/15813/23-11-25-4.webp 190w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/1cdb2/23-11-25-4.webp 380w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/9046c/23-11-25-4.webp 760w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/c89f9/23-11-25-4.webp 1140w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/7afe4/23-11-25-4.webp 1520w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/45e78/23-11-25-4.webp 2042w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/6c6005a5f179f275e7bcbd4c74c22c07/a2d4f/23-11-25-4.png 190w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/3f520/23-11-25-4.png 380w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/3c051/23-11-25-4.png 760w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/b5cea/23-11-25-4.png 1140w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/891d5/23-11-25-4.png 1520w,\n/static/6c6005a5f179f275e7bcbd4c74c22c07/772aa/23-11-25-4.png 2042w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/6c6005a5f179f275e7bcbd4c74c22c07/3c051/23-11-25-4.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>Mask RCNN 이후로 여러 Instance segmentation 모델들이 개발되어왔지만, 이 중에서 성능도 좋고 후속 연구에서 자주 언급되는 논문을 하나 꼽자면 “Conditional Convolutions for Instance Segmentation\"(CondInst)을 말할 수 있습니다. 특히, 이후 후술할 BoxInst, BoxTeacher 등에서도 CondInst의 모델 구조를 선택하였습니다.</p>\n<p>Mask RCNN은 mask를 만들어내기 위한 mask head를, 여러 입력 값에 대해 공유한다는 특징을 가지고 있습니다. 즉, mask head는 하나로 고정되어 있지만, mask head에 입력되는 입력값만 달라집니다. 하지만 이런 경우에, 만약 A, B라는 사람이 이미지 내에서 매우 가까운 위치에 존재할 때, A사람에 대해 B사람을 배경으로 예측하는 것이 쉽지가 않습니다. 왜냐하면 mask head로 들어오는 입력 값이 서로 차이가 크지 않기 때문입니다. 따라서, 저자들은 입력에 따라서 mask head가 동적으로 변화할 수 있도록 방법을 제안합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/034cc43bb97d48d54230d7fadb15a2ab/169e3/23-11-25-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.94736842105264%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAACAUlEQVQoz11S22vTUBjv36WwB1/8a2T44qYgm/oypzjBDh0+DEXEF/GOs0zwUrfaudlmmYUmadeGprmsS9KcJE2b5OfJSXrZPvg43/V3vlsujmMklLwpM43Z1HYbA49gFMYwLRe22UPf6mPgB8xvWRbevHs/yU8oN6ukcsRevdXEdv4m7F4Pjq2C+/0aW8V/+Fr+BkkWWMzdew8wN3eJyaPRKAUcgznEhdLpIAwj2H6M5p+fKKxegX5CYOs8FP4xPv118OFgC/tCieVcW1jC4uJSBhhOKwxoS7vbn7H39hnIIMKA+nSRR+PgFwvyXRuO2UaXgqunOizHZHZRamCYVTZtmbbo0y4rH19g9/l9GE5Eq/WZU5Yk+KSPIf2w0zWgKTJ0VQNxXObvKF3k158wOYqirOVMqBe/QDkqY0Q/SqrUWxIKawtw6OD1Rg3FzRXkX5aw/moDO5XvLOfW9Ru4fOHiuRlm5QZBurkwDEGGQGPvB53hPHr9AAq3g/Kjq1h+uo87m6solNLNrizfxsbaw0legnNmy+PTSejkWIBSqzA58H24tgmr78G0bXi+x+y9U3PmOuKzW57eYaprmo5mqw3P80BcF8f0JjW1S1mDZVosxjAMtKl9FjSHGRqDJSASXYhQr0MURbgU8IjncUiZ4zgQQiDLMqrVKmq1GgRBmMzwP/GD4s+epiOVAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/034cc43bb97d48d54230d7fadb15a2ab/15813/23-11-25-5.webp 190w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/1cdb2/23-11-25-5.webp 380w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/9046c/23-11-25-5.webp 760w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/c89f9/23-11-25-5.webp 1140w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/7afe4/23-11-25-5.webp 1520w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/9aab7/23-11-25-5.webp 1682w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/034cc43bb97d48d54230d7fadb15a2ab/a2d4f/23-11-25-5.png 190w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/3f520/23-11-25-5.png 380w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/3c051/23-11-25-5.png 760w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/b5cea/23-11-25-5.png 1140w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/891d5/23-11-25-5.png 1520w,\n/static/034cc43bb97d48d54230d7fadb15a2ab/169e3/23-11-25-5.png 1682w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/034cc43bb97d48d54230d7fadb15a2ab/3c051/23-11-25-5.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>작동 순서는 다음과 같습니다.</p>\n<ol>\n<li>이미지 입력을 모델에 넣어 (multi-resolution) feature를 뽑아냅니다.</li>\n<li>feature 기반으로 instance 마다의 mask head의 weight을 동적으로 결정합니다.</li>\n<li>mask head와 feature 기반으로 mask 예측을 만들어 냅니다.</li>\n</ol>\n<p>Mask RCNN은 물체가 어디에 존재할지 먼저 제안을 하고, 이에 대해 crop을 하고, 이 crop 된 이미지에 대한 mask 예측을 수행하는 과정이 필수적이었습니다. 하지만 CondInst의 경우엔 입력에 대해 모두 다른 mask head를 사용하기 때문에 crop과 같은 기작이 필요하지 않게 되었습니다.</p>\n<h3 id=\"weakly-supervised-instance-segmentation\" style=\"position:relative;\"><a href=\"#weakly-supervised-instance-segmentation\" aria-label=\"weakly supervised instance segmentation permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Weakly-Supervised Instance Segmentation</h3>\n<p>딥러닝 이전까지의 image segmentation 방식은 unsupervised learning 방식이어서 mask GT가 필요치 않았지만, 딥러닝 이후에 supervised learning 방식을 segmentation에 적용하려다 보니 문제가 생기게 됩니다. Segmentation 모델 학습을 위해서는 mask GT가 필요한데, mask GT를 만들기 위해서는 COCO dataset 기준으로 1개 객체당 평균 79.2초가 소요되었습니다. 비용이 너무 높았기 때문에 더 저렴한 비용의 라벨링 방식이 요구되었습니다.</p>\n<h5 id=\"box-supervised-methods\" style=\"position:relative;\"><a href=\"#box-supervised-methods\" aria-label=\"box supervised methods permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Box-Supervised Methods</h5>\n<p>Semantic segmentation 분야에서 이런 문제를 타개하고자 한 논문으로는 BoxSup과 Box2Seg가 존재합니다. BoxSup(Jifeng Dai, et al., ICCV 2015)은 box supervision와 MCG를 활용하여 pseudo labels 만들고 이 pseudo label을 활용하여 FCN을 학습시킵니다. Box2Seg(Viveka Kulharia, et al., ECCV 2020)는 box supervision에 GrabCut을 적용해서 mask 예측을 얻어내고 이를 pseudo label로 활용합니다.</p>\n<p>BoxSup, Box2Seg 등이 물체의 bounding box 정보를 활용하여 segmentation 모델을 학습시키는 방법을 제안하였지만 이들은 모두 MCG나 GrabCut 같은 unsupervised learning 방식에 의존하였고, box supervision에 적합한 새로운 loss를 제안하지는 않았습니다. 따라서 학습에 여러 세부 단계들이 필요하였고 하나의 통합된 framework 형태가 아니었습니다. 따라서 BoxInst(Zhi Tian, CVPR 2021)의 저자들은 box supervision에 적합한 두 가지 loss term을 제안하여 여러 단계를 거칠 필요 없는 하나의 통합된 형태의 box-supervised method를 제안하였습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ccf3eada6dbabb4e6fb92515a631ff28/51800/23-11-25-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 74.73684210526315%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAPCAYAAADkmO9VAAAACXBIWXMAABYlAAAWJQFJUiTwAAADVElEQVQ4y3WUy28bZRTFI/GPsOwCCcSirKp2UYQQKmJXibaqIpVVaFUioRYEUVMELNICikR5pOCkvFqppAk0fbgItZCHIUqCGyeN7Un9jF+1PbHn4Rl74h93PsehXfBJx9/408yZe+65Z3ra7TY+/FWpVonGYsS1GLFYFE3TcBwH27awrA5sgeu66twyTUxz+9y2FU+PT7S1taUIr47/xO59u9j3ynPseuZp9uzfTSqbxGs2od1SaMm1admKRJ3RVrtlNxRPj8/aaNiKcDIY4NDxvRzpe5neEwcY+PQNCo+y0HSo123qNSHxHIyajtuwKJXqhGNVdF0qrFcxDYMenzWuxUmn0ty4Pc7QJ2c5fOggp069xeS1y/LQI5HgcDtUoH84IuQNNjOavN5l6LsoT+29RWAqDUYGq17vVGhKLzzPY/l+mMnxq4x+M8LA+2eYm52Wqmq40kO/wnhSx3MbGJsVKdpmPaVzfTpHNm/gGJVOhTy2Mukk4cU5/p7/i+ELI2hry9IrU+Q1du7xWp0e+kVAa/u0RV3M8bo97JpSrlRIJ+IsLi3yYD1BuZRX/XUFrWqeei6hTGi2WoqwXsqhp2LohRwtUbhjiuN6NARWo4XR8MgWKpR1GQnHwxZs5Iv8fvce03MzhObnSaY2cJoe07Nz3AoG+WPmT5bCEXnR1pOS9ZpJYj1FNpMV+RmSD5PyINz47TIHXn+e3uMvsf/VZ/nl5hVcMedo34scPLaH1w6/QF//EZnD7QrzhYK4WUSXxqeiD0iEQiQFmfgaltNmITzDR+ff5vS7b/LZhbNEVhfUsH85+jEDH5zkzIf9TEx935Hs/2SzWTYEFd0gGY+h3byLFrxHaj2mZC8uLPDtxRHeO/0OJ0/0s7oSUT0cDQS4+MXn9B49xlhgTKl8QnLNsElL5NLRFTJrEbIyn6b0VYutMfHzFX784RKDg4OsrKyINW2mfr3G1PUJzp0bYmzsUofw8Swbpk2uWJV0bFIsb8peFaNcyuUSkeV/JN+rDJ0fZmY2pNy/H15SZ8E7d/jq69H/stwltCTgvvx8Pq+M8a8d+RD4bfFv8T8ExWKRXC5HOp1We7lc3lG4Q9iUwNckET780Bsy8WrOJEo+/P/dWf2/1S3qX5k/LBbXSvYMAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/ccf3eada6dbabb4e6fb92515a631ff28/15813/23-11-25-6.webp 190w,\n/static/ccf3eada6dbabb4e6fb92515a631ff28/1cdb2/23-11-25-6.webp 380w,\n/static/ccf3eada6dbabb4e6fb92515a631ff28/9046c/23-11-25-6.webp 760w,\n/static/ccf3eada6dbabb4e6fb92515a631ff28/c89f9/23-11-25-6.webp 1140w,\n/static/ccf3eada6dbabb4e6fb92515a631ff28/31d32/23-11-25-6.webp 1196w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/ccf3eada6dbabb4e6fb92515a631ff28/a2d4f/23-11-25-6.png 190w,\n/static/ccf3eada6dbabb4e6fb92515a631ff28/3f520/23-11-25-6.png 380w,\n/static/ccf3eada6dbabb4e6fb92515a631ff28/3c051/23-11-25-6.png 760w,\n/static/ccf3eada6dbabb4e6fb92515a631ff28/b5cea/23-11-25-6.png 1140w,\n/static/ccf3eada6dbabb4e6fb92515a631ff28/51800/23-11-25-6.png 1196w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/ccf3eada6dbabb4e6fb92515a631ff28/3c051/23-11-25-6.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<ul>\n<li>Projection loss term: 모델의 mask예측의 x, y축 방향 projection과, box GT가 얼마나 비슷한지를 측정합니다.</li>\n<li>Pairwise loss term: 두 pixel 사이의 색상이 유사하면 이 둘이 같은 label을 가지는 경우가 많다고 주장하며, 가까운 pixel에 대해서 유사한 색상을 가지면 같은 객체로 예측되도록 유도합니다.</li>\n</ul>\n<p>BoxInst에서 말하는 가정이(’비슷한 색상은 같은 instance 일 것이다’라는 가정) 너무 단순해서, 물체와 배경을 제대로 구분하지 못하는 경우가 존재합니다. 따라서 BoxLevelSet(Wentong Li, et al., ECCV 2022)의 저자들은 다른 방법들을 시도하는데, 과거에 segmentation 분야에서 종종 사용되던 level set method를 가져옵니다. 모델의 예측을 한번에 수행하는게 아니라, level set method를 활용해서 mask 예측을 점진적으로 업데이트하는 과정을 거치고, 업데이트된 최종 mask 예측을 모델 출력으로 사용합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b7753e242aed8b81e3f08e288e58394a/bb33f/23-11-25-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 42.10526315789473%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABs0lEQVQoz1WSy27TQBSG/cS8A2/QRSXYAQsklkgsihCquAhBBVShIk1JaYhjJ04cX2bG4/vYH2OnVWEWc1n83znn/8fp+55hma7D9dbEcYxuOkRtaE2HMYZwv8dbb5FCoquccLchDgMW3h90kY/6AwWc25OsqMfTdV3Or5fcyJrCgoeCUipWmw1FoSlNz6t3Jzx9ccz0xrc6dQBWlS3eHoBfpku+z63A3jdCsxI5V1lHVPVD68TBiij5TV2XtG3DkzcuDx7/IBUZTd/SZhnziwlCpjhxKnn++oyPP1ckTU9o316asRWKyXzBerFApb6FRezCLSK3HXUGozJkJinKnMj3mF3NLFDgLL2A02+/WCc5vm7IG4OvKnZSo3SBUAXbcIkfB+NolWkIoj1Tf44fBGNXibdCKWGHMTgyy9nuwtH8wZ+u69F1SxQndmTD4Oyj9185Pj2zRhkqO+KnD+ccHT3j8vKaoi7o23YM9b9QBuD96sdk3VSPST98+ZaTyYwm18hS24RTLj5P0WWBRd2rbIDOsN19nbv77dN6ihVVLPw1Ik2pyvJQnA5VZqg8s6V7/mX8BarLXiM5WyXSAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/b7753e242aed8b81e3f08e288e58394a/15813/23-11-25-7.webp 190w,\n/static/b7753e242aed8b81e3f08e288e58394a/1cdb2/23-11-25-7.webp 380w,\n/static/b7753e242aed8b81e3f08e288e58394a/9046c/23-11-25-7.webp 760w,\n/static/b7753e242aed8b81e3f08e288e58394a/c89f9/23-11-25-7.webp 1140w,\n/static/b7753e242aed8b81e3f08e288e58394a/7afe4/23-11-25-7.webp 1520w,\n/static/b7753e242aed8b81e3f08e288e58394a/49648/23-11-25-7.webp 2208w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/b7753e242aed8b81e3f08e288e58394a/a2d4f/23-11-25-7.png 190w,\n/static/b7753e242aed8b81e3f08e288e58394a/3f520/23-11-25-7.png 380w,\n/static/b7753e242aed8b81e3f08e288e58394a/3c051/23-11-25-7.png 760w,\n/static/b7753e242aed8b81e3f08e288e58394a/b5cea/23-11-25-7.png 1140w,\n/static/b7753e242aed8b81e3f08e288e58394a/891d5/23-11-25-7.png 1520w,\n/static/b7753e242aed8b81e3f08e288e58394a/bb33f/23-11-25-7.png 2208w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/b7753e242aed8b81e3f08e288e58394a/3c051/23-11-25-7.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h5 id=\"box--point-supervised-methods\" style=\"position:relative;\"><a href=\"#box--point-supervised-methods\" aria-label=\"box  point supervised methods permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Box + Point-Supervised Methods</h5>\n<p>BoxInst 같은 box-supervised method가 이미 존재하지만, 여전히 성능은 충분하지 않았습니다. 그렇다고 해서 fully-supervised 방식을 사용하기에도 비용이 너무 많이 들게 됩니다. 따라서 Pointly-Supervised Instance Segmentation (Bowen Cheng, et al., CVPR 2022) 논문의 저자들은 box-supervison과 더불어 point-supervision도 사용해보자는 의견을 제시하고, 이 때 point-supervision을 box-supervision 기반으로 만드는 효율적인(빠른) 방법을 제안합니다.</p>\n<ol>\n<li>작업자가 Bbox를 만들면, 이 bbox 안에서 랜덤하게 point가 찍힘</li>\n<li>이 점에 대해서 작업자가 foreground와 background 라벨링을 또 한 번 수행함</li>\n<li>이 작업은 객체당 15초 정도 소요됨. 즉, fully supervised 방식 대비 5배 정도 빠른 라벨링 가능</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/744d929dcaac396b9291107a814a90cf/cf8e5/23-11-25-8.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 51.05263157894737%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAAC5klEQVQozw2Qa0+TBwCF3yzZXJYYk2UjS8QrE8JEBFrK5S3tKqQtrS225S1gqU1vQKWjAoUZCUwmBNRKVC5zijKWbBI1wRu0HUopBmUmMqcRIibTDySLW/YbnvXjSU7Ok+cI799vkEzc5elylERiloVkjOjsDImHcdZfLLHy+wLLS3O8Wkmy9myBx4sxZuP3eTAfZT4RS/XjPEzEuTczzfqb1wgvV+fo7TfQN6gn3J2Dw7eHCt1OcmVpuA/lYFBn8FXmZ9Qfysdtk1Gu3MnmtA/4fNtHpKVvIn3XJ3z6xSaEDwUiQ6cQNt4tcW04xE8jzUz9epAbN6zMxSqZvq1jbEBDd6PI8RY5TZKGBssBzJUiMsWXKFQ7kJdsZ2/OVvbuS+fjLQJnIn0Ib9fu0dmYS7tHh9teQbPbwPnean6+4GC0XyLyrcTpHivHAk4aLWpUiiwsxlIOW9VYjSL2qq+ps5Yjlu3g+s0LCH+vp5TbdJwIHcZfW85Rr5mBEw30pfLJcD1d39gQC2U4a0sZH9Fg0WdjNalw1VYgmUuwGBRIB5UE/AX8tTGF8O75PC1OFbemJMaGdAw2ahjokOho0uK1KbDqsqlIqfndBVwcraTVV4LLlk/Qq8NuykMyZlNfLTJ0qYC1txMIzx/FqXFkcfbSfq78oOV0h46gT4HPk4/Hl0m7qxS7MYM6p5x61266QhrajqsINIuc7C/l8i8l9HSZGJ5U8uebcYR/V5f4vkuJ1pHOsZAWh1eOvioDg5RNlWsXkbCRYJMa7xEZ1ZY9qb+L8IWKsTkzCXTmEejJwuMXaQvns7L6I8I/fySZH65m9HIZExN6Lg6VMdCrpLO9mPBRGT0BNWGPmla3nBZrAd8FD6QAIg2ufbT7i2hNjYWDKjx1mUTvjyH89/oJy6NOFu/Yic8amIua+S1mIj5jZmbaxNSkmZGIllPdZbQ2FHLEW0iNK49aby6htiLOndEzOWFhsL+Y5OJV/gdeD8lPSji3TwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/744d929dcaac396b9291107a814a90cf/15813/23-11-25-8.webp 190w,\n/static/744d929dcaac396b9291107a814a90cf/1cdb2/23-11-25-8.webp 380w,\n/static/744d929dcaac396b9291107a814a90cf/9046c/23-11-25-8.webp 760w,\n/static/744d929dcaac396b9291107a814a90cf/c89f9/23-11-25-8.webp 1140w,\n/static/744d929dcaac396b9291107a814a90cf/7e452/23-11-25-8.webp 1402w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/744d929dcaac396b9291107a814a90cf/a2d4f/23-11-25-8.png 190w,\n/static/744d929dcaac396b9291107a814a90cf/3f520/23-11-25-8.png 380w,\n/static/744d929dcaac396b9291107a814a90cf/3c051/23-11-25-8.png 760w,\n/static/744d929dcaac396b9291107a814a90cf/b5cea/23-11-25-8.png 1140w,\n/static/744d929dcaac396b9291107a814a90cf/cf8e5/23-11-25-8.png 1402w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/744d929dcaac396b9291107a814a90cf/3c051/23-11-25-8.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>그리고 저자들은 이렇게 만든 point supervision이 다른 '모든' instance segmentation pipeline에도 적용 가능케하는 방법을 고안합니다. 즉, 이 point supervision을 가지고 mask loss를 계산하는 방법을 고안하여 제안합니다. 이 방법을 통해 fully supervised의 94~98% 정도 성능을 달성하였습니다.</p>\n<ol>\n<li>예측은 기존 instance segmentation의 모델과 동일하게 수행한 다음,</li>\n<li>GT points에 대해서 loss 계산을 하는데, prediction points는 prediction mask들의 bilinear interpolation을 사용. 이 방법은 기존 instance segmentation 모델에 대해 구조상으로는 변경될 것이 따로 없음</li>\n</ol>\n<h5 id=\"point-supervised-methods\" style=\"position:relative;\"><a href=\"#point-supervised-methods\" aria-label=\"point supervised methods permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Point-Supervised Methods</h5>\n<p>이 외에도 Point만을 사용해서 instance segmentation 모델을 학습시키는 방법들도 존재한다고 합니다 (읽지는 못했습니다).</p>\n<ul>\n<li>WISE-Net (Issam H Laradji, et al., ICIP 2020)</li>\n<li>BESTIE (Beomyoung Kim, et al., CVPR 2022)</li>\n<li>AttentionShift (Mingxiang Liao, et al., CVPR 2023)</li>\n</ul>\n<h3 id=\"semi-supervised-methods-w-weak-supervision\" style=\"position:relative;\"><a href=\"#semi-supervised-methods-w-weak-supervision\" aria-label=\"semi supervised methods w weak supervision permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Semi-Supervised Methods w. Weak-Supervision</h3>\n<p>BoxInst 좋지만 이제는 full-supervision 없이 모델 성능을 더욱 발전시킬 수 있는 방법이 없을까에 대한 고민도 필요한 시기가 되었습니다. 이에 대해 BoxTeacher(Tianheng Cheng, CVPR 2023)의 저자들은 ‘high-quality pseudo label’을 만들어내는 것이 성능 향상에 도움 될 것이라 주장합니다. 처음에는 그냥 BoxInst의 예측을 정답으로 여겨서 instance segmentation 모델을 학습시키면 안되나?라는 생각에 그렇게 시도해봤더니(figure 상에서 self-training으로 표기), BoxInst 보다 성능이 낮아져서 다른 방법을 고안하게 됩니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b7ca50578fca65006f3f84f33a3f3d2d/2a333/23-11-25-9.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB4klEQVQoz4WRa2sTQRSG+9/8JX4WETWIH6xQEFq8gBeEok01NGmVgkQURPOlhGrj5rLJZmc3adLsNrsx90ASS0rM5nF3kwjVqgcOc+byPnNm3iXmMZ1O/bHV66GIMIVinFyhQjqbQmT3UXSdVqdDp9NmNBrhONO5zvml93JpAXSc2UalmCWyfoFYbIu9gzThzTVeR54Qi3+hYppYtkXNqvpnNaPJXuabX0+cvwBNPcOnR1d5+/gW4Y11vn6IEI++ILoTolY1KJgtF3LIoXXMnd2nhD4WfN2PiXMWOJ0D7coRiXdviN1bYXt5za3fE9/dIfTwLnYhy2r0GYFgkMDmcwIvH7AVE+cDncnEH8uJONvXLrJx4zKvVpeJ3r5C8OZ17gcuYQkZ+UggaSqSLjhQZSq17kz/+x8uTOl3O+QTn8mnJEqqgikUlMQ+RTmNMx77Z8anp0y8em6MM9d68Qfw+8kJvX7ff4JVq/lGNFpt3+F6o0Gr3ULTNKrVKt1el8FgcL7LC2C9XkcTAsM8plg2UHI5hDvXXYiue6mTz+cxDANVVbFt+9/AZrOJJEkoikLOhXmiVCqFKlyQW3uXlUolyuUysiz/H+h1mEwm/ZwBhSvMkFEEObczb21xmQcbDodngD8BSnvTEgAE1ZUAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/b7ca50578fca65006f3f84f33a3f3d2d/15813/23-11-25-9.webp 190w,\n/static/b7ca50578fca65006f3f84f33a3f3d2d/1cdb2/23-11-25-9.webp 380w,\n/static/b7ca50578fca65006f3f84f33a3f3d2d/9046c/23-11-25-9.webp 760w,\n/static/b7ca50578fca65006f3f84f33a3f3d2d/c89f9/23-11-25-9.webp 1140w,\n/static/b7ca50578fca65006f3f84f33a3f3d2d/17fe2/23-11-25-9.webp 1484w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/b7ca50578fca65006f3f84f33a3f3d2d/a2d4f/23-11-25-9.png 190w,\n/static/b7ca50578fca65006f3f84f33a3f3d2d/3f520/23-11-25-9.png 380w,\n/static/b7ca50578fca65006f3f84f33a3f3d2d/3c051/23-11-25-9.png 760w,\n/static/b7ca50578fca65006f3f84f33a3f3d2d/b5cea/23-11-25-9.png 1140w,\n/static/b7ca50578fca65006f3f84f33a3f3d2d/2a333/23-11-25-9.png 1484w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/b7ca50578fca65006f3f84f33a3f3d2d/3c051/23-11-25-9.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>새로 제안한 방법은 다음과 같습니다.</p>\n<ol>\n<li>Teacher - Student 구조로 모델 학습시키고(Backbone으로는 CondInst 활용), 이미지 입력에 strong augmentation 적용</li>\n<li>모든 모델 예측을 pseudo label로 사용하는게 아니라, box GT와 충분히 비슷하고(high IoU), 모델이 강한 확신 보이는(high confidence) 예측만 필터링해서 pseudo label로 사용</li>\n<li>추가적으로, 예측 noise 줄이는 loss 고안해서 적용</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ef7a7f3f4632e792a58fcc1c52a3e397/506d5/23-11-25-10.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 42.63157894736842%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAACEUlEQVQoz4WSS2vUYBSG57/4J/wV7kXcCC7FhYXiorgQLxsFF6JoQRC8IVQpLVpaa2nR0k6t07kkk8m1k8kkk04nSTOTTOeSx0yGWlz5wct3vnNe3u/wnpPjPyfqd3FPWnS7XUiSNHOGf0+STHO5KApZXPzI/ItnrG1spgoup+oqp819TMvGdEwOlSL1coVe/xTp0KYg6dhtn0bLZX07j1PX/ormjvwGsw+ucH3mEvefPCJp7dP8dJNw5zlmTcF1bOydPdTCb5zOCQtbJV4t/2BHNPn8c5mrczfYWHqbCY7HI3Jx/4RSbZO9yhaGKU/SxG2RUWhPfx0NscIxutFMSyO6UUyjaROGPaxmA7WuEka98w4ngVJd4/27p3zP/yKIPXb1LRRXzEiKXGLl9WMWvixhuS6mruJYFkG3jRu2SHoh47YzdXYqmPDm2zUu377A3Pw96sc1Xq7cZbX4IfP+6/o8d25dZObhLAVNZ7ukUFYtmq6G2qpiSwd4tYPzDofDIY2OiGzvph6pxP0onWqDIDrOSMeBjeGI2K6B44fs6W0kJyCOI/pxL9uCwXiY+jfOkJNlmXJRQChLiJUq+d18+q4gClVEUUCq1pBragoFVZGRq0LKKyFJEoIgphyRSroBhUIBz/PIGYaBcWig6xqTWNM0jtpH+L5PEATZ3fE6Gdn3gwydjpcOJcxqZ7xJfTAY8AdM4Ivsvcv51AAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/ef7a7f3f4632e792a58fcc1c52a3e397/15813/23-11-25-10.webp 190w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/1cdb2/23-11-25-10.webp 380w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/9046c/23-11-25-10.webp 760w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/c89f9/23-11-25-10.webp 1140w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/7afe4/23-11-25-10.webp 1520w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/3fd9a/23-11-25-10.webp 3106w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/ef7a7f3f4632e792a58fcc1c52a3e397/a2d4f/23-11-25-10.png 190w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/3f520/23-11-25-10.png 380w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/3c051/23-11-25-10.png 760w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/b5cea/23-11-25-10.png 1140w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/891d5/23-11-25-10.png 1520w,\n/static/ef7a7f3f4632e792a58fcc1c52a3e397/506d5/23-11-25-10.png 3106w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/ef7a7f3f4632e792a58fcc1c52a3e397/3c051/23-11-25-10.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>BoxTeacher와 동일한 시기에 진행된 연구로는, 예측 기반으로 pseudo-label을 만든 후 self-training 하자고 주장하는 논문인 SIM(Ruihuang Li, et al., CVPR 2023)도 있습니다. 심지어 backbone도 BoxTeacher 논문과 동일하게 CondInst 활용합니다. 두 논문의 motiviation이 완전히 동일하다고 생각하시면 됩니다.</p>\n<p>여기서도 역시나 BoxInst 예측을 그대로 pseudo label로 활용하면 안되는 이유에 대해서 언급합니다. BoxInst에서 사용하는 pair-wise affinity loss는 비슷한 색상의 foreground, background 구분 잘 하지 못하는 문제가 발생하기 때문입니다. 이에 따라, semantic(class) level로 지식 공유하면 품질 좋아질 것이라는 생각을 하게 되지만, 당연하게도 semantic segmentation mask를 가지고는 각각의 객체를 구분하지는 못합니다. Semantic segmentation 방식은 같은 class면 다른 객체여도 모두 하나의 mask로 예측하기 때문입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/eb7e0c06c86046b1fc612de5ccf2e729/093ec/23-11-25-11.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 20.52631578947369%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABYlAAAWJQFJUiTwAAABOklEQVQY0x3Pu0sCAQCA8fs/WnpMUrS0CEEttQQhRNTQVEIl2MucS4KCuohoirLA7AERtBlES+hQer4KTy9LRfGBmp7kKRHxJc3f8vuEN4+HB3EH9/ER97s73KyvEcm9ItcU3rV3lPobaS2D2+1hxWLBal1laXGBza1NYmrsv0eqUWJ1hXQtjXC9t894VwemiUmWBvqZ1+sJVSSmxG3MhwfEiSI3FK7OzxgdHWF4aJC+Xh2GMQNS3U/wJ4x4d8lTQyKuxREcTiftnW109+gQt204LpxInwHsz7eIrkteCCPXo5ye2DEap1k2z2GancG2YcOnSgS/Q7hSj/iafpTWlaDWVJKpDzKpBOVCnkQyTUbNUfktUWhmyWo5ys0yiUQSr9dLINBSBQPIUZmiVqTYKJL/ylFqlqg2qvwBdgD4RVFd8r4AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/eb7e0c06c86046b1fc612de5ccf2e729/15813/23-11-25-11.webp 190w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/1cdb2/23-11-25-11.webp 380w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/9046c/23-11-25-11.webp 760w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/c89f9/23-11-25-11.webp 1140w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/7afe4/23-11-25-11.webp 1520w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/c8acf/23-11-25-11.webp 1930w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/eb7e0c06c86046b1fc612de5ccf2e729/a2d4f/23-11-25-11.png 190w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/3f520/23-11-25-11.png 380w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/3c051/23-11-25-11.png 760w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/b5cea/23-11-25-11.png 1140w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/891d5/23-11-25-11.png 1520w,\n/static/eb7e0c06c86046b1fc612de5ccf2e729/093ec/23-11-25-11.png 1930w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/eb7e0c06c86046b1fc612de5ccf2e729/3c051/23-11-25-11.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>따라서 저자들은 semantic mask와 instance mask 둘 다 활용해서 pseudo-label 만들자는 생각을 하게됩니다. 대략적인 동작은 다음과 같습니다.</p>\n<ol>\n<li>잘 학습된 instance segmentation 모델(CondInst or Mask2Former) 구비</li>\n<li>Class-wise Prototypes라는 것 활용해서 semantic(class) mask 예측</li>\n<li>Instance mask 예측</li>\n<li>Semantic mask와 instance mask를 적절히 조합하여 최종 pseudo mask 얻음</li>\n<li>이 pseudo mask를 가지고 모델 학습</li>\n<li>이 외에도 Copy-paste라는 데이터 증강 방법도 사용…</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/2a478c564f29a5d0dfbc6b9fb6941b49/4969b/23-11-25-12.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 44.73684210526316%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAACHElEQVQoz0WRy2sTYRTF5+914dqNGxcu3epOEXSjCGIVMdSm1TQ2dtKkSToJTfOYySQzybzf75+fRfCDw73wXc4591ypaRr+o6YpUigS8sgnjwPKPKHIYgrR16KvRO8eDHz7IOYKKm9H5WhUtkqd+Ejcv+ZfqRFM6LrF0ckdeRITrueYvXNSQVjEviDacz0cMptOBPGAKtpTR66ATZ2GSEleEggeO0wIowQyn3ZX4+WHGTs3YPHtK1a7BVUlxDL09RJZltFEjZw5WehQCuFG/DV5itTrjXj68BlPXp/RVVSK1GFj7DEPJs9bX+gNfwtlD0UbsdxO8fwdaeIKeGLjlCR2iEKbJNSpyhjptNPj0YPHvPrYRllvSHIPJ3DpXG9Y2BrWfoUxkrnRFW41hfV2wWVfZjiWCcItgSAPIwtt9VmIKUiVWOVvgk1VUpYFZeVz1t/y4t2UMAsxfh6jtz6RliKOJmWkjHn7/ojz7ncCe4zjWiyv2sjDN0TuD6S6roX1QqAkL3KqJmaysoRbm6hM2YmsLMvEy0TG4sqquaNz0RUEAw6+S1El2OaS+eLi3rEUhiGqqrJYLDDEcEHObD7nUu5jBDGa7bHcmdyqG6aazs1kxkmrzdVVn62hEKQJSRUKsYBMbCHFcYy5N7EOFom4VlbHjO+WdPpjjDBgZTsMN3sm+h7d8xjeKByfnvJrMEC1DnhpwMBRODVltrHBH+1YmFli3yrXAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/2a478c564f29a5d0dfbc6b9fb6941b49/15813/23-11-25-12.webp 190w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/1cdb2/23-11-25-12.webp 380w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/9046c/23-11-25-12.webp 760w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/c89f9/23-11-25-12.webp 1140w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/7afe4/23-11-25-12.webp 1520w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/b23db/23-11-25-12.webp 2454w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/2a478c564f29a5d0dfbc6b9fb6941b49/a2d4f/23-11-25-12.png 190w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/3f520/23-11-25-12.png 380w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/3c051/23-11-25-12.png 760w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/b5cea/23-11-25-12.png 1140w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/891d5/23-11-25-12.png 1520w,\n/static/2a478c564f29a5d0dfbc6b9fb6941b49/4969b/23-11-25-12.png 2454w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/2a478c564f29a5d0dfbc6b9fb6941b49/3c051/23-11-25-12.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>Instance segmentation 모델의 예측으로 pseudo-label을 만들 때 생기는 문제점이 False-Negative(Missing), False-Positive(Noise) 케이스들입니다. 즉, 객체를 ‘아예 잡지 못하거나’, ‘잘못된 영역을 객체라고 잡는 문제’가 발생할 수 있습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/380b6e675572305f010f55483ff86254/f36fd/23-11-25-13.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 41.05263157894738%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB20lEQVQoz2WRTWsTURiFZ+PSvX/CnyNd2UKWuhJEEXGhC7VQabEigkgNohGJCZZuoiFQY23GmsRMTcLMJDFDmUya72+TSWYe70wpFHvhcN/Fveec9xzJdV18AI7j+DNn5tlsRq/Xo9vt0ul0GAwG2NMpi/mcucDpu9Nb4r9jGEfomk7NNGk0GmiahqIoZDIZ5FQKtVLBjMc5Lhbpj0bYQvDs8QnrrTaHqsbf2Zz9g++8fhfk7fsPyHLKd1Sr1QT5sS9QsyzGwvFkPGU6s8VvF9ue0+oJ57aN5LgL3mzf5+Hzq+wkXpIrfCVxEGIrtEY4HGY0GmMKt7qu+/BIu70+8q8Su7JKo7/g9s0bXJQk1jfWTwifBJe59uAyW5F7yD9jvIht8Cq8SjQSxLJMyuWKT1YoFCiXSjQbTeSMyk78B+qfBnfv3OLSBYnNZ0+RvCAV7RvJ9EcqRp50fpfPiU1Cj64T/RTEdWyazRbtdtuHV45XQPmoLfK0/NxisRjLgQDJZPJ8KcPRAKteJa+kUA5/U60aqKpKLpcjm82K0gw/x8Reju0vaVSjy9LSFSSx8uO11ROHjrNgIeApnxMYDqnX6352liDy5slkQkE3xdpFmu0B0WiElcAKyf09/gEIxDJvpkwwFQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/380b6e675572305f010f55483ff86254/15813/23-11-25-13.webp 190w,\n/static/380b6e675572305f010f55483ff86254/1cdb2/23-11-25-13.webp 380w,\n/static/380b6e675572305f010f55483ff86254/9046c/23-11-25-13.webp 760w,\n/static/380b6e675572305f010f55483ff86254/c89f9/23-11-25-13.webp 1140w,\n/static/380b6e675572305f010f55483ff86254/7afe4/23-11-25-13.webp 1520w,\n/static/380b6e675572305f010f55483ff86254/c2d9d/23-11-25-13.webp 2488w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/380b6e675572305f010f55483ff86254/a2d4f/23-11-25-13.png 190w,\n/static/380b6e675572305f010f55483ff86254/3f520/23-11-25-13.png 380w,\n/static/380b6e675572305f010f55483ff86254/3c051/23-11-25-13.png 760w,\n/static/380b6e675572305f010f55483ff86254/b5cea/23-11-25-13.png 1140w,\n/static/380b6e675572305f010f55483ff86254/891d5/23-11-25-13.png 1520w,\n/static/380b6e675572305f010f55483ff86254/f36fd/23-11-25-13.png 2488w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/380b6e675572305f010f55483ff86254/3c051/23-11-25-13.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>따라서 The Devil is in the Points (Beomyoung Kim, et al., CVPR 2023) 논문의 저자들은 이 두 가지 False case를 해결해준다면 성능을 높여줄 수 있을 것이라 생각하였고, 이를 위해서 (라벨링에 cost가 적은) point label을 활용하게 됩니다.</p>\n<ol>\n<li>만약 dataset의 10%만 fully labeled 되어있다면, 나머지 90%는 point supervision을 부여함. 당연히 이 부분은 사람이 만들어야하지만, 객체 하나 당 point 하나이기 때문에 비용은 적음</li>\n<li>\n<p>(Step 1): Fully-labeled data로 먼저 Teacher network와 MaskRefineNet이라는 모듈을 학습함.</p>\n<ul>\n<li>MaskRefineNet: <code class=\"language-text\">Teacher의 mask 예측</code>, <code class=\"language-text\">이미지</code>, <code class=\"language-text\">Instance Point</code>를 입력으로 받아서, mask 예측을 개선시키는 네트워크임. 즉, Point label을 사용해서 더 좋은 예측으로 mask를 업데이트!</li>\n<li>Teacher 학습시에는 데이터셋에 point supervision이 존재하지 않기 때문에, mask 예측의 center point를 입력 point로 사용</li>\n<li>Student 학습시에는 데이터셋에 point supervision이 존재하기 때문에 해당 point supervision 활용</li>\n</ul>\n</li>\n<li>(Step 2): Teacher의 예측을, MaskRefineNet 기반으로 개선시킨 후에, 개선된 mask를 pseudo-label로 사용함</li>\n<li>이 외에도 adaptive strategy라는 pseudo label 좀 더 보완하는 방법도 추가 제안함</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/8dfcc6ee72bcb01c45f57e55fe02d77f/0330c/23-11-25-14.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 45.78947368421053%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAABYlAAAWJQFJUiTwAAACF0lEQVQoz1WSW08TURRG+z/9G/4FY4xIBQkhsQmWWwgEnZYgoIkITUgg0QegsZ1CL9OZtnM5M512Lq2FLs+ZB42T7MzLOeusb++dQ36mb2J4LUyvTTCycYXJIOzTF11Mp8M4FfLUnDAM8YXA930sy2IwGNDtdvE8D9d1GY/H5HzPYWvhJddHFW6ubtg/O2br/JC1lTyl4zK+51Kp/kREIXZfAno9wtGIarWK4zi0Wi0iCVKPpWlKbtCzKBffc1e65mPpjOcnuxS/n7D6doHCVhHjdJ18sYAujRwJa7TbTKbTDDSbzRDSWIEUcDKZkNP7Js92V+Vhi8R2CMIY7UuF/b0PlI+/wtOQu9tfMk5MFEWIICCQ1Wg06HQ6Wew4jv8ZDoeCnqPzFFsIu8YsNklEC7dXJw4NRsMuPHqq1fQGNoYEqKi6rmd2qhRQ/ZMkITf9nbB/UGB5Kc9l+YzS0QnLhxsyZp7N3XVsq83KzgadgcVURlImtm1Tr9cxDCOzHMmeqsoMo0iwVnjF0vIKVe0Hxb3PLB5qvFh7Q/7dIvXTHda1T4h4TCoNlIma6sPDw19D1Qo1+cwwnaTU9Brz+ZzbWpOn2SN3DZMDbY/LiwvmyRitrEkrh4k0CIbD7PL9/X22Mmp9/uuh6wq+nVfomibb5SvsRoup2YFhCHFCGgheb29iux59NWU5XbU2zWYzi2nKewqm9lCZ/gHLz4PCTcNCYwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/8dfcc6ee72bcb01c45f57e55fe02d77f/15813/23-11-25-14.webp 190w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/1cdb2/23-11-25-14.webp 380w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/9046c/23-11-25-14.webp 760w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/c89f9/23-11-25-14.webp 1140w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/7afe4/23-11-25-14.webp 1520w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/eb0ee/23-11-25-14.webp 2458w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/8dfcc6ee72bcb01c45f57e55fe02d77f/a2d4f/23-11-25-14.png 190w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/3f520/23-11-25-14.png 380w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/3c051/23-11-25-14.png 760w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/b5cea/23-11-25-14.png 1140w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/891d5/23-11-25-14.png 1520w,\n/static/8dfcc6ee72bcb01c45f57e55fe02d77f/0330c/23-11-25-14.png 2458w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/8dfcc6ee72bcb01c45f57e55fe02d77f/3c051/23-11-25-14.png\"\n            alt=\"23-11-25-4\"\n            title=\"23-11-25-4\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>결과적으로 50%의 fully-supervised data만 가지고도 fully-supervised method 성능에 도달하였습니다. (다만, 오해하면 안되는게, 나머지 50%는 point supervision 추가적으로 제공한 것입니다). 또한 5%만 가지고도 기존 semi-supervised SOTA를 훨씬 뛰어넘었습니다.</p>\n<h3 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h3>\n<ul>\n<li>Boykov, Yuri Y., and M-P. Jolly. \"Interactive graph cuts for optimal boundary &#x26; region segmentation of objects in ND images.\" <em>Proceedings eighth IEEE international conference on computer vision. ICCV 2001</em>. Vol. 1. IEEE, 2001.</li>\n<li>Rother, Carsten, Vladimir Kolmogorov, and Andrew Blake. \"\" GrabCut\" interactive foreground extraction using iterated graph cuts.\" <em>ACM transactions on graphics (TOG)</em> 23.3 (2004): 309-314.</li>\n<li>Pablo Arbeláez, et al. \"Multiscale combinatorial grouping.\" <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2014.</li>\n<li>Bharath Hariharan, et al. \"Simultaneous detection and segmentation.\" <em>Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13</em>. Springer International Publishing, 2014.</li>\n<li>Kaiming He, et al. \"Mask r-cnn.\" <em>Proceedings of the IEEE international conference on computer vision</em>. 2017.</li>\n<li>Zhi Tian, Chunhua Shen, and Hao Chen. \"Conditional convolutions for instance segmentation.\" <em>Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16</em>. Springer International Publishing, 2020.</li>\n<li>Jifeng Dai, Kaiming He, and Jian Sun. \"Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation.\" <em>Proceedings of the IEEE international conference on computer vision</em>. 2015.</li>\n<li>Viveka Kulharia, et al. \"Box2seg: Attention weighted loss and discriminative feature learning for weakly supervised segmentation.\" <em>European Conference on Computer Vision</em>. Cham: Springer International Publishing, 2020.</li>\n<li>Zhi Tian, et al. \"Boxinst: High-performance instance segmentation with box annotations.\" <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2021.</li>\n<li>Wentong Li, et al. \"Box-supervised instance segmentation with level set evolution.\" <em>European conference on computer vision</em>. Cham: Springer Nature Switzerland, 2022.</li>\n<li>Bowen Cheng, Omkar Parkhi, and Alexander Kirillov. \"Pointly-supervised instance segmentation.\" <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2022.</li>\n<li>Tianheng Cheng, et al. \"Boxteacher: Exploring high-quality pseudo labels for weakly supervised instance segmentation.\" <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2023.</li>\n<li>Ruihuang Li, et al. \"SIM: Semantic-aware Instance Mask Generation for Box-Supervised Instance Segmentation.\" <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2023.</li>\n<li>Beomyoung Kim, et al. \"The Devil is in the Points: Weakly Semi-Supervised Instance Segmentation via Point-Guided Mask Representation.\" <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2023.</li>\n</ul>","tableOfContents":"<ul>\n<li><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#introduction\">Introduction</a></li>\n<li>\n<p><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#instance-segmentation\">Instance Segmentation</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#unsupervised-methods\">Unsupervised Methods</a></li>\n<li><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#deep-instance-segmentation\">Deep Instance Segmentation</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#weakly-supervised-instance-segmentation\">Weakly-Supervised Instance Segmentation</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#box-supervised-methods\">Box-Supervised Methods</a></li>\n<li><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#box--point-supervised-methods\">Box + Point-Supervised Methods</a></li>\n<li><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#point-supervised-methods\">Point-Supervised Methods</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#semi-supervised-methods-w-weak-supervision\">Semi-Supervised Methods w. Weak-Supervision</a></li>\n<li><a href=\"/MachineLearning/23-11-25-Weakly-Supervised%20Instance%20Segmentation/#references\">References</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/23-11-25/","title":"Weakly-Supervised Instance Segmentation","category":"Deep Learning","date":"2023-11-25"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}