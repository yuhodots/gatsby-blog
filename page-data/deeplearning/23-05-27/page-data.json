{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/23-05-27/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>ViT 관련 알고리즘들을 모아 정리합니다. 알고리즘은 시간 순으로 나열하였습니다. 포스팅의 제목은 Variants of Vision Transformer이나, 구조적 차이를 갖는 논문 외에도 동일한 구조에서 학습 방법만 차이를 갖는 논문들도 기록하였습니다. </p>\n</blockquote>\n<h3 id=\"vision-transformer\" style=\"position:relative;\"><a href=\"#vision-transformer\" aria-label=\"vision transformer permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Vision Transformer</h3>\n<p>Alexey Dosovitskiy, et al. “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.” (Oct 2020 / ICLR2021)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/f64510be344af58225dec98a359991f1/b0bb3/ViT.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 40%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABLklEQVQoz4VRTU+DQBDt/z960nj3pImapj0oStIP00KhlA9NZcuyFBLAtAmNCqblCUtrUtLWl0zezs682ZmdBrbI85xbCV8TsPBekX0lSJMY1W2+l1PX7LhxKEifLhHoIqjUAu1fo45DhXfYK4gysTh/BhZ+khBRFMH3XM5BEPxZGFaxko92WH+Jj16IXeaBui4mug7TNDkTQsA8xnm9Xh8fuUSapvDnHpbLBXw2hyYrMDUD1sTg/GZaMMYT6KoGMn3fav8pqBZCmzjIsgwxY/DGI8zkAWxZhqMosCUJZKQgtu1qWac7/MbVbQd3osV9MmO4aQ3QFDS8dHt4Fvt4ELp4bN+j1xGx2WxOj7xarXBxfoZmu8X9KP7AUHegTkM4DsWMUmiGjqHc539bX8ovjZJcLjssPsoAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/f64510be344af58225dec98a359991f1/15813/ViT.webp 190w,\n/static/f64510be344af58225dec98a359991f1/1cdb2/ViT.webp 380w,\n/static/f64510be344af58225dec98a359991f1/9046c/ViT.webp 760w,\n/static/f64510be344af58225dec98a359991f1/c89f9/ViT.webp 1140w,\n/static/f64510be344af58225dec98a359991f1/7afe4/ViT.webp 1520w,\n/static/f64510be344af58225dec98a359991f1/64607/ViT.webp 2344w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/f64510be344af58225dec98a359991f1/a2d4f/ViT.png 190w,\n/static/f64510be344af58225dec98a359991f1/3f520/ViT.png 380w,\n/static/f64510be344af58225dec98a359991f1/3c051/ViT.png 760w,\n/static/f64510be344af58225dec98a359991f1/b5cea/ViT.png 1140w,\n/static/f64510be344af58225dec98a359991f1/891d5/ViT.png 1520w,\n/static/f64510be344af58225dec98a359991f1/b0bb3/ViT.png 2344w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/f64510be344af58225dec98a359991f1/3c051/ViT.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Alexey Dosovitskiy, et al.</i></p></center>\n<ol>\n<li>전체 이미지를 16x16 size의 patch로 절단. 예를 들어 48 x 48 이미지라면 9개의 patch로 나뉨: <code class=\"language-text\">ViT-Base/16</code> model에서 16이 의미하는 것이 patch size임</li>\n<li>각각의 patch를 평탄화(16x16x3 = 768) 한 뒤에, 워드 임베딩을 만드는 것처럼 linear projection을 통해 embedding vector의 형태로 변환</li>\n<li>\n<p>해당 embedding vector에, BERT 처럼 CLS 토큰과 positional embedding을 추가함. 이 둘은 learnable parameter 임</p>\n<ul>\n<li>CLS 토큰은 patch embedding과 동일한 사이즈이며, 9개의 patch embedding들과 concat 함</li>\n<li>position embedding은 총 9 + 1 = 10개로, patch embedding에 합(+)해주는 형태로 구성</li>\n</ul>\n</li>\n<li>각 embedding vector를 하나의 워드 토큰처럼여겨, transformer의 입력으로 전달</li>\n<li>BERT 처럼, CLS 토큰의 최종 출력이 해당 이미지의 output class라고 가정. 따라서 transformer 출력 단에서 CLS의 embedding이 어떻게 변했는지 확인하여 inference 수행</li>\n</ol>\n<h3 id=\"deit\" style=\"position:relative;\"><a href=\"#deit\" aria-label=\"deit permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DeiT</h3>\n<p>Hugo Touvron, et al., \"Training data-efficient image transformers &#x26; distillation through attention.\" (Dec 2020, ICML 2021)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/bc73995310b6fe3781e674cd2acaa8e4/1bba3/Deit.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.94736842105264%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA00lEQVQoz52SUa/CIAyF/f//0BejwkYoZXA3r4ls7FhQE703PgySpodT8qVQdviz1nWteV4WEIcPL+cMYxmL1N7997X7BrzdEk49/6sdtRNg3g6c5xlhGDCOI2KIiDFiEh3CUGubgeV614vFdTKYoq5RdPFeZzYCFzhWIDqC3OkRop1TrcAM7zswnyUrCf3UXXuHBWDtAVrtofUeJLp4zR3+XH7BIUAbA90beBlM8ZqAZZLWkrwZo+v6GuSc7F3blMvnJaLaacnes4ASlFJIKX0F3gGy1RACMYMXCQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/bc73995310b6fe3781e674cd2acaa8e4/15813/Deit.webp 190w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/1cdb2/Deit.webp 380w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/9046c/Deit.webp 760w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/c89f9/Deit.webp 1140w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/b68e6/Deit.webp 1433w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/bc73995310b6fe3781e674cd2acaa8e4/a2d4f/Deit.png 190w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/3f520/Deit.png 380w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/3c051/Deit.png 760w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/b5cea/Deit.png 1140w,\n/static/bc73995310b6fe3781e674cd2acaa8e4/1bba3/Deit.png 1433w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/bc73995310b6fe3781e674cd2acaa8e4/3c051/Deit.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<ul>\n<li>ViT는 CNN에 비해 inductive bias가 적은 구조를 가지지만 그에 따라 매우 많은 양의 학습 데이터를 요구함</li>\n<li>따라서 DeiT 논문에서는 ViT 구조는 그대로 유지하고, transformer에 적합한 teacher-student 방식인 distillation token을 추가하여 빠른 수렴 가능하게 했음</li>\n<li>\n<p>Distillation token: ViT에서 CLS token과 더불어 distillation token이라는 것을 추가하고, teacher model의 output을 distillation token의 target으로 사용</p>\n<ul>\n<li>CNN 기반 model을 teacher로 사용했을 때 성능 좋음</li>\n</ul>\n</li>\n</ul>\n<h3 id=\"swin-transformer\" style=\"position:relative;\"><a href=\"#swin-transformer\" aria-label=\"swin transformer permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Swin Transformer</h3>\n<p>Ze Liu, et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\" (March 2021, ICCV 2021)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/77599310dfe08c588bac0ce6e3bcb560/11f80/SwinTransformer.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 48.42105263157895%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsTAAALEwEAmpwYAAAB0ElEQVQoz2WT246bMBCG9/1fpxetqu5Nq666oa1EE3KABEjAIeEQTuEYzL8zzu6m0VpYHo093/zjMQ+gMcoRZXlGkRfwfR993yMnu65q2hzpG1GVBc55DjkMqMsSeZpioHO8ryZzaH1g4yhsGLoGsXXQNC2K0xFzfQLPWaPvelwuPUz9L7amhXAfYP1nAnexgLDWOIUhyiy7BwY7C7PJD/g2AfoL0sjHcjqhBBsFlEOP2a9HPD9+JtA/rLXvePr6CUvtiSrJUVXVDSiHCxa/f0L79gWb2VQBs1iQ4md47lXhOA7wbALNNYSBDbGZwjImOLhLdT3Xgl+BbVNj7znwSc0piZXz0rfI0gRd29wOS6As6J6LUimqzpWysyxVMXcl/z+klG93TPa1IeyTcngNkojiDMcoVY3b+aFS+QGYUtfiOKasBZIkIYXpu4/videQGpBTA4yNjaXjEjSCZszRtu0NWNc1XNeFYRgwTROHw0HZ/Hxs28ZqtVIg9gkhSF2MHQH3Ww9JFGMzX90Du65TEA50HEcFWZalfJyIfUEQqGS8x+otOuuSPyaFM10HMz6UzFlY7dvkYC6zaRrVBF558uMXewERCASU1Nm59FOU78AXcI/zlamco9gAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/77599310dfe08c588bac0ce6e3bcb560/15813/SwinTransformer.webp 190w,\n/static/77599310dfe08c588bac0ce6e3bcb560/1cdb2/SwinTransformer.webp 380w,\n/static/77599310dfe08c588bac0ce6e3bcb560/9046c/SwinTransformer.webp 760w,\n/static/77599310dfe08c588bac0ce6e3bcb560/c89f9/SwinTransformer.webp 1140w,\n/static/77599310dfe08c588bac0ce6e3bcb560/7afe4/SwinTransformer.webp 1520w,\n/static/77599310dfe08c588bac0ce6e3bcb560/37d62/SwinTransformer.webp 2498w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/77599310dfe08c588bac0ce6e3bcb560/a2d4f/SwinTransformer.png 190w,\n/static/77599310dfe08c588bac0ce6e3bcb560/3f520/SwinTransformer.png 380w,\n/static/77599310dfe08c588bac0ce6e3bcb560/3c051/SwinTransformer.png 760w,\n/static/77599310dfe08c588bac0ce6e3bcb560/b5cea/SwinTransformer.png 1140w,\n/static/77599310dfe08c588bac0ce6e3bcb560/891d5/SwinTransformer.png 1520w,\n/static/77599310dfe08c588bac0ce6e3bcb560/11f80/SwinTransformer.png 2498w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/77599310dfe08c588bac0ce6e3bcb560/3c051/SwinTransformer.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Ze Liu, et al. </i></p></center>\n<ul>\n<li>기존 ViT는 이미지를 위한 특성이 ViT에 제대로 적용되지 않음. 즉, 해상도(resolution)과 물체의 크기(scale)가 이미지에 따라 달라지므로 이를 고려한 모델링 필요함</li>\n<li>\n<p>따라서 local window &#x26; patch merging이라는 inductive bias를 ViT에 적용</p>\n<ul>\n<li>기존 ViT는 모든 layer가 16x16x3의 패치(의 임베딩)를 동일하게 입력으로 받았다면, Swin Transformer는 처음에는 4x4x3 패치부터 시작해서, layer를 거칠 때 마다 window의 사이즈가 점점 커지는 형태</li>\n<li>Swin Transformer block: window-based multi-head self-attention(W-MSA)와 shifted window multi-head self-attention(SW-MSA) 존재</li>\n<li>Swin Transformer block을 구현하기 위해 efficient batch computation, relative position bias 등의 방법들 사용하는데 자세한 내용 논문 참고</li>\n</ul>\n</li>\n<li>Layer 계층에 따라 다른 해상도의 결과를 얻을 수 있기 때문에 detection, segmentation task에 활용도 높음. 관련하여 muti-resolution model인 FPN과 비교해보면 좋을듯</li>\n</ul>\n<h3 id=\"dino\" style=\"position:relative;\"><a href=\"#dino\" aria-label=\"dino permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>DINO</h3>\n<p>Mathilde Caron, et al. “Emerging properties in self-supervised vision transformers.” (Apr 2021, ICCV 2021)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/5496c/DINO.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 46.8421052631579%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsTAAALEwEAmpwYAAABAUlEQVQoz5WSW2vCQBCF/f+/R2NRsM/SNwuBQIIpmuZ+J/fL6c7gljRNqx5YdpgdPs7M7AoTjePIh9Q0DeI45tjzPDiOw3Gapvwm6+dazROyqG1bhGHIcZZliKKIYwInSfLLwCJQPtZ1Ddu2YZ7P31DpTtd1mKaJsiwXXS4C6SZXBJXtkcilpmkwDOM5YF1V8MXcPNdFLCDkuBK56+UCVVXxfjohujl/COiKOb0oCrabDV4PB5RFgTzP8XY8Yr/bQVmv8SHaJg3DcB/Y9z1v2LIsFAImRc4/RS4IAnRiaXcdzqGB73Or1LI89IXaP2D/LgW3uOs6/iahcDVd0I/aib4Afi68b7A6ycsAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/15813/DINO.webp 190w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/1cdb2/DINO.webp 380w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/9046c/DINO.webp 760w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/c89f9/DINO.webp 1140w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/7afe4/DINO.webp 1520w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/04951/DINO.webp 2452w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/a2d4f/DINO.png 190w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/3f520/DINO.png 380w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/3c051/DINO.png 760w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/b5cea/DINO.png 1140w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/891d5/DINO.png 1520w,\n/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/5496c/DINO.png 2452w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/af8d5b7ce77c04e5245f6c2ee86ecdc3/3c051/DINO.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Mathilde Caron, et al. </i></p></center>\n<ul>\n<li>\n<p>Self-supervised ViT에 대해 특징 발견</p>\n<ol>\n<li>Self-supervised ViT feature들은 그 자체로 scene layout and object boundaries 같은 정보를 지니고 있음</li>\n<li>Finetuning, linear classifier, data augmentation 없이도 좋은 kNN classifier 성능 보임</li>\n</ol>\n</li>\n<li>\n<p>아래의 방법 활용해서 self-supervised ViT 만듦</p>\n<ol>\n<li>ViT를 <a href=\"https://yuhodots.github.io/deeplearning/21-04-04/\">BYOL</a>의 manner로 학습. 즉, momentum update 활용함</li>\n<li>BYOL과 달리 normalized embedding의 L2 distance를 사용하지는 않고, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>𝑝</mi><mn>2</mn></msub><mi>log</mi><mo>⁡</mo><msub><mi>𝑝</mi><mn>1</mn></msub></mrow><annotation encoding=\"application/x-tex\">𝑝_2\\log𝑝_1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\">lo<span style=\"margin-right:0.01389em;\">g</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">p</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.30110799999999993em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span> 형태의 cross-entropy loss 사용</li>\n<li>Momentum teacher output의 centering, sharpening 만으로 collapse 방어</li>\n</ol>\n</li>\n</ul>\n<h3 id=\"masked-autoencoder-mae\" style=\"position:relative;\"><a href=\"#masked-autoencoder-mae\" aria-label=\"masked autoencoder mae permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Masked AutoEncoder (MAE)</h3>\n<p>Kaiming He, et al. \"Masked autoencoders are scalable vision learners.\" (Nov 2021, CVPR 2022)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/66580287771f37112f8c93bd3d42ee45/16529/MAE.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.368421052631575%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABWklEQVQoz2VRTUsCURT1L7XtL7Sthf2Mdm3aNRtzGUWFSRAVGQyIpKAIpWGaypik4pBGJJOOT2d0/Gg+Tu/NqDl24c47b969h3Pv8ViWBZYs2Gkaho0HwyEGmmZj9qfdJVDHE1g9gul7HazD1HVXL0sPZjF/mEf9+QHFCI+2OkSlp0CSZQymP0CfEoo1V+1yr8dWZZr2pdkQwQePUMim0chlkA3d4KVSRUkm+Ja7kEgPY6kF5a2Mi+AZAj4OYtUhZxy2QvYxZmMm+FvsejdwfehHOszjLnCKx1wepY6ML5rx1BPyiThahRx825s4WF+DEAk7hJRjQThXKL4W4d/fQ4yOKkTDSJ6foFCrU0KqTlHxSVUqH01olTKOOQ473i1kkgk34eoedN1Rm4reI3Z1iRYdU6BEfW2EiUlrRhr0tuTULhni2uGfU+YCd6gJhBAbT+gEqy7PFvfP5V8AKQWH12SEhwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/66580287771f37112f8c93bd3d42ee45/15813/MAE.webp 190w,\n/static/66580287771f37112f8c93bd3d42ee45/1cdb2/MAE.webp 380w,\n/static/66580287771f37112f8c93bd3d42ee45/9046c/MAE.webp 760w,\n/static/66580287771f37112f8c93bd3d42ee45/c89f9/MAE.webp 1140w,\n/static/66580287771f37112f8c93bd3d42ee45/7afe4/MAE.webp 1520w,\n/static/66580287771f37112f8c93bd3d42ee45/7da0b/MAE.webp 2506w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/66580287771f37112f8c93bd3d42ee45/a2d4f/MAE.png 190w,\n/static/66580287771f37112f8c93bd3d42ee45/3f520/MAE.png 380w,\n/static/66580287771f37112f8c93bd3d42ee45/3c051/MAE.png 760w,\n/static/66580287771f37112f8c93bd3d42ee45/b5cea/MAE.png 1140w,\n/static/66580287771f37112f8c93bd3d42ee45/891d5/MAE.png 1520w,\n/static/66580287771f37112f8c93bd3d42ee45/16529/MAE.png 2506w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/66580287771f37112f8c93bd3d42ee45/3c051/MAE.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Kaiming He, et al. </i></p></center>\n<ul>\n<li>Masked image modeling (MIM) task</li>\n<li>\n<p>기존의 ViT 학습 방식보다는, 마치 BERT 처럼 masked patch를 복원하는 방식으로 SSL pre-training 수행 후에 downstream task(e.g., classification)을 푸는 것이 더 좋음. 물론 CNN으로도 가능하지만 ViT일 때 더 좋음</p>\n<ul>\n<li>약 75%정도를 masking하고, input에 masked patch는 넣어주지 않아 학습 빠름</li>\n<li>Masked patch에 대해서만 reconstruction loss 부여</li>\n</ul>\n</li>\n<li>유사한 연구로는 BEiT가 있음. BEiT는 dVAE 기반의 image tokenizer를 사용하여 masked token을 예측하는 방식으로, BERT와 거의 유사함</li>\n</ul>\n<h3 id=\"masked-siamese-networks-msn\" style=\"position:relative;\"><a href=\"#masked-siamese-networks-msn\" aria-label=\"masked siamese networks msn permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Masked Siamese Networks (MSN)</h3>\n<p>Mahmoud Assran, et al. \"Masked siamese networks for label-efficient learning.\" (Apr 2022, ECCV 2022)</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6d18191c92138eac275f2ef70f363711/1cb21/MSN.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 39.473684210526315%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABt0lEQVQoz22SSWtTURTH8/X8BFpw7dIibvQLdOHCqUo3ihunYhaCihasjU0xphVLUypY0iZmaN/ryxvy5vnneTemuvAPB+7h3Ps7060hKstSWaU4zUnSUqwgjBKiOGWuoig4m1g4rqf8IIzwg1DuRRKbva8pmFwULFmW0u1+o73zln7/B6fakNH4mKkX4Pohnh9hTz3le0EkwBjTcTEsh3lhNf5RWRbcvLHI40dLbDRe09xssNdpMdZNDntjxpqJbthoYiNtwtGvE5afrPJ9/+efDgSYJiHbrTovX91i/+ALL1brbDU/op+dsrvbwfN84iRTVVUjsBxPVVf59tTn8HigzhVMVahpIy5dvMDy/eu8/7DC/5TnubQbqLNhTQXqCjQ4j0dxLElns67ZjsXitSvcvnOVVvsN7iDA6OrE04jJ0YTICtXFUBZQqarOk3lWy6iUZhmJwDJJqoDzLFkek4UpO8++svZ8jUF7wOenn9jb7BBEMb2hLtv1cWUEofim7UjLLutb2wxP9L9LmX2Z+VagUd/g8sIC/U6Pe0t3WXnwUIVM21VtVe1nWa4eV9B3600M0z4H/gY8iVFW+UCzNQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/6d18191c92138eac275f2ef70f363711/15813/MSN.webp 190w,\n/static/6d18191c92138eac275f2ef70f363711/1cdb2/MSN.webp 380w,\n/static/6d18191c92138eac275f2ef70f363711/9046c/MSN.webp 760w,\n/static/6d18191c92138eac275f2ef70f363711/c89f9/MSN.webp 1140w,\n/static/6d18191c92138eac275f2ef70f363711/7afe4/MSN.webp 1520w,\n/static/6d18191c92138eac275f2ef70f363711/36d21/MSN.webp 2586w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/6d18191c92138eac275f2ef70f363711/a2d4f/MSN.png 190w,\n/static/6d18191c92138eac275f2ef70f363711/3f520/MSN.png 380w,\n/static/6d18191c92138eac275f2ef70f363711/3c051/MSN.png 760w,\n/static/6d18191c92138eac275f2ef70f363711/b5cea/MSN.png 1140w,\n/static/6d18191c92138eac275f2ef70f363711/891d5/MSN.png 1520w,\n/static/6d18191c92138eac275f2ef70f363711/1cb21/MSN.png 2586w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/6d18191c92138eac275f2ef70f363711/3c051/MSN.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<center><p><i>Taken from Mahmoud Assran, et al. </i></p></center>\n<ul>\n<li>\n<p>Siamese Network: \"<a href=\"https://www.cs.cmu.edu/~rsalakhu/papers/oneshot1.pdf\">Siamese Neural Networks for One-shot Image Recognition</a>\"</p>\n<ul>\n<li>동일한 이미지에 대한 다른 view가, 유사한 representation을 가지도록 학습</li>\n<li>두 입력 이미지에 대해 동일한 모델을 사용 (shared weights)</li>\n<li>다만 collapse 발생할 수 있어 최근 연구들 triplet이나 contrastive loss 활용</li>\n</ul>\n</li>\n<li>\n<p>MAE patch reconstruction의 문제점</p>\n<ul>\n<li>MAE의 reconstruction loss는 단순 classification task를 푸는데에도 너무 디테일한 low-level image modeling 요구</li>\n<li>이러한 특징이 low-shot fine-tuning에서 over-fitting을 유발</li>\n</ul>\n</li>\n<li>\n<p>Masked Siamese Networks</p>\n<ul>\n<li>Masekd anchor view와 unmasked target view가 유사한 output 확률 분포 가지도록 학습</li>\n<li>Prototype는 learnable parameter인데, cluster assignment(for 확률 분포 출력)를 위해 사용. Class 개수 모른다고 가정하기 때문에 prototype 개수는 하이퍼파라미터</li>\n</ul>\n</li>\n</ul>","tableOfContents":"<ul>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#vision-transformer\">Vision Transformer</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#deit\">DeiT</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#swin-transformer\">Swin Transformer</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#dino\">DINO</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#masked-autoencoder-mae\">Masked AutoEncoder (MAE)</a></li>\n<li><a href=\"/MachineLearning/23-05-27-Variants%20of%20Vision%20Transformer/#masked-siamese-networks-msn\">Masked Siamese Networks (MSN)</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/23-05-27/","title":"Variants of Vision Transformer","category":"Deep Learning","date":"2023-05-27"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}