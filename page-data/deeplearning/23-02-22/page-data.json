{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/23-02-22/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>CVPR 2021 학회에서 발표된 'BoxInst: High-performance instance segmentation with box annotations' 논문을 정리합니다. 해당 논문은 mask-level annotation이 아닌 bounding box annotation 만으로도 instance segmentation 모델을 학습시킬 수 있는 projection loss와 pairwise affinity loss라는 두 가지 loss를 제안합니다.</p>\n</blockquote>\n<p><code class=\"language-text\">아직 작성이 완료되지 않은 포스팅입니다.</code></p>\n<h3 id=\"segmentation-tasks\" style=\"position:relative;\"><a href=\"#segmentation-tasks\" aria-label=\"segmentation tasks permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Segmentation Tasks</h3>\n<p>설명은 한줄씩만 적고, 나머지는 사진으로 제공</p>\n<ul>\n<li>Semantic Segmentation</li>\n<li>Instance Segmentation</li>\n<li>panoptic segmentation</li>\n<li>point cloud (3d) semantic segmentation</li>\n</ul>\n<p>최근, 속도 측면에서 detector FCOS나 CondInst 기반의 알고리즘들의 instance segmentation 속도가 bounding-box object detection 만큼 빠르기 때문에 굳이 object detec하는 경우에도 instance segmentation 쓰는 경우 늘어나고 있다고 함. 따라서 pixel-level mask annotation이라는 작업만 없다면 완전히 대체될 수도 있지 않을까 싶음 (weakly supervised instance segmentation의 중요성)</p>\n<h3 id=\"weakly-supervised-image-segmentation\" style=\"position:relative;\"><a href=\"#weakly-supervised-image-segmentation\" aria-label=\"weakly supervised image segmentation permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Weakly-Supervised Image Segmentation</h3>\n<p>설명은 한줄씩만 적고, 나머지는 사진으로 제공하기</p>\n<ul>\n<li>Image-level class label</li>\n<li>Bounding boxes</li>\n<li>Scribbles</li>\n</ul>\n<h5 id=\"explanation-by-chatgpt\" style=\"position:relative;\"><a href=\"#explanation-by-chatgpt\" aria-label=\"explanation by chatgpt permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Explanation by ChatGPT</h5>\n<p>Weakly supervised instance segmentation is a type of image segmentation where only partial or weak supervision is available during training. In this approach, the labeling information is not precise, and only some information about the location or presence of instances is provided.</p>\n<p>To deal with this type of weak supervision, different types of labels can be used, including:</p>\n<ol>\n<li>Image-level labels: This type of label provides information about the presence or absence of instances within an image. These labels are usually obtained by labeling the entire image rather than individual objects within it.</li>\n<li>Scribble or bounding box labels: These labels provide more precise information about the location of objects within the image. Scribble labels are rough annotations made by drawing a rough outline around an object, while bounding box labels define the approximate rectangular boundaries of an object.</li>\n<li>Point or dot labels: Point or dot labels are similar to scribble labels, but instead of drawing an outline, a single point or dot is used to indicate the presence of an object.</li>\n<li>Partial or incomplete labels: These labels indicate that only part of the object is visible in the image. These types of labels are useful when the object is occluded or only partially visible.</li>\n<li>Noisy labels: In some cases, the labels provided may be noisy or incorrect, and this can be addressed using techniques like label smoothing or data augmentation.</li>\n</ol>\n<p>The choice of label type depends on the nature of the data and the specific requirements of the application. In general, more precise labels lead to better performance, but they also require more annotation effort.</p>\n<h3 id=\"preliminaries\" style=\"position:relative;\"><a href=\"#preliminaries\" aria-label=\"preliminaries permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Preliminaries</h3>\n<h5 id=\"dynamic-mask-heads\" style=\"position:relative;\"><a href=\"#dynamic-mask-heads\" aria-label=\"dynamic mask heads permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Dynamic Mask Heads</h5>\n<ul>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h5 id=\"multiscale-combinatorial-grouping-mcg\" style=\"position:relative;\"><a href=\"#multiscale-combinatorial-grouping-mcg\" aria-label=\"multiscale combinatorial grouping mcg permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Multiscale Combinatorial Grouping (MCG)</h5>\n<ul>\n<li>non-deep learning based model</li>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h5 id=\"grabcut-and-grabcut\" style=\"position:relative;\"><a href=\"#grabcut-and-grabcut\" aria-label=\"grabcut and grabcut permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>GrabCut and GrabCut+</h5>\n<ul>\n<li>non-deep learning based model</li>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h5 id=\"conditional-random-field-crf\" style=\"position:relative;\"><a href=\"#conditional-random-field-crf\" aria-label=\"conditional random field crf permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conditional Random Field (CRF)</h5>\n<ul>\n<li>non-deep learning based model</li>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h5 id=\"mask-r-cnn\" style=\"position:relative;\"><a href=\"#mask-r-cnn\" aria-label=\"mask r cnn permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Mask R-CNN</h5>\n<ul>\n<li>내용을 추가하고 있습니다.</li>\n<li>Fixed mask head(Mask R-CNN)와 dynamic filter(CondInst)의 차이점 서술</li>\n</ul>\n<h5 id=\"region-of-interest-roi\" style=\"position:relative;\"><a href=\"#region-of-interest-roi\" aria-label=\"region of interest roi permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Region of Interest (RoI)</h5>\n<ul>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h3 id=\"prior-works\" style=\"position:relative;\"><a href=\"#prior-works\" aria-label=\"prior works permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Prior Works</h3>\n<h5 id=\"boxsup-and-box2seg\" style=\"position:relative;\"><a href=\"#boxsup-and-box2seg\" aria-label=\"boxsup and box2seg permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BoxSup and Box2Seg</h5>\n<ul>\n<li>BoxSup은 MCG의 region proposal를, Box2Seg는 GrabCut으로 만들어낸 mask를 pseudo label로 사용</li>\n<li>하지만 이런 방법들이 modern GPU로 병렬화되기 힘들기 때문에 학습에 매우 오랜 시간 소요되고, 이중 몇몇은 iterative training이 요구되므로 하이퍼파라미터 튜닝도 더 많이 필요하다</li>\n<li>그리고 제일 중요한 것은, 이들 중에 large benchmark인 COCO 같은 데이터셋에 좋은 성능을 보이는 논문은 아무것도 없다. 작은 benchmark인 Pascal VOC에 대해서만 평가</li>\n</ul>\n<h5 id=\"sdi-and-bbtp\" style=\"position:relative;\"><a href=\"#sdi-and-bbtp\" aria-label=\"sdi and bbtp permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>SDI and BBTP</h5>\n<ul>\n<li>SDI: Simple Does It: Weakly Supervised Instance and Semantic Segmentation</li>\n<li>SDI가 box annotation으로 instance segmentation 풀었던 첫 논문. 이 논문 또한 MCG의 region proposal에 의존</li>\n<li>BBTP: Weakly Supervised Instance Segmentation Using the Bounding Box Tightness Prior</li>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h3 id=\"boxinst\" style=\"position:relative;\"><a href=\"#boxinst\" aria-label=\"boxinst permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>BoxInst</h3>\n<h5 id=\"conditional-convolutions-for-instance-segmentation-condinst\" style=\"position:relative;\"><a href=\"#conditional-convolutions-for-instance-segmentation-condinst\" aria-label=\"conditional convolutions for instance segmentation condinst permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Conditional Convolutions for Instance Segmentation (CondInst)</h5>\n<ul>\n<li>Mask R-CNN과의 차이점 설명 (Dynamic filter에 대해 설명)</li>\n<li>어떻게해서 RoI free method가 되었는지 설명</li>\n<li>Full-image mask segmentation이 가능하다는 특징이 BoxInst에 어떤 도움을 주었는지 설명</li>\n<li>최종적으로, CondInst의 어떤 losses 들이 있었고, 이것이 BoxInst로 발전되면서 어떻게 변했는지 설명</li>\n<li>Dice loss의 장점에 대해 설명</li>\n<li>BoxInst의 core idea: CondInst 내의 pixel-wise mask losses를 projection loss와 pairwise affinity loss로 대체하는 것</li>\n</ul>\n<h5 id=\"projection-loss\" style=\"position:relative;\"><a href=\"#projection-loss\" aria-label=\"projection loss permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Projection Loss</h5>\n<ul>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h5 id=\"pairwise-affinity-loss\" style=\"position:relative;\"><a href=\"#pairwise-affinity-loss\" aria-label=\"pairwise affinity loss permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Pairwise Affinity Loss</h5>\n<ul>\n<li>Pairwise Affinity Loss 식에 대한 설명 (Mask annotation이 있다는 가정하에)</li>\n<li>그렇다면 mask annotation이 없는 경우에는 어떻게 학습하는가? Learning without Mask Annotations 파트 설명</li>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h3 id=\"experiments\" style=\"position:relative;\"><a href=\"#experiments\" aria-label=\"experiments permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments</h3>\n<h5 id=\"evaluation-metric\" style=\"position:relative;\"><a href=\"#evaluation-metric\" aria-label=\"evaluation metric permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Evaluation Metric</h5>\n<ul>\n<li>Object detection과 Image Segmentation에서 사용되는 evaluation metric에 대해 소개합니다.</li>\n<li>Intersection of Union (IoU)</li>\n<li>Precision and Recall</li>\n<li>mean Average Precision (mAP)</li>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h5 id=\"experimental-results\" style=\"position:relative;\"><a href=\"#experimental-results\" aria-label=\"experimental results permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experimental Results</h5>\n<ul>\n<li>PolarMask나 YOLACK 같은 mask annotation 기반의 모델들 보다 성능이 좋다는 것은 인상적</li>\n<li>내용을 추가하고 있습니다.</li>\n</ul>\n<h3 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h3>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\">\n<p>Tian, Zhi, et al. \"BoxInst: High-performance instance segmentation with box annotations.\" <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2021.</p>\n<a href=\"#fnref-1\" class=\"footnote-backref\">↩</a>\n</li>\n</ol>\n</div>","tableOfContents":"<ul>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#segmentation-tasks\">Segmentation Tasks</a></li>\n<li>\n<p><a href=\"/MachineLearning/23-02-22-BoxInst/#weakly-supervised-image-segmentation\">Weakly-Supervised Image Segmentation</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#explanation-by-chatgpt\">Explanation by ChatGPT</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/23-02-22-BoxInst/#preliminaries\">Preliminaries</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#dynamic-mask-heads\">Dynamic Mask Heads</a></li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#multiscale-combinatorial-grouping-mcg\">Multiscale Combinatorial Grouping (MCG)</a></li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#grabcut-and-grabcut\">GrabCut and GrabCut+</a></li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#conditional-random-field-crf\">Conditional Random Field (CRF)</a></li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#mask-r-cnn\">Mask R-CNN</a></li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#region-of-interest-roi\">Region of Interest (RoI)</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/23-02-22-BoxInst/#prior-works\">Prior Works</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#boxsup-and-box2seg\">BoxSup and Box2Seg</a></li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#sdi-and-bbtp\">SDI and BBTP</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/23-02-22-BoxInst/#boxinst\">BoxInst</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#conditional-convolutions-for-instance-segmentation-condinst\">Conditional Convolutions for Instance Segmentation (CondInst)</a></li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#projection-loss\">Projection Loss</a></li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#pairwise-affinity-loss\">Pairwise Affinity Loss</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/23-02-22-BoxInst/#experiments\">Experiments</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#evaluation-metric\">Evaluation Metric</a></li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#experimental-results\">Experimental Results</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/23-02-22-BoxInst/#references\">References</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/23-02-22/","title":"BoxInst: Instance Segmentation with Box Annotations","category":"Deep Learning","date":"2023-02-22"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}