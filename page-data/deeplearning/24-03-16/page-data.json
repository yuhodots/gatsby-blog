{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/24-03-16/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>ICLR 2023 Spotlight으로 선정된 Vision Transformer Adapter for Dense Predictions 논문을 정리합니다.</p>\n</blockquote>\n<h3 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h3>\n<p>ViT가 고안된 이후로 여러 vision task에서도 transformer 구조를 적극적으로 활용하고 있습니다. 하지만 object detection이나 image segmentation 같은 dense prediction task에서는 plain(vanilla) ViT가 성능이 그렇게 좋지는 않습니다. 왜냐하면 dense prediction task에 대해서는 multi-resolution feature와 spatial feature를 잘 뽑아내는 것이 중요한데, ViT는 image-related prior knowledge가 따로 없기 때문입니다. 그래서 일반적으로 vision-specific task에서는 SwinT와 같은 image-related inductive bias가 적용된 모델을 주로 사용하곤 합니다.</p>\n<p>하지만 이런 SwinT 부류는 transfer learning시 전체 모델을 fine-tuning 해야한다는 단점과, pre-training 단계에서도 SwinT 기반으로 pre-training된 weight만 사용해야 한다는 한계가 존재합니다. 따라서 본 논문의 저자들은 vision-specific task의 성능을 높이면서도 ViT의 flexibility를 그대로 유지하는 ViT Adapter 라는 구조를 제안합니다. ViT 구조를 그대로 가져가게 되면, input-data의 형태에 대한 가정이 따로 없기 때문에 여러 multi-modal data를 pre-training 단계에서 활용할 수 있고, 이에 따라 semantic-rich representation을 학습하는 것도 가능해집니다. </p>\n<h3 id=\"related-works\" style=\"position:relative;\"><a href=\"#related-works\" aria-label=\"related works permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Related Works</h3>\n<h5 id=\"vitdet\" style=\"position:relative;\"><a href=\"#vitdet\" aria-label=\"vitdet permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>ViTDet</h5>\n<p>ViT 구조를 dense prediction에서 활용하는 방법으로는 ViTDet이 존재합니다. 하지만 해당 구조는 ViT에 feature를 injecting 해주는 기작이 따로 없고, 또한 input image를 새로운 모듈의 입력으로 활용하지 않고 ViT의 intermediate feature 만을 활용하기에 ViT Adapter와는 차이를 가집니다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/34e178825ab8989ee80d2c6bb81c75ce/949b7/24-03-16-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 37.368421052631575%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABZklEQVQoz3WQW5OaUBCE/f9/KZWHVEzKaABdZFFAuR+uAVE2lqzw7YEttzYPmVP9Mn2mp3tmwzDwKHunICKHtKgI44g0E/y9vjCMT36zzRVReCStLjzpG0QS075cptlRZ8TsIdh1HWVZIoTgyy+d7cHG8jfY4TP3/v7Be1GC5pXYwQ7jqJLX4v+CWZYRS2fftL0cCDFdlWNsTfztdqPIc9wg5vvTgUPooNsrqnP5SZB/BauqkqIpX3+bOFHMxlrgCnvir9erdFjghgkrJ8MTLsruB6dLNfH90L87fNzvfr+TpukU+ae6JUxSjoFFKPyPG7dtS5JmWEGKH3o43jPda8fnmo2bm6bhdGoIZMwgCCR8fN8jEcnUy2XU8Rx5Xsi+T/FHJilryrrlfGllP6OqG/peOqybM8beYef4KKqGpqmo6juWyyWKorBer5nP5ywWC3Rdx9zbGOaejW6wNQw8z5PwpdtX3gARFgoD2SmsSgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/34e178825ab8989ee80d2c6bb81c75ce/15813/24-03-16-1.webp 190w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/1cdb2/24-03-16-1.webp 380w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/9046c/24-03-16-1.webp 760w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/c89f9/24-03-16-1.webp 1140w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/7afe4/24-03-16-1.webp 1520w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/8d8b9/24-03-16-1.webp 1583w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/34e178825ab8989ee80d2c6bb81c75ce/a2d4f/24-03-16-1.png 190w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/3f520/24-03-16-1.png 380w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/3c051/24-03-16-1.png 760w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/b5cea/24-03-16-1.png 1140w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/891d5/24-03-16-1.png 1520w,\n/static/34e178825ab8989ee80d2c6bb81c75ce/949b7/24-03-16-1.png 1583w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/34e178825ab8989ee80d2c6bb81c75ce/3c051/24-03-16-1.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>ViT에 대한 설명은 <a href=\"https://yuhodots.github.io/deeplearning/23-05-27/\">이전 게시글</a>에서 확인 가능합니다.</p>\n<h5 id=\"adapter\" style=\"position:relative;\"><a href=\"#adapter\" aria-label=\"adapter permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Adapter</h5>\n<p>Adapter는 2019년에 제안된 Parameter Efficient Fine-Tuning (PEFT)의 초기 방법론 중 하나이며, Transformer 구조에 일부 weight을 추가해서, 일부 weight만 튜닝하더라도 LLM 전체를 fine-tuning 하는 효과를 얻기 위해 사용되는 모듈입니다. 더 과거에는 이러한 방식이 inference learning task를 푸는데에 적용되기도 하였습니다.</p>\n<h3 id=\"vision-transformer-adapter\" style=\"position:relative;\"><a href=\"#vision-transformer-adapter\" aria-label=\"vision transformer adapter permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Vision Transformer Adapter</h3>\n<h5 id=\"overall-architecture\" style=\"position:relative;\"><a href=\"#overall-architecture\" aria-label=\"overall architecture permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Overall Architecture</h5>\n<p>ViT Adapter의 전체적인 구조는 다음과 같습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/75b8db78d20fe4ec22578f514d23fa15/8b84a/24-03-16-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 33.15789473684211%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABsUlEQVQozy2QWY/SYBSG+f9/wHhr1ItJ1IuJyswQZGYAgbIU6F7a0tIWunxlGQYFNY+fy0nei5P35F1Ojf8jhCBOVvi+RxgGpElCFP3ZAzxvgWka6LpOmqYURU6ebSQfEscxZVmimwuq7Z6aM9WoxBOmYdNqPvK1q3Df7tPpDhgNFfT5DMN0uW08oE4mjIcDaRjx0FYYKCNGyoBAGk6mJqLaUVNvr3DnS4ZWh+veWx5nDSz1A5bZo6MtuRl7zKd3uPonxlbIR8VlKJPqymsM2+BO8s2JxWiisd0dqB1FzC7fYkQajVmdnhRaBW1WicPIr2iZuXRv4Zhf0EK52zmq67NeNgnThO5C0HMShiPtX+VCVByPR7JtwUokZPsCL5P/E2v2J8FeLLGsMa47Y5OaPJ1OBNmBgbVGW2ak1ZGo2LOS+H75Qc22bXmYsUjn1NVXtNU616039LUWUWbjajf46gsi7SWR9Z5ondJUHd71LQmb+5nL52lA14r4dpaCYRiSbXKKQ0pUOngyhZcaJNuA4zknS+aUG/0vDsLi+XImrp5xE9lkXeKvBf5GUJ5+cvkFvwFCC/NDOiyzuAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/75b8db78d20fe4ec22578f514d23fa15/15813/24-03-16-2.webp 190w,\n/static/75b8db78d20fe4ec22578f514d23fa15/1cdb2/24-03-16-2.webp 380w,\n/static/75b8db78d20fe4ec22578f514d23fa15/9046c/24-03-16-2.webp 760w,\n/static/75b8db78d20fe4ec22578f514d23fa15/c89f9/24-03-16-2.webp 1140w,\n/static/75b8db78d20fe4ec22578f514d23fa15/7afe4/24-03-16-2.webp 1520w,\n/static/75b8db78d20fe4ec22578f514d23fa15/56373/24-03-16-2.webp 1531w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/75b8db78d20fe4ec22578f514d23fa15/a2d4f/24-03-16-2.png 190w,\n/static/75b8db78d20fe4ec22578f514d23fa15/3f520/24-03-16-2.png 380w,\n/static/75b8db78d20fe4ec22578f514d23fa15/3c051/24-03-16-2.png 760w,\n/static/75b8db78d20fe4ec22578f514d23fa15/b5cea/24-03-16-2.png 1140w,\n/static/75b8db78d20fe4ec22578f514d23fa15/891d5/24-03-16-2.png 1520w,\n/static/75b8db78d20fe4ec22578f514d23fa15/8b84a/24-03-16-2.png 1531w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/75b8db78d20fe4ec22578f514d23fa15/3c051/24-03-16-2.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<ol>\n<li>Spatial Prior Module로 이미지를 입력해줍니다. 이를 통해 3가지 target resolution에 대한 spatial feature map을 얻어냅니다. 해당 feature map은 flatten 되고 concatenated 되어서 다음 레이어인 Spatial Feature Injector로 전달됩니다.</li>\n<li>Spatial Feature Injector에서는, ViT feature와 spatial feature 사이에 cross-attention을 수행하고, 결과 feature를 ViT로 주입해줍니다.</li>\n<li>그 다음 레이어인 Multi-Scale Feature Extractor에서도 ViT feature와 spatial feature 사이 cross-attention을 수행하고, spatial feature를 강화합니다.</li>\n<li>이 과정을 N 번 반복하며, 최종 feature는 다시 3가지 target resolution으로 split 하고 reshape 합니다.</li>\n<li>최종적으로 얻어진 multi-resolution feature를 dense prediction task에 활용하게 됩니다.</li>\n</ol>\n<h5 id=\"spatial-prior-module\" style=\"position:relative;\"><a href=\"#spatial-prior-module\" aria-label=\"spatial prior module permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Spatial Prior Module</h5>\n<p>Spatial Prior Module은 CNN 구조를 활용하여 local semantics(spatial prior)를 얻어내기 위한 모듈입니다. </p>\n<ol>\n<li>ResNet에 있는 standard covolutional stem 구조를 그대로 활용합니다.</li>\n<li>Input image를 conv stem에 입력으로 전달하여, 1/8, 1/16, 1/32 resolution의 feature를 얻어냅니다. 이후 각각의 feature들은 1x1 convolution을 적용하여 D dimension으로 동일하게 맞춰줍니다.</li>\n<li>해당 3가지 resolution의 feature map을 flatten &#x26; concatenate 하여 그 다음 레이어로 전달합니다.</li>\n</ol>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msubsup><mi mathvariant=\"script\">F</mi><mrow><mi mathvariant=\"normal\">s</mi><mi mathvariant=\"normal\">p</mi></mrow><mn>1</mn></msubsup><mo>∈</mo><msup><mi mathvariant=\"double-struck\">R</mi><mrow><mrow><mo fence=\"true\">(</mo><mfrac><mrow><mi>H</mi><mi>W</mi></mrow><msup><mn>8</mn><mn>2</mn></msup></mfrac><mo>+</mo><mfrac><mrow><mi>H</mi><mi>W</mi></mrow><mrow><mn>1</mn><msup><mn>6</mn><mn>2</mn></msup></mrow></mfrac><mo>+</mo><mfrac><mrow><mi>H</mi><mi>W</mi></mrow><mrow><mn>3</mn><msup><mn>2</mn><mn>2</mn></msup></mrow></mfrac><mo fence=\"true\">)</mo></mrow><mo>×</mo><mi>D</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">\\mathcal{F}_{\\mathrm{sp}}^1 \\in \\mathbb{R}^{\\left(\\frac{H W}{8^2}+\\frac{H W}{16^2}+\\frac{H W}{32^2}\\right) \\times D}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.2472159999999999em;vertical-align:-0.383108em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.864108em;\"><span style=\"top:-2.4530000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">s</span><span class=\"mord mathrm mtight\">p</span></span></span></span></span><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">1</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.383108em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.15092em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathbb\">R</span></span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.15092em;\"><span style=\"top:-3.4534200000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"minner mtight\"><span class=\"mopen sizing reset-size3 size6 mtight delimcenter\" style=\"top:0.07500000000000001em;\"><span class=\"mtight\">(</span></span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8720928571428572em;\"><span style=\"top:-2.5061857142857145em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">8</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9384399999999999em;\"><span style=\"top:-2.93844em;margin-right:0.1em;\"><span class=\"pstrut\" style=\"height:2.64444em;\"></span><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.49381428571428565em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8720928571428572em;\"><span style=\"top:-2.5061857142857145em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">1</span><span class=\"mord mtight\"><span class=\"mord mtight\">6</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9384399999999999em;\"><span style=\"top:-2.93844em;margin-right:0.1em;\"><span class=\"pstrut\" style=\"height:2.64444em;\"></span><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.49381428571428565em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\"><span class=\"mopen nulldelimiter sizing reset-size3 size6\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8720928571428572em;\"><span style=\"top:-2.5061857142857145em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">3</span><span class=\"mord mtight\"><span class=\"mord mtight\">2</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9384399999999999em;\"><span style=\"top:-2.93844em;margin-right:0.1em;\"><span class=\"pstrut\" style=\"height:2.64444em;\"></span><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span></span></span><span style=\"top:-3.2255000000000003em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line mtight\" style=\"border-bottom-width:0.049em;\"></span></span><span style=\"top:-3.384em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.08125em;\">H</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.49381428571428565em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter sizing reset-size3 size6\"></span></span><span class=\"mclose sizing reset-size3 size6 mtight delimcenter\" style=\"top:0.07500000000000001em;\"><span class=\"mtight\">)</span></span></span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">D</span></span></span></span></span></span></span></span></span></span></span></span></span>\n<h5 id=\"feature-interaction-1-spatial-feature-injector\" style=\"position:relative;\"><a href=\"#feature-interaction-1-spatial-feature-injector\" aria-label=\"feature interaction 1 spatial feature injector permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Feature Interaction 1. Spatial Feature Injector</h5>\n<p>Spatial Feature Injector는 spatial prior를 ViT 내부로 잘 흘려보내주기 위한 모듈입니다.</p>\n<ol>\n<li>ViT feature를 query, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"script\">F</mi><mrow><mi mathvariant=\"normal\">s</mi><mi mathvariant=\"normal\">p</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{F}_{\\mathrm{sp}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">s</span><span class=\"mord mathrm mtight\">p</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>를 key, value로 하는 cross-attention을 수행합니다. (Computational cost를 위해서 sparse attention인 deformable attention을 사용합니다)</li>\n<li>Cross-attention을 통해 얻어진 feature를, 아래의 식을 통해(i.e., sum) ViT에 다시 넣어줍니다.</li>\n<li>다만 학습안정성을 위해서 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>γ</mi></mrow><annotation encoding=\"application/x-tex\">\\gamma</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.625em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span></span></span></span>​​를 0으로 초기화된 learnable parameter로 둡니다.</li>\n</ol>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><msubsup><mover accent=\"true\"><mi mathvariant=\"script\">F</mi><mo>^</mo></mover><mrow><mi mathvariant=\"normal\">v</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">t</mi></mrow><mi>i</mi></msubsup><mo>=</mo><msubsup><mi mathvariant=\"script\">F</mi><mrow><mi mathvariant=\"normal\">v</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">t</mi></mrow><mi>i</mi></msubsup><mo>+</mo><msup><mi>γ</mi><mi>i</mi></msup><mi mathvariant=\"normal\">Attention</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><mi mathvariant=\"normal\">norm</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><msubsup><mi mathvariant=\"script\">F</mi><mrow><mi mathvariant=\"normal\">v</mi><mi mathvariant=\"normal\">i</mi><mi mathvariant=\"normal\">t</mi></mrow><mi>i</mi></msubsup><mo fence=\"true\">)</mo></mrow><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">norm</mi><mo>⁡</mo><mrow><mo fence=\"true\">(</mo><msubsup><mi mathvariant=\"script\">F</mi><mrow><mi mathvariant=\"normal\">s</mi><mi mathvariant=\"normal\">p</mi></mrow><mi>i</mi></msubsup><mo fence=\"true\">)</mo></mrow><mo fence=\"true\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\">\\hat{\\mathcal{F}}_{\\mathrm{vit}}^i=\\mathcal{F}_{\\mathrm{vit}}^i+\\gamma^i \\operatorname{Attention}\\left(\\operatorname{norm}\\left(\\mathcal{F}_{\\mathrm{vit}}^i\\right), \\operatorname{norm}\\left(\\mathcal{F}_{\\mathrm{sp}}^i\\right)\\right)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.1937699999999998em;vertical-align:-0.247em;\"></span><span class=\"mord\"><span class=\"mord accent\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.9467699999999999em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span></span></span><span style=\"top:-3.25233em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.16666em;\"><span class=\"mord\">^</span></span></span></span></span></span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8746639999999999em;\"><span style=\"top:-2.4530000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\" style=\"margin-right:0.01389em;\">v</span><span class=\"mord mathrm mtight\">i</span><span class=\"mord mathrm mtight\">t</span></span></span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.121664em;vertical-align:-0.247em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8746639999999999em;\"><span style=\"top:-2.4530000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\" style=\"margin-right:0.01389em;\">v</span><span class=\"mord mathrm mtight\">i</span><span class=\"mord mathrm mtight\">t</span></span></span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.2577720000000001em;vertical-align:-0.383108em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05556em;\">γ</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8746639999999999em;\"><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">A</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">e</span><span class=\"mord mathrm\">n</span><span class=\"mord mathrm\">t</span><span class=\"mord mathrm\">i</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">n</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">(</span></span><span class=\"mop\"><span class=\"mord mathrm\">n</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">r</span><span class=\"mord mathrm\">m</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">(</span></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8746639999999999em;\"><span style=\"top:-2.4530000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\" style=\"margin-right:0.01389em;\">v</span><span class=\"mord mathrm mtight\">i</span><span class=\"mord mathrm mtight\">t</span></span></span></span></span><span style=\"top:-3.113em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.247em;\"><span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">)</span></span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mop\"><span class=\"mord mathrm\">n</span><span class=\"mord mathrm\">o</span><span class=\"mord mathrm\">r</span><span class=\"mord mathrm\">m</span></span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">(</span></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.874664em;\"><span style=\"top:-2.4530000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">s</span><span class=\"mord mathrm mtight\">p</span></span></span></span></span><span style=\"top:-3.1130000000000004em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">i</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.383108em;\"><span></span></span></span></span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">)</span></span></span><span class=\"mclose delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size1\">)</span></span></span></span></span></span></span>\n<h5 id=\"feature-interaction-2-multi-scale-feature-extractor\" style=\"position:relative;\"><a href=\"#feature-interaction-2-multi-scale-feature-extractor\" aria-label=\"feature interaction 2 multi scale feature extractor permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Feature Interaction 2. Multi-Scale Feature Extractor</h5>\n<p>Multi-Scale Feature Extractor는, 업데이트된 ViT feature를 통해 spatial feature를 다시 강화해주는 모듈입니다.</p>\n<ol>\n<li>Spatial Feature Injector와는 반대로, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi mathvariant=\"script\">F</mi><mrow><mi mathvariant=\"normal\">s</mi><mi mathvariant=\"normal\">p</mi></mrow></msub></mrow><annotation encoding=\"application/x-tex\">\\mathcal{F}_{\\mathrm{sp}}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.969438em;vertical-align:-0.286108em;\"></span><span class=\"mord\"><span class=\"mord\"><span class=\"mord mathcal\" style=\"margin-right:0.09931em;\">F</span></span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15139200000000003em;\"><span style=\"top:-2.5500000000000003em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mathrm mtight\">s</span><span class=\"mord mathrm mtight\">p</span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.286108em;\"><span></span></span></span></span></span></span></span></span></span>를 query, ViT feature를 key, value로 하는 cross-attention을 수행합니다. </li>\n<li>여기서도 동일하게 sparse attention인 deformable attention을 사용합니다.</li>\n</ol>\n<p>Feature interaction(즉, Spatial Feature Injector &#x26; Multi-Scale Feature Extractor)을 총 4회 반복하게 되고, 최종 Multi-Scale Feature Extractor를 통해 얻어진 feature를 split 하고 reshape 하여 3가지 resolution의 feature map으로 복원합니다.</p>\n<h3 id=\"experiments\" style=\"position:relative;\"><a href=\"#experiments\" aria-label=\"experiments permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments</h3>\n<p>MMDetection 기반으로 실험 환경을 구현하였고, COCO dataset에 대해서 object detection, image segmentation 성능을 측정하였습니다. Table 1을 보면 동일 pre-training에 대해 ViT와 ViTDet이 비해 ViT Adapter의 성능이 많이 좋다는 것을 확인할 수 있습니다. 특히, Table 3, 4에서는 ImageNet22k with multi-modal pre-training을 적용하였는데 SwinT 보다 좋은 성능을 보이는 것을 확인할 수 있습니다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/6fcb6/24-03-16-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 71.05263157894737%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAOCAYAAAAvxDzwAAAACXBIWXMAAAsTAAALEwEAmpwYAAACNUlEQVQ4y2VUa4+bQAzM//9XV6mnAIG8IIRnICzvNBD1JFJV1XTH6d596Adj8K7tGY+T1TRNKIoCz+dT24LnsogPwxB5niOOY5xOJ2RZJu9BEKCqKvi+j/P5LHH6/X6Ptm2x+nG/I0tTpIVC2T/QjC9jUpIkSPUZPYvTLpeLFDbFjEVR9CpY1zVcb4v02uJcjkiqEY+PBWVZyiUmG3T8JhuipLEZjXFa13VY8fHt7Q2248I7hjgnF9Rtry+8aDDxcDiIPx6PQn+z2cDzPIkzRr/b7YTVqu97+Dq4D2IktZ5nO+HS3PWh+qRKFPRESoSka2Zsznl3GAasyNvV3aJcIWtmTfuG++NDuhGVSWYCabEoEVEUjoFxGu/IDJVS+P7+jiAu4acKUdFiWZ4yQ9d1JZE0mcwkNuEottutvJsYmzRNg9U4jthqhLv9EWGsaZUKVTMg/teVqAwKg5BNWITfZMCGNClIhJbtaGQdIjXjVLxU5nowkfOhcPSMEQCTKQIL/qcyB7ler7E7+PDDWO9jhWmakWpELGgoG2qkzmJGeSI0cxaVb7ebfJSqQ64Gsfnx83OBjYLX61UUpcpEw3PO2ahPE5X5cBwHwYk7mCHJS4y3uy4WCQpDxyw20RCZmelrfuHXDCk1aXFZXW22bcGxbWnCGO1db4GtY3y3LEvOaFxuKm5Za2xcD02nEU5658q61+o2oi6hG4r0Rk1Sl9+wHk+a6T+NSw3Vjjp3wFW1mOcHfv3+g7+kbOfbZz9TTgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/15813/24-03-16-3.webp 190w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/1cdb2/24-03-16-3.webp 380w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/9046c/24-03-16-3.webp 760w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/c89f9/24-03-16-3.webp 1140w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/7afe4/24-03-16-3.webp 1520w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/dff8d/24-03-16-3.webp 1523w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/a2d4f/24-03-16-3.png 190w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/3f520/24-03-16-3.png 380w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/3c051/24-03-16-3.png 760w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/b5cea/24-03-16-3.png 1140w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/891d5/24-03-16-3.png 1520w,\n/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/6fcb6/24-03-16-3.png 1523w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/b5ebd3f7c09b85da5b5804e35f1b3e3b/3c051/24-03-16-3.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/eb9733efd2a435397f05229245a4afe2/d5bfb/24-03-16-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 42.10526315789473%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAAAsTAAALEwEAmpwYAAABfklEQVQoz32SW3OjMAyF+f+/ax/abRPu5mYbCBBuIaE0kOxMMmdlZfpaz2ikB+k7x7Kteb7g2HZYrhuu1xXDMKBuGjQUWZZxHA4Hzkop6rnit2P1XYe3v5/wkxzHfoLWGlEUIQgCjjzPGZamKdeXywXP5xPLsmDbNtzv/zivq6nvsKbpBC8QiMsBzbigqmoeNIAkSdD3Pbs2Lk1+Qe4oyxLTNKHINVx3jzSJ0LYNrI4cfu5s+LFmh0ppBoWhoEYXRVHwsOM4kFKys3meuW7blvpc2Ps3RMJGeyxfwJ3tQcga3WlmwA8wjmPeZVVVDDfOf4BGxISSMcLAOPTpFhrWMPSwHRciVeiGkYYUPM+jPQrUdYVxHFGSiOs6yLVi4EswpD4fPq0rFBEbMAIEPGK/+0MP8EGWJbQKyP6OVD/ge++QmYsk9citSw8WkrsT/4YTCU3nM+Zlxdf3iu12w+PxMMCBBiRkXiGMUr5mHCcQQtDrSl5BpnIUh4bzmSC/nf+3D0oYLVRQnQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/eb9733efd2a435397f05229245a4afe2/15813/24-03-16-4.webp 190w,\n/static/eb9733efd2a435397f05229245a4afe2/1cdb2/24-03-16-4.webp 380w,\n/static/eb9733efd2a435397f05229245a4afe2/9046c/24-03-16-4.webp 760w,\n/static/eb9733efd2a435397f05229245a4afe2/f13ee/24-03-16-4.webp 1072w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/eb9733efd2a435397f05229245a4afe2/a2d4f/24-03-16-4.png 190w,\n/static/eb9733efd2a435397f05229245a4afe2/3f520/24-03-16-4.png 380w,\n/static/eb9733efd2a435397f05229245a4afe2/3c051/24-03-16-4.png 760w,\n/static/eb9733efd2a435397f05229245a4afe2/d5bfb/24-03-16-4.png 1072w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/eb9733efd2a435397f05229245a4afe2/3c051/24-03-16-4.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>Spatial Prior Module에서 CNN 구조를 활용하였기 때문에, high-pass filter의 특징도 확인 가능합니다. (CNN와 ViT에 대한 자세한 차이는 \"How Do Vision Transformer Work?\" 논문에서 확인 가능)</p>","tableOfContents":"<ul>\n<li><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#introduction\">Introduction</a></li>\n<li>\n<p><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#related-works\">Related Works</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#vitdet\">ViTDet</a></li>\n<li><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#adapter\">Adapter</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#vision-transformer-adapter\">Vision Transformer Adapter</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#overall-architecture\">Overall Architecture</a></li>\n<li><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#spatial-prior-module\">Spatial Prior Module</a></li>\n<li><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#feature-interaction-1-spatial-feature-injector\">Feature Interaction 1. Spatial Feature Injector</a></li>\n<li><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#feature-interaction-2-multi-scale-feature-extractor\">Feature Interaction 2. Multi-Scale Feature Extractor</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/24-03-16-ViT%20Adapter/#experiments\">Experiments</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/24-03-16/","title":"Vision Transformer Adapter for Dense Predictions","category":"Deep Learning","date":"2024-03-16"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}