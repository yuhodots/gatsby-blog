{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/24-04-21/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>CVPR 2024에서 발표된 On Train-Test Class Overlap and Detection for Image Retrieval 논문을 정리합니다.</p>\n</blockquote>\n<h3 id=\"introduction\" style=\"position:relative;\"><a href=\"#introduction\" aria-label=\"introduction permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Introduction</h3>\n<h5 id=\"image-retrieval\" style=\"position:relative;\"><a href=\"#image-retrieval\" aria-label=\"image retrieval permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Image Retrieval</h5>\n<p>Image retrieval task는 주어진 query 이미지 상에 특정 object를 검출한 뒤에 해당 object와 매칭되는 internal db 상의 다른 이미지를 찾아내는 task입니다. 아래 이미지처럼 주어진 건축물에 대해 관련된 이미지를 잘 찾아내는 것이 task의 목적입니다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6c4b306a3b3d1138f155568dfb42301c/b5c21/24-04-21-1.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 30%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAABo0lEQVQY022RXUtTARjH91mEvLHLkD5CX0Cju1Q8G8u9MaZrsaQMUdrcpOFabmcZMwVRyCRs1tZNmIthBjklc6/qjpLojmfnyNp+jXnrAw//Pzz/l4tHxzVTqzcoSuekto8py3B5qbFzoJHM/OPLLvzYqyCVciS3Tnm/WSWeqZE90Vpe3fbvLL/28nzaKjKdLBD/eczZ6V9cZhOGXj0T/giiGOV5cBnb4wh97hjjwRXmpjw8HPbR5RC5502w+DZ2FegRl3D7Ityx+rl5348z9BG5IuMJzPNoLIzz6QvCoZeEApM4Rn080PcgTnrZXU+Qjs+yuiQyNR0ksfbhKtA+5KS9o4PO27e40d5Gf38fteZhNPqZsflvjIdX2Mls8mQiwEL6D69feYn4R/i6nuLd8gLf00k2UmvEZqOUTo7QaZqKolygqk2sKjQaDSTpkAHhLs/cdlw2CyMuO91d3ZgHrFhNRgx6AcE8iMUkMGQzMuiwYDQIzLyZQXfdU+r1OtVmwYWicnZeoVAstnY/l2c/m2vyEgeHR+QLBaRyGVmWW3qlqvIfldJ7pCeBgfYAAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/6c4b306a3b3d1138f155568dfb42301c/15813/24-04-21-1.webp 190w,\n/static/6c4b306a3b3d1138f155568dfb42301c/1cdb2/24-04-21-1.webp 380w,\n/static/6c4b306a3b3d1138f155568dfb42301c/9046c/24-04-21-1.webp 760w,\n/static/6c4b306a3b3d1138f155568dfb42301c/c89f9/24-04-21-1.webp 1140w,\n/static/6c4b306a3b3d1138f155568dfb42301c/7afe4/24-04-21-1.webp 1520w,\n/static/6c4b306a3b3d1138f155568dfb42301c/f8990/24-04-21-1.webp 2154w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/6c4b306a3b3d1138f155568dfb42301c/a2d4f/24-04-21-1.png 190w,\n/static/6c4b306a3b3d1138f155568dfb42301c/3f520/24-04-21-1.png 380w,\n/static/6c4b306a3b3d1138f155568dfb42301c/3c051/24-04-21-1.png 760w,\n/static/6c4b306a3b3d1138f155568dfb42301c/b5cea/24-04-21-1.png 1140w,\n/static/6c4b306a3b3d1138f155568dfb42301c/891d5/24-04-21-1.png 1520w,\n/static/6c4b306a3b3d1138f155568dfb42301c/b5c21/24-04-21-1.png 2154w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/6c4b306a3b3d1138f155568dfb42301c/3c051/24-04-21-1.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h5 id=\"previous-works-two-stage-training-process\" style=\"position:relative;\"><a href=\"#previous-works-two-stage-training-process\" aria-label=\"previous works two stage training process permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Previous Works: Two-Stage Training Process</h5>\n<p>Image retrieval을 위한 가장 일반적인 방법은 two-stage training process를 활용하는 것입니다. Two-stage 방식은, 먼저 주어진 query 이미지에 대해서 localization network를 기반으로 instance detection을 수행합니다. 그 다음에는 검출된(detected, cropped) instance에 대해서 embedding을 추출한 뒤 가장 유사한 image embedding을 DB 상에서 검색합니다. 추가적으로 DB 검색 단계에서는 cropped된 image embedding만 활용하는 것이 아니라, global descriptors라는 이미지 단위의 feature를 활용하는 경우도 존재합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/493d4d90f083869e2b75beeb8e843d5f/c211c/24-04-21-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 38.94736842105263%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAICAYAAAD5nd/tAAAACXBIWXMAABYlAAAWJQFJUiTwAAABaklEQVQoz42Sz07bQBjEeSz6AH2S0hdoxaNEQhwoh1aAUMUfcYBjFSGqVgJHSpzEIUFZBztrJ7ZjO9nY+fNjSUhPVcJI3x5m9hvNjnYLjfnrzF9PSEeKrvSQXYmUkiRJyfOcLMtRasxY6/EgZjqdLu6v9lbY+ke+Cd2WhbBMhGtj1is82YJoNCYMfKTXwaqYPNabDFO1wVBjlk+wyneUjCJVUeH0/JhPO58Ros2vmsH+/RV7hQK7X74Sh9HScLbGcJ5lxGmC40mCfh9bpzu7vCQaRNw/lPj24wTjz19uLq4ZJcP1CRf0ZEKgu3OETcd1dZeSKFom+X5wyMftD1TMZ34bIUmYbn7yXCmUeCLR0ymXcdttPMdBDQb0fJ9m1aQjulTLNrlS70io+yj+POL26hTfKOHWaniWxXg4JKzXaV7fED028K0q2UbDN6EnBI5elp5HPwiY6C8znc3I4wTZamE3GgS9/oL7n+ELSlpZjJhAuS0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/493d4d90f083869e2b75beeb8e843d5f/15813/24-04-21-2.webp 190w,\n/static/493d4d90f083869e2b75beeb8e843d5f/1cdb2/24-04-21-2.webp 380w,\n/static/493d4d90f083869e2b75beeb8e843d5f/9046c/24-04-21-2.webp 760w,\n/static/493d4d90f083869e2b75beeb8e843d5f/c89f9/24-04-21-2.webp 1140w,\n/static/493d4d90f083869e2b75beeb8e843d5f/dfb61/24-04-21-2.webp 1502w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/493d4d90f083869e2b75beeb8e843d5f/a2d4f/24-04-21-2.png 190w,\n/static/493d4d90f083869e2b75beeb8e843d5f/3f520/24-04-21-2.png 380w,\n/static/493d4d90f083869e2b75beeb8e843d5f/3c051/24-04-21-2.png 760w,\n/static/493d4d90f083869e2b75beeb8e843d5f/b5cea/24-04-21-2.png 1140w,\n/static/493d4d90f083869e2b75beeb8e843d5f/c211c/24-04-21-2.png 1502w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/493d4d90f083869e2b75beeb8e843d5f/3c051/24-04-21-2.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>이런 방식의 추론을 위해서는 당연스럽게도 localization network와 feature extractor를 각각 따로 학습시켜야 하며, feature extractor의 학습을 위해서는 이미지 상 object(instance)에 대한 location supervision 또한 필요하게 됩니다. </p>\n<h5 id=\"contributions\" style=\"position:relative;\"><a href=\"#contributions\" aria-label=\"contributions permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Contributions</h5>\n<p>따라서 본 논문의 저자들은 image retrieval task에 대해 꼭 localization network와 feature extractor를 각각 따로 학습시켜야하는지, location supervision이 꼭 필요한지에 대해 의문을 제기하며, location supervision이 필요없는 attention map 기반의 end-to-end image retrieval method (i.e., CiDeR)을 제안합니다.</p>\n<p>또한, 기존에 benchmark dataset으로 사용하던 Google Landmarks v2 (GLDv2)의 문제점을 언급하며, 데이터 정제를 통해 개선된 Revisiting Google Landmarks v2 (RGLDv2-clean)을 공개합니다. </p>\n<h3 id=\"revisiting-google-landmarks-v2\" style=\"position:relative;\"><a href=\"#revisiting-google-landmarks-v2\" aria-label=\"revisiting google landmarks v2 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Revisiting Google Landmarks v2</h3>\n<h5 id=\"training-and-evaluation-of-image-retrieval-task\" style=\"position:relative;\"><a href=\"#training-and-evaluation-of-image-retrieval-task\" aria-label=\"training and evaluation of image retrieval task permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Training and Evaluation of Image Retrieval Task</h5>\n<p>Image retrieval task의 benchmark evaluation set은 Revisited Oxford and Paris (ROxford, RParis)이고, training set은 Google Landmarks v2 (GLDv2)입니다. 기본적으로 benchmark training set에서는 아래의 두 가지 기준이 존재합니다.</p>\n<ul>\n<li>Depict particular landmarks</li>\n<li>To not contain landmarks that overlap with those in the evaluation sets.</li>\n</ul>\n<p>하지만 GLDv2는 ROxford, RParis에 대해 두 번째 기준을 어기고 있다는 것을 아래 figure를 통해 확인 가능합니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/49933a75b15c1167749dfb724deb44cf/53e84/24-04-21-3.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 31.578947368421055%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAGCAYAAADDl76dAAAACXBIWXMAABYlAAAWJQFJUiTwAAABsElEQVQY0yWQ7U9SARjF+af62H/gh7baWsWqD14vsYqsGEm7EgVXBOVKSIgrlWJaU7fKNXW59aYoJIvqQ4uAMS+Xl83GeBl8gp8Xebbz7Gzn2dk5j6HX69FsNikUClhujiJ7FcYf2JmbCeKeCLE4F2LiiYetzR3evIqxs/GeZ9ENhu8GES6aiIXCLEWi/Eql6I+h2+3SarXIZbMI10eQHtoQb1u5Itq5cO48xqGzCENnmPdJKB4fskPmjsWJKMqMWSZRnPcQrhmJvV4fGPYTtttt8rkcZpOZ1ZUYy2tv8b94x9gjP577Iyw4rrK1PENwWkFy+7F5FogsrRCcDTAVCOB1mFhfjQ4M+6vRaJDN/EMUbjB6S2TaNc6U18+wNMvT8HPmwxHWXkZRXI+xyhHMVgmH3Ybb6dJv3RgvX8I3KQ8MO53OacJarUY+85f47leSiX3SPw5J7H0i/uUzyd1t/vxO8f0gQT6b4VDXP25+ILkf52DvG9s6/5lO03+fQVVVcnrdPo6OVDRN01FCVYsUixrlSpWiVqZUrqCVtIFeKp9qlWqVer2hN2xyfPxf53VOAAl+cRaveyV9AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/49933a75b15c1167749dfb724deb44cf/15813/24-04-21-3.webp 190w,\n/static/49933a75b15c1167749dfb724deb44cf/1cdb2/24-04-21-3.webp 380w,\n/static/49933a75b15c1167749dfb724deb44cf/9046c/24-04-21-3.webp 760w,\n/static/49933a75b15c1167749dfb724deb44cf/c89f9/24-04-21-3.webp 1140w,\n/static/49933a75b15c1167749dfb724deb44cf/7afe4/24-04-21-3.webp 1520w,\n/static/49933a75b15c1167749dfb724deb44cf/ff286/24-04-21-3.webp 2074w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/49933a75b15c1167749dfb724deb44cf/a2d4f/24-04-21-3.png 190w,\n/static/49933a75b15c1167749dfb724deb44cf/3f520/24-04-21-3.png 380w,\n/static/49933a75b15c1167749dfb724deb44cf/3c051/24-04-21-3.png 760w,\n/static/49933a75b15c1167749dfb724deb44cf/b5cea/24-04-21-3.png 1140w,\n/static/49933a75b15c1167749dfb724deb44cf/891d5/24-04-21-3.png 1520w,\n/static/49933a75b15c1167749dfb724deb44cf/53e84/24-04-21-3.png 2074w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/49933a75b15c1167749dfb724deb44cf/3c051/24-04-21-3.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>따라서 GLDv2를 통해 학습된 모델들은 evaluation set의 landmark를 이미 학습에서 본 적이 있기 때문에 그 성능이 과장되어 있을 수 있습니다. </p>\n<h5 id=\"identifying-overlapping-landmarks\" style=\"position:relative;\"><a href=\"#identifying-overlapping-landmarks\" aria-label=\"identifying overlapping landmarks permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Identifying Overlapping Landmarks</h5>\n<p>저자들은 GLDv2 데이터에 대한 모델 기반 검수와, 검수자를 통한 검수를 수행합니다.</p>\n<ol>\n<li>Local featuer와 descriptor(GID)로  eval query에 대한 train set 이미지들 매칭</li>\n<li>해당 이미지를 3명의 사람이 직접 보면서 검수 . 한 명이라도 같은 카테고리라고 말하면 삭제</li>\n<li>추가적으로 Oxford, Paris 들어가는 GID 삭제</li>\n</ol>\n<p>결과적으로 GLDv2에서 evaluation set과 18개의 중복 GID를 발견하였고, 이를 제거하여 RGLDv2-clean을 제작하였습니다. </p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/44e43c08532a0df2f193c46a9e8247e4/f2f8c/24-04-21-4.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAABvElEQVQoz41SaY+bQAzd//9DKvVjq6qqVFVVq0pN94yylCvDwMISIDDLmft4td1kpXzbQQ+Dx35jv/HV8XgEr9UwoK8qrFcrpFmOOHnGcrkUnGPesq7OwQMRxkmCrCigH24R2xaarkffdULI2O12gv1+L3a73QrOfo55JSzLEuPxGMZU8P668GwXFVXcESGvtm2Rpinm8zniOBbL4Lwsy8ReELYlBd7+RjQz+DhS+HofojANFkMv+3Vdo6Dq+76XZO6ID1mRRPzP/ouWF4sl8tBBNL2DVjaesxKmbtCfCDnJsiw4jgPbtqGUQhiGAq01giAQKV4JWfzYV0goYeZPURU52qbGcGqZtWqaBgnpXOS5yMH/hizrfDgcLltedAbaGiENLOhE4XryB47/CO15EiykJPyEquyo3ZKIItKyIB3b06FCeDj+D56ZGJ9+vceHH+8wsn/CtgJMHjzET7nsM2n18oJHOsClNj1GFEFRxU90Kev1+qQh6CHOpIjw/f4Lvt18xkTdoWs7ughqyRgJZsLNZiPjwZZ9GwZ/08WwXwj5xXo4jotgGkAHIXzXh+vS6FA1PCJ8u28Zbo75B7r49RrIO221AAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/44e43c08532a0df2f193c46a9e8247e4/15813/24-04-21-4.webp 190w,\n/static/44e43c08532a0df2f193c46a9e8247e4/1cdb2/24-04-21-4.webp 380w,\n/static/44e43c08532a0df2f193c46a9e8247e4/9046c/24-04-21-4.webp 760w,\n/static/44e43c08532a0df2f193c46a9e8247e4/c89f9/24-04-21-4.webp 1140w,\n/static/44e43c08532a0df2f193c46a9e8247e4/752cd/24-04-21-4.webp 1490w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/44e43c08532a0df2f193c46a9e8247e4/a2d4f/24-04-21-4.png 190w,\n/static/44e43c08532a0df2f193c46a9e8247e4/3f520/24-04-21-4.png 380w,\n/static/44e43c08532a0df2f193c46a9e8247e4/3c051/24-04-21-4.png 760w,\n/static/44e43c08532a0df2f193c46a9e8247e4/b5cea/24-04-21-4.png 1140w,\n/static/44e43c08532a0df2f193c46a9e8247e4/f2f8c/24-04-21-4.png 1490w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/44e43c08532a0df2f193c46a9e8247e4/3c051/24-04-21-4.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h3 id=\"single-stage-pipeline-for-d2r\" style=\"position:relative;\"><a href=\"#single-stage-pipeline-for-d2r\" aria-label=\"single stage pipeline for d2r permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Single-Stage Pipeline for D2R</h3>\n<h5 id=\"motivation\" style=\"position:relative;\"><a href=\"#motivation\" aria-label=\"motivation permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Motivation</h5>\n<p>Prior works에서 자주 사용하는 two-stage process는 아래와 같은 단점들이 존재합니다.</p>\n<ul>\n<li>Representation learning을 위해 location supervision 있어야 함</li>\n<li>End-to-End 학습 아닌, detection, representation 모델 각각 따로 학습</li>\n<li>두 번의 forward pass 거쳐야 하므로, 이미지 당 search cost 높음</li>\n</ul>\n<p>따라서 저자들은 localization step을 spatial attention으로 대체하여 location supervision이 필요없는 end-to-end learning process 만들었습니다. 이 방식을 논문 상에서 attentional localization (AL)이라 명명합니다. </p>\n<h5 id=\"attentional-localization-al\" style=\"position:relative;\"><a href=\"#attentional-localization-al\" aria-label=\"attentional localization al permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Attentional Localization (AL)</h5>\n<ol>\n<li>Feature map에 1x1 conv를 적용하여 spatial attention map을 얻어냄</li>\n<li>T개의 threshold에 따라 T 개의 binary(bit) mask를 생성함 (실험에서는 T=2로 설정)</li>\n<li>Binary mask와 feature map으로 convex combination(weighted sum) 수행하여 feature fusion</li>\n<li>최종 feature인 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msup><mi mathvariant=\"bold\">F</mi><mi>l</mi></msup></mrow><annotation encoding=\"application/x-tex\">\\mathbf F^l</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.849108em;vertical-align:0em;\"></span><span class=\"mord\"><span class=\"mord mathbf\">F</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.849108em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.01968em;\">l</span></span></span></span></span></span></span></span></span></span></span>를 image retrieval에 사용</li>\n</ol>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/cdfb9f857551e872eedb2f8e78d9bab2/0d390/24-04-21-5.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 47.89473684210526%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAAClUlEQVQoz22SS0iUYRSGZx8RBYGtWrWNLiREBpFRgRGWizLcZTshSkgpiyRvJWZNTJhaalqalhiaYmFFVy95nXHm9x/HaWac5v47/nNxLurTp1CL6MDLdxaH93zPOUezurrKmv7E//JoRKWu10rDkMS9Hyq1Jg8Fj+o4c/oUBTdKeTmm0DXiIbkCmpXlZULhEE6XB7/fTzKRYCGcRI0k1izXDb3hBXJ7OyjvrCS7/QOVfToOZqWxccMmUtMOUXD3KbcePCMWT6JZiiVYikQZfa9nUV0ikYjTPxVkUvYJq5V1Q2dIJad7mKymd+R9G6Jx4DbHzx5i69ZtpKcfprauhYo7OkG0gubNqMKI5GTKOIgaXSSshvj03YZk8bO4GKR/QGLa7qd6sJ2KjhrqvnXT3lRIVv45Mk+cJDPrDNqHrdQ+6VhvrvkViOFSggwZfiD/tBPwB/jy6iuy7CG4oIifj2PzerlZX0KxtoKinmYuv9SxL/c821NS2LknlUv5Vymv0pJMCuRIJMyw0cvnsXk+jFgF8jKDLz5h0jv+LkeJRihtraS48SFX+jq4/r6FHWcvkLJlM3tTd1N5v4yax1ri8RiaQEDB5fHhdP7C5/cRUlUUp0u8IVSRx0RD+8IS+U0lXG+4RtmzKuorsjmQk0HG0WPs2n+EC3mFXCyqJpYQM7TMWTDLMxj0esbHx5mYmMBgNGIwGJBM4tUbCChupuR2Po50YTO8ZnKomd63rWi1OtrantPT2crHgX7WrkxjNptZ06yQwzGPzWrF7XLhdrvxen14PB4UQWGW7czKViSzDdniwGabF/UOrKLePDvHvM0uTi6O5t/D/l9Eo1EslllmBIksJElGIbH96Wn0gswoiCSTScw/wW/BUJWPQM+EAwAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/cdfb9f857551e872eedb2f8e78d9bab2/15813/24-04-21-5.webp 190w,\n/static/cdfb9f857551e872eedb2f8e78d9bab2/1cdb2/24-04-21-5.webp 380w,\n/static/cdfb9f857551e872eedb2f8e78d9bab2/9046c/24-04-21-5.webp 760w,\n/static/cdfb9f857551e872eedb2f8e78d9bab2/c89f9/24-04-21-5.webp 1140w,\n/static/cdfb9f857551e872eedb2f8e78d9bab2/098de/24-04-21-5.webp 1472w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/cdfb9f857551e872eedb2f8e78d9bab2/a2d4f/24-04-21-5.png 190w,\n/static/cdfb9f857551e872eedb2f8e78d9bab2/3f520/24-04-21-5.png 380w,\n/static/cdfb9f857551e872eedb2f8e78d9bab2/3c051/24-04-21-5.png 760w,\n/static/cdfb9f857551e872eedb2f8e78d9bab2/b5cea/24-04-21-5.png 1140w,\n/static/cdfb9f857551e872eedb2f8e78d9bab2/0d390/24-04-21-5.png 1472w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/cdfb9f857551e872eedb2f8e78d9bab2/3c051/24-04-21-5.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h5 id=\"components\" style=\"position:relative;\"><a href=\"#components\" aria-label=\"components permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Components</h5>\n<p>단순히 feature fusion된 feature를 그대로 사용하는 것은 아니고, image retrieval 분야에서 자주 활용되는 테크닉을 모두 활용하였다고 합니다. </p>\n<p>가장 먼저는 backbone network로 feature를 뽑고, SENet이나 ASPP, SKNet 같은 방법으로 feature map을 업데이트한 뒤에, 논문에서 제안한 attentional localization을 적용하고, 최종적으로 GeM(Generalized Mean Pooling) 방법을 통해 D dimension의 feature를 뽑아내게 됩니다. </p>\n<h3 id=\"experiments\" style=\"position:relative;\"><a href=\"#experiments\" aria-label=\"experiments permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Experiments</h3>\n<p>가장 먼저는 RGLDv2-clean으로 training set을 변경하였을 때 기존 prior works들의 성능이 얼마나 하락하는지를 보입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/19bb3/24-04-21-6.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 35.26315789473684%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAABYlAAAWJQFJUiTwAAABOElEQVQoz3WS227CMBBE8/8/1j7QCoHaxLmQBhJCLs6NEDAgprOuUvHQWhqttV4fza7tPB4PyLper+jaFuZiMAyDVV1V6PsebdOgKAqcxhFd16Gua3vesn48jva+cETODDQElhULLyN026DkpThJsMsybNPUxizPke73VpLThJ/M+W/gcZrw5ntQTYp1tMEqDLGjg7gssRHRoSiiJCeSffM/8IxXtUYyagJjrDYRfDoReXQzS9Gp5CQqRm0m0p6B+FnDOOFFrZBdeiz9EO++wlZr6yI8HH4VsG2J1mFZQN/OeF7O/X63m57DXcQekr5huxGWbPmLc5xBAhZnMzDY53DpOGX9jfM3xuBGlpOzII5jhBFbJMTzfaggsPpwXRtdpfDpeQhYo3guNSlbloeram1/grz+xHf4BosWDUNu/cvgAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/15813/24-04-21-6.webp 190w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/1cdb2/24-04-21-6.webp 380w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/9046c/24-04-21-6.webp 760w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/c89f9/24-04-21-6.webp 1140w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/7afe4/24-04-21-6.webp 1520w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/9672a/24-04-21-6.webp 2066w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/a2d4f/24-04-21-6.png 190w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/3f520/24-04-21-6.png 380w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/3c051/24-04-21-6.png 760w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/b5cea/24-04-21-6.png 1140w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/891d5/24-04-21-6.png 1520w,\n/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/19bb3/24-04-21-6.png 2066w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/6c21f74b7dbbb5a84d356c5f9cc6d0af/3c051/24-04-21-6.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>그 다음으로는 CiDeR 방법의 성능을 보입니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/121738b26b7af9926f34a44244d0c5ad/0ff19/24-04-21-7.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 56.84210526315789%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAYAAAB/Ca1DAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB70lEQVQoz0VSaZOqQAzk//+o/f6srVU5hEUOQZRLDikRvLZfOtbbN1WpTDKTTndmjPv9jqZpMM8z5mnGOI44nU4YhgFt2+qevmlOKMtS48PhoLbfp2J7JEmCoihALGO+zdjFMcIwRJIlqNsayS5BVZViFfq+x/F4VAA2ZkxQWpqmYokC8v7z+YQxTRO+/W+EuxBlXaDtWkRRKB1z7Upm2+0WsTRlUd936LoO53Ov7N6gqTZXQEoNggDRIdLunchj1zx/A5KJaZp6hwwJRk/LskzB3gwrPB4PGNfxCi/wcBpkbtOA8TLC81wFIwMWUTILqIYxAThP7nnn371fwC/nC+epBxcZkxELWMjZkeXlctFzNmK+rutfdvQciQLyZZO9SKzzt5y2URAWsmsURyqXBWSay2yZJ2OCvq36/8rTdYJtW8jPR2VwFSaeu0FZ5TiSnRTwF5ylGVctzAlO1q/XS3P0fBAuYxSJH4s/sNIYrSRz+YcLx4Evj+KIFE9AA2GTCmB9u8EVds42RpBmyEpRU7RiFfa5PMrrB0YlDBarFT5NC478RVvkLTcbOOIt38fKdeFGkcb0a8+DZdvy1XxRZsOSedMc2eci2+BcrPUaG2G1Wi710LYs2LwoeVPMkZiesS+AnKkvgPQE5j/1JE+sv9ErLMdQdIPjAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/121738b26b7af9926f34a44244d0c5ad/15813/24-04-21-7.webp 190w,\n/static/121738b26b7af9926f34a44244d0c5ad/1cdb2/24-04-21-7.webp 380w,\n/static/121738b26b7af9926f34a44244d0c5ad/9046c/24-04-21-7.webp 760w,\n/static/121738b26b7af9926f34a44244d0c5ad/c89f9/24-04-21-7.webp 1140w,\n/static/121738b26b7af9926f34a44244d0c5ad/7afe4/24-04-21-7.webp 1520w,\n/static/121738b26b7af9926f34a44244d0c5ad/3c5e9/24-04-21-7.webp 2084w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/121738b26b7af9926f34a44244d0c5ad/a2d4f/24-04-21-7.png 190w,\n/static/121738b26b7af9926f34a44244d0c5ad/3f520/24-04-21-7.png 380w,\n/static/121738b26b7af9926f34a44244d0c5ad/3c051/24-04-21-7.png 760w,\n/static/121738b26b7af9926f34a44244d0c5ad/b5cea/24-04-21-7.png 1140w,\n/static/121738b26b7af9926f34a44244d0c5ad/891d5/24-04-21-7.png 1520w,\n/static/121738b26b7af9926f34a44244d0c5ad/0ff19/24-04-21-7.png 2084w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/121738b26b7af9926f34a44244d0c5ad/3c051/24-04-21-7.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/ca4b5b52c8bc5ff333e2031aaab096e1/d8fc6/24-04-21-8.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 50%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAABYlAAAWJQFJUiTwAAAB00lEQVQoz01S2XLbMAz0/39RPyB969SHakeHRYq6b8lOTHdcK9slnDbWDASAXAKLYwV+l8sF0zThdrthGAaUZSn+PM+o61rkdDphHEe5d3d93wvOYZztYrhv5X72auXB1V7RNDWMMQQXKIoCURTheDwijo9iZ1kqgf7dV1XFs0ze/w/oGPiJj9k+sjmg1lrAHf0kSSRJ27Zi53kuLB3OiWNurf0KWDHjXnty0MsjDaVi2Ms7lvsdZZFjZOCPj0UYTgwA2k3ToCPeYZZl+QqYksnL7icae4Gpe4QqQXg0yMsBzXCGymtoVtEOM5SpkGQN2vENSc5KyLboOrxdnxi2zLh59eFnBlnJnqQp8ozgWCHwAwRBiF++j4AYpbTcZSIZy8+QsB3z+amHY9cjMj7ef59QZrmUrCkmNexvJWVqtsD1UWv1OK9KpNTOLtgS+zzlmn1YRx7sckc3jAzIANqxLFGwv6mwKcmmFO2kbjpOumGwmlJxbT5L/sPpBHGMby/f8YNr4YUhdt4B2+0e280eu/2Bvk//gPXuFRuKt/N57mPNM4++05pJz1yd1cyxG+6ZUgpREDx2juW5/gyfi+ykH3pZKaeneeI6dWi7lphBtJu2w/0Fm/HmUzYbcS0AAAAASUVORK5CYII='); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/ca4b5b52c8bc5ff333e2031aaab096e1/15813/24-04-21-8.webp 190w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/1cdb2/24-04-21-8.webp 380w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/9046c/24-04-21-8.webp 760w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/c89f9/24-04-21-8.webp 1140w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/7afe4/24-04-21-8.webp 1520w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/51cd0/24-04-21-8.webp 2052w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/ca4b5b52c8bc5ff333e2031aaab096e1/a2d4f/24-04-21-8.png 190w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/3f520/24-04-21-8.png 380w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/3c051/24-04-21-8.png 760w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/b5cea/24-04-21-8.png 1140w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/891d5/24-04-21-8.png 1520w,\n/static/ca4b5b52c8bc5ff333e2031aaab096e1/d8fc6/24-04-21-8.png 2052w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/ca4b5b52c8bc5ff333e2031aaab096e1/3c051/24-04-21-8.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<h3 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h3>\n<p>Song, et al. \"On Train-Test Class Overlap and Detection for Image Retrieval.\" CVPR 2024.</p>","tableOfContents":"<ul>\n<li>\n<p><a href=\"/MachineLearning/24-04-21-CiDeR/#introduction\">Introduction</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#image-retrieval\">Image Retrieval</a></li>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#previous-works-two-stage-training-process\">Previous Works: Two-Stage Training Process</a></li>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#contributions\">Contributions</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/24-04-21-CiDeR/#revisiting-google-landmarks-v2\">Revisiting Google Landmarks v2</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#training-and-evaluation-of-image-retrieval-task\">Training and Evaluation of Image Retrieval Task</a></li>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#identifying-overlapping-landmarks\">Identifying Overlapping Landmarks</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/24-04-21-CiDeR/#single-stage-pipeline-for-d2r\">Single-Stage Pipeline for D2R</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#motivation\">Motivation</a></li>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#attentional-localization-al\">Attentional Localization (AL)</a></li>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#components\">Components</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#experiments\">Experiments</a></li>\n<li><a href=\"/MachineLearning/24-04-21-CiDeR/#references\">References</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/24-04-21/","title":"On Train-Test Class Overlap for Image Retrieval","category":"Deep Learning","date":"2024-04-21"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}