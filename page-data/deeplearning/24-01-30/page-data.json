{"componentChunkName":"component---src-templates-post-js","path":"/deeplearning/24-01-30/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>Meta AI Research의 Segment Anything 논문의 Data Engine 파트와 Appendix F 파트를 정리하며 mask annotation 자동화 과정에서 고려할 점들이 무엇인지 파악합니다.</p>\n</blockquote>\n<h3 id=\"data-engine\" style=\"position:relative;\"><a href=\"#data-engine\" aria-label=\"data engine permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Engine</h3>\n<h5 id=\"1-assisted-manual-stage\" style=\"position:relative;\"><a href=\"#1-assisted-manual-stage\" aria-label=\"1 assisted manual stage permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Assisted-Manual Stage</h5>\n<p>제일 처음 단계에서는 사람이 직접 mask annotation 만들었음. 모델 도움도 일부 받음</p>\n<ul>\n<li>처음에는 SAM을 public segmentation dataset으로 학습함</li>\n<li>Browser-based interactive segmentation tool 기반으로 SAM 도움 받아 foreground/background 클릭하면서 mask 작업 수행. 이 때, 'brush'와 'eraser' 기능이 내부 툴에 존재해서 mask 수정 쉽게 가능했음</li>\n<li>Label은 그저 'stuff'와 'things'로 annotator들이 자유롭게 라벨링 하도록 두었음</li>\n<li>데이터를 충분히 취득하면 SAM을 다시 학습하고(이 과정을 6회 반복), 모델도 ViT-B에서 ViT-H로 키워갔음. </li>\n<li>하나의 mask 라벨링에 대해 30초 정도의 속도를 권장하였고, 처음에는 34초에서 시작해서 모델이 좋아진 뒤에는 14초까지 줄었음</li>\n<li>이 단계에서 총 12만 이미지에 대해 430만 mask를 수집했음</li>\n</ul>\n<h5 id=\"2-semi-automatic-stage\" style=\"position:relative;\"><a href=\"#2-semi-automatic-stage\" aria-label=\"2 semi automatic stage permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Semi-Automatic Stage</h5>\n<p>그 다음 단계에서는, 모델 기반으로 confident mask를 미리 만들어 준 뒤에 나머지 영역을 사람이 annotation 수행함</p>\n<ul>\n<li>Annotator들이 덜 두드러진 영역만 신경쓰면 되도록, 모델 기반으로 confident mask를 미리 만들어주고 mask가 만들어지지 않은 영역에 대해서만 annotation 수행하도록 했음</li>\n<li>Confident mask 검출을 위해서는, first stage에서 만들어진 모든 mask 데이터를 활용하여 'object' class 가진 bounding box detector를 학습시켰음 (이와 관련해서 <em>BoxTeacher</em>나 <em>SIM</em> 같은 weakly-supervised segmentation method 같이 확인해보면 좋을듯)</li>\n<li>(1)단계에서처럼 모델을 재학습(5회 반복)하는 과정을 거쳤고 automatic mask를 제외한 mask에 대해 평균 34초 정도 소요됨 </li>\n<li>이 단계에서 총 18만 이미지에 대해 590만 mask를 수집했음 (1단계와 합치면 1020만 mask)</li>\n</ul>\n<h5 id=\"3-fully-automatic-stage\" style=\"position:relative;\"><a href=\"#3-fully-automatic-stage\" aria-label=\"3 fully automatic stage permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Fully Automatic Stage</h5>\n<p>마지막 단계에서는 annotation 완전 자동화를 수행함</p>\n<ul>\n<li>32x32 grid point를 모델에 입력하고, 각 포인트 마다 object에 대한 mask 출력</li>\n<li>SAM 모델은 subpart, part, whole object에 대한 mask를 모두 뱉게 되는데, 여기서 IoU prediction module로 confident mask를 선택</li>\n<li>그리고 이 중에서 오직 stable mask만 선정: probability map을 0.5-a와 0.5+a로 변경해 보았을 때 비슷한 mask인 경우를 stable로 여김</li>\n<li>만들어진 전체 confident &#x26; stable mask들에 NMS 적용하여 중복 제거</li>\n<li>작은 mask 퀄리티 향상 위해서 multiple overlapping zoomed-in Image crops 등도 적용했는데, 자세한 내용은 Appendix B 확인</li>\n<li>이 단계에서 총 1100만 이미지에 대해 11억 mask를 수집했음</li>\n</ul>\n<h5 id=\"example-data-format\" style=\"position:relative;\"><a href=\"#example-data-format\" aria-label=\"example data format permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Example Data Format</h5>\n<ul>\n<li><code class=\"language-text\">counts</code> key: COCO run-length encoded(RLE) mask</li>\n</ul>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n    <span class=\"token property\">\"image\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"image_id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">560</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"width\"</span><span class=\"token operator\">:</span> <span class=\"token number\">2250</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"height\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1500</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"file_name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"sa_560.jpg\"</span>\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"annotations\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n        <span class=\"token punctuation\">{</span>\n            <span class=\"token property\">\"bbox\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1167.0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">157.0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">107.0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">129.0</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"area\"</span><span class=\"token operator\">:</span> <span class=\"token number\">7948</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"segmentation\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n                <span class=\"token property\">\"size\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token number\">1500</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2250</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                <span class=\"token property\">\"counts\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"hf]e1>W^19L4J7K7I4L4L4L3N2M2O2N2N2N1O2N1O2O1N1N2O2N1O2N1O1N3N1O0O10J5O2N110O01O1N200O101N1O1O2N101N2N2O101N1O10001N101N1O2O0O100O10000O101O1O1O0001O2N2N4L1N3N1O001O001N2O1O3M3iMSdNY1R\\\\1^NSdN`1f\\\\1O1O2N2N3L5L4L4L2N1O1O1O0O2N1N2O2M2N2N3M3L5Lfoc\\\\1\"</span>\n            <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"predicted_iou\"</span><span class=\"token operator\">:</span> <span class=\"token number\">0.8973201513290405</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"point_coords\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">[</span><span class=\"token number\">1233.9375</span><span class=\"token punctuation\">,</span> <span class=\"token number\">177.1875</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"crop_box\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token number\">998</span><span class=\"token punctuation\">,</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span> <span class=\"token number\">755</span><span class=\"token punctuation\">,</span> <span class=\"token number\">567</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"id\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1103484716</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"stability_score\"</span><span class=\"token operator\">:</span> <span class=\"token number\">0.9846441745758057</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        ...\n    <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<h3 id=\"data-annotation-card\" style=\"position:relative;\"><a href=\"#data-annotation-card\" aria-label=\"data annotation card permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Data Annotation Card</h3>\n<h5 id=\"task-formulation\" style=\"position:relative;\"><a href=\"#task-formulation\" aria-label=\"task formulation permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Task Formulation</h5>\n<ul>\n<li>Task의 주관적인 영역은 무엇이었는지?: Segmentation 작업 자체가 내재적으로 주관적임. 동일한 신발 object에 대해 신발 켤레를 하나의 mask로 segment 할 수도 있고, 신발 한 짝 마다 각각의 mask로 segment 할 수도 있음. Mask의 완벽함보다는 다양성에 중점을 두고 segment 했기에 이렇게 하는 것이 상관없었음</li>\n<li>Annotator와 관련된 가정: Full time으로 일하며, 인원 감축을 크게 하지 않음. 작업 목적에 대한 확인한 이해와 명확한 가이드라인(시각/비디오 자료), 그리고 OKR 공유와 작업자들의 주간 미팅 참여가 annotation의 양과 질을 높이는데 기여했음</li>\n<li>Instruction의 명확성을 검증하기 위해 어떤 작업을 거쳤는지?: 리서치 팀이 annotation tool을 사용해서 30개 정도의 annotation task를 직접 작업하면서 복잡한 케이스를 확인하거나 가이드라인을 수정함. 또한, 리서치팀이 annotator들과 주간 피드백 세션을 가짐</li>\n<li>얼마나 자세한 instruction을 제공했는지?: 오직 '주어진 이미지에 대해, 모든 가능한 object를 segment 하는 것이 목적'이라는 high-level instruction만 제공함. 내부 interactive segmentation tool을 사용하여 foreground/background 클릭을 통해 mask를 추가하거나 삭제할 수 있고, bounding box를 그릴수도 있음. 또한 pixel-precise tool을 통해 mask 수정도 가능 (CVAT 같은 툴 생각하면 될듯)</li>\n</ul>\n<h5 id=\"platform-and-infrastructure\" style=\"position:relative;\"><a href=\"#platform-and-infrastructure\" aria-label=\"platform and infrastructure permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Platform and Infrastructure</h5>\n<ul>\n<li>Annotators 인원: 130명 정도의 Kenya annotators와 작업함</li>\n<li>Annotation에 사용한 플랫폼: proprietary annotation platform 사용함 (내부 플랫폼 사용한 것으로 해석하였음)</li>\n<li>Annotators와의 커뮤니케이션 방법: Annotation QA team과는 daily로 피드백 주고받고, 리서치팀과는 weekly로 피드백 주고받았으며, 스프레드시트나 chat group 통해 주기적으로 소통했음. 이 과정이 데이터 생산의 양과 질을 매우 높일 수 있었음</li>\n</ul>\n<h3 id=\"reference\" style=\"position:relative;\"><a href=\"#reference\" aria-label=\"reference permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reference</h3>\n<ul>\n<li>Segment Anything: <a href=\"https://arxiv.org/abs/2304.02643\">https://arxiv.org/abs/2304.02643</a></li>\n<li>SA-1B Dataset: <a href=\"https://ai.meta.com/datasets/segment-anything-downloads/\">https://ai.meta.com/datasets/segment-anything-downloads/</a></li>\n</ul>","tableOfContents":"<ul>\n<li>\n<p><a href=\"/MachineLearning/24-01-30-Segment%20Anything%20Data%20Engine/#data-engine\">Data Engine</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/24-01-30-Segment%20Anything%20Data%20Engine/#1-assisted-manual-stage\">1. Assisted-Manual Stage</a></li>\n<li><a href=\"/MachineLearning/24-01-30-Segment%20Anything%20Data%20Engine/#2-semi-automatic-stage\">2. Semi-Automatic Stage</a></li>\n<li><a href=\"/MachineLearning/24-01-30-Segment%20Anything%20Data%20Engine/#3-fully-automatic-stage\">3. Fully Automatic Stage</a></li>\n<li><a href=\"/MachineLearning/24-01-30-Segment%20Anything%20Data%20Engine/#example-data-format\">Example Data Format</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"/MachineLearning/24-01-30-Segment%20Anything%20Data%20Engine/#data-annotation-card\">Data Annotation Card</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"/MachineLearning/24-01-30-Segment%20Anything%20Data%20Engine/#task-formulation\">Task Formulation</a></li>\n<li><a href=\"/MachineLearning/24-01-30-Segment%20Anything%20Data%20Engine/#platform-and-infrastructure\">Platform and Infrastructure</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/MachineLearning/24-01-30-Segment%20Anything%20Data%20Engine/#reference\">Reference</a></li>\n</ul>","frontmatter":{"path":"/deeplearning/24-01-30/","title":"Segment Anything Data Engine","category":"Deep Learning","date":"2024-01-30"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}