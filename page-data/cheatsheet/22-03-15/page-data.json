{"componentChunkName":"component---src-templates-post-js","path":"/cheatsheet/22-03-15/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>관심 분야의 논문 리스트를 기록합니다. 최근에 읽은 논문은 핵심 내용을 세 줄 요약으로 추가하고 있습니다. </p>\n</blockquote>\n<h3 id=\"few-shot--meta-learning\" style=\"position:relative;\"><a href=\"#few-shot--meta-learning\" aria-label=\"few shot  meta learning permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Few-shot / Meta Learning</h3>\n<ul>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://dl.acm.org/doi/abs/10.5555/3157382.3157504\">Vinyals, Oriol, et al. \"Matching networks for one shot learning.\" NIPS 2016.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"http://proceedings.mlr.press/v48/santoro16.pdf\">Santoro, et al. \"Meta-Learning with Memory-Augmented Neural Networks.\" ICML 2016.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://dl.acm.org/doi/abs/10.5555/3294996.3295163\">Snell, Jake, Kevin Swersky, and Richard Zemel. \"Prototypical networks for few-shot learning.\" NIPS 2017.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"http://proceedings.mlr.press/v97/yoon19a.html\">Yoon, Sung Whan, Jun Seo, and Jaekyun Moon. \"TapNet: Neural network augmented with task-adaptive projection for few-shot learning.\" ICML 2019.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"http://proceedings.mlr.press/v70/finn17a\">Finn, Chelsea, Pieter Abbeel, and Sergey Levine. \"Model-agnostic meta-learning for fast adaptation of deep networks.\" ICML 2017.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://arxiv.org/abs/2006.08306\">Mukaiyama, Kei, Issei Sato, and Masashi Sugiyama. \"LFD-Protonet: prototypical network based on local fisher discriminant analysis for few-shot learning.\" arXiv preprint arXiv:2006.08306, 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://arxiv.org/abs/1805.10002\">Liu, Yanbin, et al. \"Learning to propagate labels: Transductive propagation network for few-shot learning.\" ICLR 2019.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://openaccess.thecvf.com/content_CVPR_2019/html/Kim_Edge-Labeling_Graph_Neural_Network_for_Few-Shot_Learning_CVPR_2019_paper.html\">Kim, Jongmin, et al. \"Edge-labeling graph neural network for few-shot learning.\" CVPR 2019.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"http://proceedings.mlr.press/v119/goldblum20a.html\">Goldblum, Micah, et al. \"Unraveling meta-learning: Understanding feature representations for few-shot tasks.\" ICML 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"http://proceedings.mlr.press/v119/xu20i.html\">Xu, Jin, et al. \"Metafun: Meta-learning with iterative functional updates.\" ICML 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://dl.acm.org/doi/abs/10.5555/3327546.3327622\">Finn, Chelsea, Kelvin Xu, and Sergey Levine. \"Probabilistic model-agnostic meta-learning.\" ICML 2018.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openaccess.thecvf.com/content/WACV2021/html/Fortin_Towards_Contextual_Learning_in_Few-Shot_Object_Classification_WACV_2021_paper.html\">Fortin, Mathieu Page, and Brahim Chaib-draa. \"Towards Contextual Learning in Few-Shot Object Classification.\" CVPR 2021.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"http://proceedings.mlr.press/v119/bronskill20a.html\">Bronskill, et al. \"TaskNorm: Rethinking Batch Normalization for Meta-Learning.\" ICML 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://arxiv.org/abs/1803.02999\">Nichol, Alex, Joshua Achiam, and John Schulman. \"On first-order meta-learning algorithms.\" arXiv preprint arXiv:1803.02999 (2018).</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openreview.net/pdf?id=umIdUL8rMH\">Oh, Jaehoon, et al. \"BOIL: Towards Representation Change for Few-shot Learning.\" ICLR 2021.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://dl.acm.org/doi/abs/10.5555/3326943.3327010\">Oreshkin, Boris N., et al. \"Tadam: Task dependent adaptive metric for improved few-shot learning.\" NIPS 2018.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://arxiv.org/pdf/2012.04324.pdf\">Volpi, Riccardo, et al. \"Continual Adaptation of Visual Representations via Domain Randomization and Meta-learning.\" CVPR 2021.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://proceedings.neurips.cc/paper/2021/hash/bcb41ccdc4363c6848a1d760f26c28a0-Abstract.html\">Cha, Junbum, et al. \"Swad: Domain generalization by seeking flat minima.\" NIPS 2021.</a></li>\n</ul>\n<h3 id=\"incremental--continual-learning\" style=\"position:relative;\"><a href=\"#incremental--continual-learning\" aria-label=\"incremental  continual learning permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Incremental / Continual Learning</h3>\n<ul>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://ieeexplore.ieee.org/abstract/document/8107520\">Li, Zhizhong, and Derek Hoiem. \"Learning without forgetting.\" IEEE transactions on pattern analysis and machine intelligence 40.12 (2017): 2935-2947.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://www.sciencedirect.com/science/article/pii/S0893608019300231\">Parisi, German I., et al. \"Continual lifelong learning with neural networks: A review.\" Neural Networks 113 (2019): 54-71.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"http://proceedings.mlr.press/v119/yoon20b.html\">Yoon, Sung Whan, et al. \"XtarNet: Learning to Extract Task-Adaptive Representation for Incremental Few-Shot Learning.\" ICML 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://papers.nips.cc/paper/2019/hash/e833e042f509c996b1b25324d56659fb-Abstract.html\">Ren, Mengye, et al. \"Incremental Few-Shot Learning with Attention Attractor Networks.\" NIPS 2019.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://openaccess.thecvf.com/content_CVPR_2020/html/Tao_Few-Shot_Class-Incremental_Learning_CVPR_2020_paper.html\">Tao, Xiaoyu, et al. \"Few-shot class-incremental learning.\" CVPR 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://arxiv.org/abs/2007.04546\">Ren, Mengye, et al. \"Wandering within a world: Online contextualized few-shot learning.\" arXiv preprint arXiv:2007.04546, 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://ojs.aaai.org/index.php/AAAI/article/view/5942\">Luo, Yadan, et al. \"Learning from the Past: Continual Meta-Learning with Bayesian Graph Neural Networks.\" AAAI 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openaccess.thecvf.com/content_cvpr_2018/html/Gidaris_Dynamic_Few-Shot_Visual_CVPR_2018_paper.html\">Gidaris, Spyros, and Nikos Komodakis. \"Dynamic few-shot visual learning without forgetting.\" CVPR 2018.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://arxiv.org/abs/2103.04059\">Cheraghian, Ali, et al. \"Semantic-aware Knowledge Distillation for Few-Shot Class-Incremental Learning.\" CVPR 2021.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://ojs.aaai.org//index.php/AAAI/article/view/7079\">Liu, Bing. \"Learning on the job: Online lifelong and continual learning.\" AAAI 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://arxiv.org/abs/1606.04671\">Rusu, Andrei A., et al. \"Progressive neural networks.\" arXiv preprint arXiv:1606.04671 (2016).</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openreview.net/pdf?id=Sk7KsfW0-\">Yoon, Jaehong, et al. \"Lifelong Learning with Dynamically Expandable Networks.\" ICLR. 2018.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://arxiv.org/abs/2010.15277\">Masana, Marc, et al. \"Class-incremental learning: survey and performance evaluation.\" arXiv preprint arXiv:2010.15277 (2020).</a></li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" checked disabled> <a href=\"file:///C:/Users/ECE/Desktop/16334-Article%20Text-19828-1-2-20210518.pdf\">Mazumder, Pratik, Pravendra Singh, and Piyush Rai. \"Few-Shot Lifelong Learning.\" AAAI 2021.</a></p>\n<ul>\n<li>CEC와 동일하게 data-init을 사용하여 base, novel classifier 생성</li>\n<li>CE loss를 사용하지 않고 triplet loss, minimize cosine similarity loss(for prototype), regularization loss를 사용</li>\n<li>전체 weight 중에서 크기가 작은 10%만 골라서, 이를 novel sample에 대해서 fine-tuning 진행</li>\n</ul>\n</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://openaccess.thecvf.com/content/CVPR2021/html/Zhu_Self-Promoted_Prototype_Refinement_for_Few-Shot_Class-Incremental_Learning_CVPR_2021_paper.html\">Zhu, Kai, et al. \"Self-Promoted Prototype Refinement for Few-Shot Class-Incremental Learning.\" CVPR 2021.</a></li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" disabled> <a href=\"https://www.pnas.org/content/114/13/3521.short\">Kirkpatrick, James, et al. \"Overcoming catastrophic forgetting in neural networks.\" PNAS 2017.</a></p>\n<ul>\n<li>Fisher Information Matrix를 사용하여, parameter space 상에서 특정 covariance를 제약으로 parameter 학습이 이루어지도록 하는 알고리즘 (mahalanobis distance와 동일한 formulation)</li>\n<li>참고할 블로그 <a href=\"https://yukyunglee.github.io/Fisher-Information-Matrix/\">링크1</a>, <a href=\"https://nzer0.github.io/Fisher-Info-and-NLL-Hessian.html\">링크2</a></li>\n</ul>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" checked disabled> <a href=\"https://proceedings.neurips.cc/paper/2021/hash/357cfba15668cc2e1e73111e09d54383-Abstract.html\">Shi, Guangyuan, et al. \"Overcoming Catastrophic Forgetting in Incremental Few-Shot Learning by Finding Flat Minima.\" NeurIPS 2021.</a></p>\n<ul>\n<li>Robust optimization과 관계가 깊은 논문. Figure 2만 봐도 논문이 말하고자 하는 내용은 파악 가능</li>\n</ul>\n</li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" disabled> <a href=\"https://openreview.net/pdf?id=MEpKGLsY8f\">Chi, Haoang, et al. \"Meta discovery: Learning to discover novel classes given very limited data.\" ICLR 2022.</a></p>\n<ul>\n<li>Learning to discover novel classes(L2DNC) task에 대해서 이전 가정이 이론적으로 잘못되었다는 것을 증명하고, 이론적으로 가능한 L2DNC 상황을 재정립함. 이와 더불어 더 실생활과 관련있는 few novel observation 상황을 가정하여, L2DNCL task를 정의함.</li>\n<li>재정립한 L2DNC이 meta-learning의 가정과 유사하여 MAML, ProtoNet의 아이디어를 차용한 MM, MP를 제안함.</li>\n<li>그리고 meta-learning에 L2DNC를 접목할 수 있도록 Clustering-rule-aware Task Sampler(CATA)를 제안함.</li>\n</ul>\n</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openaccess.thecvf.com/content/ICCV2021/html/Cha_Co2L_Contrastive_Continual_Learning_ICCV_2021_paper.html\">Cha, Hyuntak, Jaeho Lee, and Jinwoo Shin. \"Co2l: Contrastive continual learning.\" ICCV 2021.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openreview.net/pdf?id=gYgMSlZznS\">Hu, Dapeng, et al. \"How Well Does Self-Supervised Pre-Training Perform with Streaming ImageNet?.\" NeruIPS 2021.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openreview.net/pdf?id=9Hrka5PA7LW\">Madaan, Divyam, et al. \"Rethinking the Representational Continuity: Towards Unsupervised Continual Learning.\" ICLR 2022.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://arxiv.org/pdf/2112.04215.pdf\">Fini, Enrico, et al. \"Self-Supervised Models are Continual Learners.\" arXiv preprint arXiv:2112.04215, 2021.</a></li>\n</ul>\n<h3 id=\"self--semi-supervised-learning\" style=\"position:relative;\"><a href=\"#self--semi-supervised-learning\" aria-label=\"self  semi supervised learning permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Self / Semi-supervised Learning</h3>\n<ul>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://arxiv.org/abs/2006.07733\">Grill, Jean-Bastien, et al. \"Bootstrap your own latent: A new approach to self-supervised learning.\" NIPS 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://openaccess.thecvf.com/content/CVPR2021/html/Pham_Meta_Pseudo_Labels_CVPR_2021_paper.html\">Pham, Hieu, et al. \"Meta pseudo labels.\" CVPR 2021.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openaccess.thecvf.com/content_CVPR_2019/html/Kolesnikov_Revisiting_Self-Supervised_Visual_Representation_Learning_CVPR_2019_paper.html\">Kolesnikov, Alexander, Xiaohua Zhai, and Lucas Beyer. \"Revisiting self-supervised visual representation learning.\" CVPR 2019.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://arxiv.org/abs/2102.06810\">Tian, Yuandong, Xinlei Chen, and Surya Ganguli. \"Understanding self-supervised Learning Dynamics without Contrastive Pairs.\" arXiv preprint arXiv:2102.06810, 2021.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://arxiv.org/abs/2103.05247?fbclid=IwAR3T_ZxXT0bmygQnpbWdPy_9_ilNR9nrCbALNgc6EffsXAevguFxQ_myPFE\">Kevin Lu, et al. \"Pretrained Transformers as Universal Computation Engines.\" arXiv preprint arXiv:2103.05247, 2021</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://arxiv.org/abs/2103.01988\">Goyal, Priya, et al. \"Self-supervised Pretraining of Visual Features in the Wild.\" arXiv preprint arXiv:2103.01988, 2021.</a></li>\n<li class=\"task-list-item\">\n<p><input type=\"checkbox\" checked disabled> <a href=\"https://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/ijcai05.pdf\">Zhou, Zhi-Hua, and Ming Li. \"Semi-supervised regression with co-training.\" IJCAI 2005.</a></p>\n<ul>\n<li>두 개의 kNN regressor를 사용하여 co-training 진행</li>\n<li>Sufficient and redundant view를 위해서 두 regressor의 metric은 p=2 Minkowsky와 p=5 Minkowsky로 서로 다르게 설정함 </li>\n<li>Key mechanism은 regression에서 confidence를 만들어내는 작업이었고, <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>u</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_u</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>가 추가됨에 따라서 MSE가 얼마나 개선되는지를 계산하여(<span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mo>△</mo><mi>u</mi></msub></mrow><annotation encoding=\"application/x-tex\">\\vartriangle_u</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69986em;vertical-align:-0.15em;\"></span><span class=\"mrel\"><span class=\"mrel amsrm\">△</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>) 이 값이 제일 커지는 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>u</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_u</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에 대해 confidence가 높다고 판단하여 해당 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>x</mi><mi>u</mi></msub></mrow><annotation encoding=\"application/x-tex\">x_u</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.58056em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">x</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.151392em;\"><span style=\"top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">u</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span>에 pseudo label을 부여</li>\n</ul>\n</li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://par.nsf.gov/servlets/purl/10080181\">Jean, Neal, Sang Michael Xie, and Stefano Ermon. \"Semi-supervised Deep Kernel Learning: Regression with Unlabeled Data by Minimizing Predictive Variance.\" NIPS 2018.</a></li>\n</ul>\n<h3 id=\"reinforcement-learning\" style=\"position:relative;\"><a href=\"#reinforcement-learning\" aria-label=\"reinforcement learning permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Reinforcement Learning</h3>\n<ul>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://arxiv.org/pdf/1611.05763.pdf\">Wang, Jane X., et al. \"Learning to reinforcement learn.\" arXiv preprint arXiv:1611.05763, 2016</a></li>\n</ul>\n<h3 id=\"natural-language-processing\" style=\"position:relative;\"><a href=\"#natural-language-processing\" aria-label=\"natural language processing permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Natural Language Processing</h3>\n<ul>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openreview.net/pdf?id=cu7IUiOhujH\">Gunel, Beliz, et al. \"Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning.\" ICLR 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://aclanthology.org/2020.acl-main.45.pdf\">Li, Xiaoya, et al. \"Dice Loss for Data-imbalanced NLP Tasks.\" ACL 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://aclanthology.org/C18-1182.pdf\">Yadav, Vikas, and Steven Bethard. \"A Survey on Recent Advances in Named Entity Recognition from Deep Learning models.\" COLING 2018.</a></li>\n</ul>\n<h3 id=\"etc\" style=\"position:relative;\"><a href=\"#etc\" aria-label=\"etc permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>etc.</h3>\n<ul>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://arxiv.org/pdf/2105.07576.pdf\">Wu, Yuxin, and Justin Johnson. \"Rethinking\" Batch\" in BatchNorm.\" arXiv preprint arXiv:2105.07576, 2021</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://arxiv.org/abs/1807.03341\">Lipton, Zachary C., and Jacob Steinhardt. \"Troubling trends in machine learning scholarship.\" arXiv preprint arXiv:1807.03341, 2018.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"http://proceedings.mlr.press/v48/gal16.html\">Gal, Yarin, and Zoubin Ghahramani. \"Dropout as a bayesian approximation: Representing model uncertainty in deep learning.\" ICML 2016.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://proceedings.neurips.cc/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html\">Khosla, Prannay, et al. \"Supervised contrastive learning.\" NIPS 2020.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://proceedings.neurips.cc/paper/2016/hash/6b180037abbebea991d8b1232f8a8ca9-Abstract.html\">Sohn, Kihyuk. \"Improved deep metric learning with multi-class n-pair loss objective.\" NIPS 2016.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" disabled> <a href=\"https://openaccess.thecvf.com/content_cvpr_2018/html/He_Triplet-Center_Loss_for_CVPR_2018_paper.html\">He, Xinwei, et al. \"Triplet-center loss for multi-view 3d object retrieval.\" CVPR 2018.</a></li>\n<li class=\"task-list-item\"><input type=\"checkbox\" checked disabled> <a href=\"https://openaccess.thecvf.com/content/CVPR2021/html/Martin-Brualla_NeRF_in_the_Wild_Neural_Radiance_Fields_for_Unconstrained_Photo_CVPR_2021_paper.html\">Martin-Brualla, Ricardo, et al. \"Nerf in the wild: Neural radiance fields for unconstrained photo collections.\" CVPR 2021.</a></li>\n</ul>","tableOfContents":"<ul>\n<li><a href=\"/Archive/22-03-15-Deep%20Learning%20Paper%20List/#few-shot--meta-learning\">Few-shot / Meta Learning</a></li>\n<li><a href=\"/Archive/22-03-15-Deep%20Learning%20Paper%20List/#incremental--continual-learning\">Incremental / Continual Learning</a></li>\n<li><a href=\"/Archive/22-03-15-Deep%20Learning%20Paper%20List/#self--semi-supervised-learning\">Self / Semi-supervised Learning</a></li>\n<li><a href=\"/Archive/22-03-15-Deep%20Learning%20Paper%20List/#reinforcement-learning\">Reinforcement Learning</a></li>\n<li><a href=\"/Archive/22-03-15-Deep%20Learning%20Paper%20List/#natural-language-processing\">Natural Language Processing</a></li>\n<li><a href=\"/Archive/22-03-15-Deep%20Learning%20Paper%20List/#etc\">etc.</a></li>\n</ul>","frontmatter":{"path":"/cheatsheet/22-03-15/","title":"Deep Learning Paper List","category":"Cheat Sheet","date":"2022-03-15"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}