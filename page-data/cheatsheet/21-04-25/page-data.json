{"componentChunkName":"component---src-templates-post-js","path":"/cheatsheet/21-04-25/","result":{"data":{"markdownRemark":{"html":"<p>새롭게 알게 된 지식 중에서 하나의 포스팅으로 만들기에는 부담스러운 내용들을 이곳에 모아둡니다. 매일 공부한 내용을 기록하기보다는 아무때나 업데이트 할 생각입니다! 나중에는 카테고리 포스팅을 나눌 수 있을 정도로 내용이 엄청 많아졌으면 좋겠네요 🤓</p>\n<blockquote>\n<p>최근에 작성한 내용들이 하단에 위치하도록 배열하였습니다.</p>\n</blockquote>\n<h5 id=\"🥧-python\" style=\"position:relative;\"><a href=\"#%F0%9F%A5%A7-python\" aria-label=\"🥧 python permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🥧 Python</h5>\n<p><em>2021.04.25</em></p>\n<p><a href=\"https://docs.python.org/3/reference/simple_stmts.html#future\">파이썬 도큐먼트</a>의 <code class=\"language-text\">future</code> 문에 대한 설명을 읽었습니다. <code class=\"language-text\">future</code> 문은 미래 버전 파이썬의 기능들을 쉽게 마이그레이션(하나의 운영환경에서 다른 운영환경으로 옮기는 것)하기 위해 만들어졌습니다. import 뒤에 따라오는 new feature가 만약 파이썬 3의 기능이라고 하더라도 파이썬 2 버전에서 사용 가능하게 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">from</span> __future__ <span class=\"token keyword\">import</span> print_function</code></pre></div>\n<h5 id=\"🧩-ml-library\" style=\"position:relative;\"><a href=\"#%F0%9F%A7%A9-ml-library\" aria-label=\"🧩 ml library permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🧩 ML library</h5>\n<p><em>2021.04.25</em> </p>\n<p><a href=\"https://www.tensorflow.org/versions/r1.15/api_docs/python/tf/map_fn\">텐서플로우 공식문서</a>의 <code class=\"language-text\">tf.map_fn</code> 함수에 대한 설명을 읽었습니다. dimension 0에서 unpack된 elems이라는 tensor list의 요소들을 fn에 map합니다. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">tf<span class=\"token punctuation\">.</span>map_fn<span class=\"token punctuation\">(</span>fn<span class=\"token punctuation\">,</span> elems<span class=\"token punctuation\">,</span> dtype<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> parallel_iterations<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">,</span> back_prop<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span>\n    \t  swap_memory<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span> infer_shape<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">,</span> name<span class=\"token operator\">=</span><span class=\"token boolean\">None</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>MAML을 구현 할 때 meta-batch에 대한 cross entropy를 병렬적으로 계산하기 위해서 아래와 같은 코드를 사용할 수 있습니다. 여기서 xs의 shape은 [meta-batch size, nway*kshot, 84*84*3] 입니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">cent<span class=\"token punctuation\">,</span> acc <span class=\"token operator\">=</span> tf<span class=\"token punctuation\">.</span>map_fn<span class=\"token punctuation\">(</span><span class=\"token keyword\">lambda</span> inputs<span class=\"token punctuation\">:</span> self<span class=\"token punctuation\">.</span>get_loss_single<span class=\"token punctuation\">(</span>inputs<span class=\"token punctuation\">,</span> weights<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\t\t\t\t\t elems<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>xs<span class=\"token punctuation\">,</span> ys<span class=\"token punctuation\">,</span> xq<span class=\"token punctuation\">,</span> yq<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\t\t\t\t \t dtype<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span>tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">,</span> tf<span class=\"token punctuation\">.</span>float32<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n\t\t\t\t \t parallel_iterations<span class=\"token operator\">=</span>self<span class=\"token punctuation\">.</span>metabatch<span class=\"token punctuation\">)</span></code></pre></div>\n<h5 id=\"🧩--ml-library\" style=\"position:relative;\"><a href=\"#%F0%9F%A7%A9--ml-library\" aria-label=\"🧩  ml library permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🧩  ML library</h5>\n<p><em>2021.04.27</em></p>\n<p>모델 그래프를 빌드하는 함수에서 for loop를 많이 사용하면 이게 그대로 모델 training 단계에서도 매번 for loop가 적용되어 모델의 학습이 느려지겠구나라고 생각했었는데 곰곰히 생각해보니까 아니더라구요. </p>\n<p>빌드하는 단계에서는 for loop가 여러 번 돌더라도, 그래프의 각 노드들이 연결되고 난 뒤에는 빌드 된 그래프 구조 자체가 중요하지, 빌드 단계에서의 for loop는 관련이 없게 됩니다. 꽤나 오랫동안 아무렇지 않게 착각하고 있었어서 이 곳에 기록합니다. 그럼 map_fn은 특히 어떤 경우에 메리트를 가질까 궁금하긴 하네요 🧐</p>\n<h5 id=\"🧩--ml-library-1\" style=\"position:relative;\"><a href=\"#%F0%9F%A7%A9--ml-library-1\" aria-label=\"🧩  ml library 1 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🧩  ML library</h5>\n<p><em>2021.05.02</em></p>\n<p>TensorFlow 1.15로 코드를 짜다가 <code class=\"language-text\">softmax_cross_entropy_with_logits</code>는 loss에 대한 2nd-order 계산을 지원하지만 <code class=\"language-text\">sparse_softmax_cross_entropy_with_logits</code>는 loss에 대한 2nd-order 계산을 지원하지 않는다는걸 알게 되었습니다. 이 둘의 차이는 label이 one-hot 형태로 주어지냐 아니냐의 차이밖에 없는데 이런 결과를 나타냈다는게 이상해서 찾아보다가 tensorflow repository에 <a href=\"https://github.com/tensorflow/tensorflow/issues/5876\">관련 이슈</a>가 올라왔던 것을 발견했습니다.</p>\n<p>요약하자면 일부 indexing 작업에 대한 도함수 계산이 아직 제대로 구현되지 않았거나, 몇 가지 operation에 대해서 2차 미분 계산이 개발자들도 아직 해결하지 못한 오류를 가진다고 말하고 있습니다(구체적인 원인은 모르겠습니다). 0.2 버전에서 1.15 까지 개발이 진행되면서도 TensorFlow 팀이 지속적으로 해결하지 못하고 있는 문제점이 있다는 것이 신기했습니다.</p>\n<h5 id=\"-ml--dl\" style=\"position:relative;\"><a href=\"#-ml--dl\" aria-label=\" ml  dl permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🤖 ML &#x26; DL</h5>\n<p><em>2021.05.10</em></p>\n<p><a href=\"https://www.youtube.com/watch?v=KQmZlxdnnuY\">PR-317: MLP-Mixer: An all-MLP Architecture for Vision</a> 영상을 통해 CNN과 MLP가 별로 다르지 않다는 것을 알았습니다. 영상에서 이진원님은 CNN weight이 Fully-Conneted weight과 다른 점 두 가지가 weight sharing과 locally connected라고 설명하고 있습니다. 시각화된 자료만 봐도 이렇게 간단하게 이해되는 내용인데 왜 지금까지 깨닫지 못했을까라는 생각이 들었고, CNN에 몇 개의(사실은 엄청 많은 양이지만) weight을 추가하는 것만으로도 Fully-Connected와 완전히 동일한 구조로 만들수 있다는 것을 이해했습니다.</p>\n<h5 id=\"🧩--ml-library-2\" style=\"position:relative;\"><a href=\"#%F0%9F%A7%A9--ml-library-2\" aria-label=\"🧩  ml library 2 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🧩  ML library</h5>\n<p><em>2021.05.11</em></p>\n<p><code class=\"language-text\">tf.contrib.layers.batch_norm</code> 함수를 사용할 때 <code class=\"language-text\">is_traning</code> 아규먼트 설정에 주의해야 합니다. Batch normalization을 사용할 때 학습 상황인지 테스트 상황인지에 따라서 mean과 variance로 사용하는 statistics의 출처가 달라지기 때문에 <code class=\"language-text\">is_traning</code>를 잘못 설정한다면 정확도는 높게 나오더라도 그 실험이 잘못된 결과일 수 있습니다.</p>\n<p><code class=\"language-text\">is_training</code>이 True인 경우에는 moving<em>mean 텐서와 moving</em>variance 텐서에 statistics of the moments(미니 배치 평균과 분산)을 exponential moving average 식에 따라 축적합니다. BN 계산에는 미니배치의 평균과 분산을 사용합니다.  <code class=\"language-text\">is_training</code>이 False인 경우에는 그동안 축적하였던 moving<em>mean 텐서와 moving</em>variance 텐서 값을 가져와 BN 계산에 사용합니다. </p>\n<p>Few-shot learning setting에서 support set과 query set에 대해서 둘 다 <code class=\"language-text\">is_training</code>을 True로 설정하면 이는 transductive setting이 됩니다. 즉 query를 추정하기 위해서 support 뿐만 아니라 query 분포의 정보까지 사용하겠다는 것을 의미합니다. Few-shot learning에서는 대부분 transductive setting이 non-transductive에 비해 3%정도의 성능 향상을 보이기 때문에 본인의 실험 상황에 알맞게 아규먼트 값을 설정해야 합니다. </p>\n<p><code class=\"language-text\">tf.contrib.layers.group_norm</code> 같은 instance-based normalization 방식은 미니배치에 대한 running statistics를 사용하지 않기 때문에 <code class=\"language-text\">is_tranable</code> 파라미터가 존재하지 않습니다.</p>\n<h5 id=\"-ml--dl-1\" style=\"position:relative;\"><a href=\"#-ml--dl-1\" aria-label=\" ml  dl 1 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🤖 ML &#x26; DL</h5>\n<p><em>2021.05.14</em></p>\n<p>Moment<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup>는 물리학에서 특정 물리량과 distance의 곱을 통해 물리량이 공간상 어떻게 위치하는지를 나타내며 Force, Torque, Angular momentum 등을 예로 들 수 있습니다. Moment of mass에 대해서 zeroth moment는 total mass, 1st moment는 center of mass, 2nd moment는 moment of inertia를 의미합니다.</p>\n<p>수학에서는 함수의 특징을 나타내기위해 moment라는 워딩을 사용합니다. 함수가 확률분포 형태인 경우 first moment는 확률 분포의 기댓값을 의미하며, 이를 moments about zero라고도 말합니다. 또한 second central moment로는 variance, third standardized moment는 skewness(비대칭도),  fourth standardized moment는 kurtosis(첨도, 뾰족한 정도) 등이 있습니다.</p>\n<h5 id=\"-cs\" style=\"position:relative;\"><a href=\"#-cs\" aria-label=\" cs permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👨‍💻 CS</h5>\n<p><em>2021.05.24</em></p>\n<p><a href=\"https://ko.wikipedia.org/wiki/API\">API</a>(Application Programming Interfaces)<sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup>는 응용 프로그램에서 사용할 수 있도록, 운영 체제나 프로그래밍 언어가 제공하는 기능을 제어할 수 있게 만든 인터페이스를 말합니다. 외부와 새로운 연결들을 구축할 필요 없이 내부 기능들이 서로 잘 통합되어 있으며, API를 사용하면 해당 API의 자세한 작동원리와 구현방식은 알지 못해도, 제품/서비스간에 커뮤니케이션이 가능합니다.</p>\n<p>웹 API가 늘어나면서 메세지 전달을 위한 표준을 만들고자 SOAP(Simple Object Access Protocol)가 개발되었고, 최근 웹 API로는 <a href=\"https://ko.wikipedia.org/wiki/REST\">REST</a>ful API라는 <em>아키텍쳐 스타일</em>이 더 많이 사용되고 있습니다. REST는 규정된 프로토콜이 아니라 아키텍쳐 스타일이기 때문에 정해진 표준은 없습니다. 다만 Roy Fielding의 논문에 정의된 아래의 6가지 원칙을 기본으로 합니다. (자세한 설명은 위키피디아 문서<sup id=\"fnref-3\"><a href=\"#fn-3\" class=\"footnote-ref\">3</a></sup> 참고)</p>\n<ul>\n<li><code class=\"language-text\">인터페이스 일관성</code>, <code class=\"language-text\">무상태(Stateless)</code>, <code class=\"language-text\">캐시 처리 가능(Cacheable)</code>, <code class=\"language-text\">계층화(Layered System)</code>, <code class=\"language-text\">Code on demand (optional)</code>, <code class=\"language-text\">클라이언트/서버 구조</code></li>\n</ul>\n<p><a href=\"https://ko.wikipedia.org/wiki/%ED%86%B5%ED%95%A9_%EC%9E%90%EC%9B%90_%EC%8B%9D%EB%B3%84%EC%9E%90\">URI</a>는 Uniform Resource Identifier(통합 자원 식별자)<sup id=\"fnref-4\"><a href=\"#fn-4\" class=\"footnote-ref\">4</a></sup>의 약자로 특정 자원의 위치를 나타내주는 유일한 주소를 말합니다. RESTful API는 웹 상에서 사용되는 리소스를 HTTP URI로 표현하고, 리소스에 대한 작업들을 HTTP Method로 정의합니다.</p>\n<h5 id=\"🥧-python-1\" style=\"position:relative;\"><a href=\"#%F0%9F%A5%A7-python-1\" aria-label=\"🥧 python 1 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🥧 Python</h5>\n<p><em>2021.05.24</em></p>\n<p>파이썬의 객체는 그 속성이 mutable(값이 변한다)과 immutable로 구분됩니다. (<a href=\"https://wikidocs.net/32277\">이곳</a>과 <a href=\"https://wikidocs.net/16038\">이곳</a>을 참고하였습니다.)</p>\n<ul>\n<li>Immutable : 숫자(number), 문자열(string), 튜플(tuple)</li>\n<li>Mutable : 리스트(list), 딕셔너리(dictionary), NumPy의 배열(ndarray)</li>\n</ul>\n<p>Immutable 타입인 int에 대해 예를 들어 보겠습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">x <span class=\"token operator\">=</span> <span class=\"token number\">1</span>\ny <span class=\"token operator\">=</span> x\ny <span class=\"token operator\">+=</span> <span class=\"token number\">3</span>\n\n<span class=\"token comment\"># results: x = 1, y = 4</span></code></pre></div>\n<p>두 번째 라인까지는 x와 y가 1이라는 동일한 <strong><em>객체</em></strong>를 가리키고 있습니다. 세 번째에서 y의 값을 변경하는 순간 y는 4를, x는 1을 가리키게 됩니다.</p>\n<p>C/C++같은 언어 관점에서 보면 <code class=\"language-text\">y=x</code>가 실행하는 순간 값을 복사하는 것으로 이해할 수 있지만, 파이썬은 <code class=\"language-text\">y=x</code>가 호출되는 시점에는 동일한 객체를 가리키다가 immutable 타입인 y를 변경했을 때 변경됩니다.</p>\n<h5 id=\"🥧-python-2\" style=\"position:relative;\"><a href=\"#%F0%9F%A5%A7-python-2\" aria-label=\"🥧 python 2 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🥧 Python</h5>\n<p><em>2021.08.05</em></p>\n<p>최근에 알게된 유용한 Pycharm 단축키를 정리합니다.</p>\n<ul>\n<li>변수/함수가 사용된 위치 찾기: <code class=\"language-text\">Find Usages</code>, <code class=\"language-text\">Alt + F7</code> (<code class=\"language-text\">Option + F7</code>)</li>\n<li>변수/함수 선언부 찾기: <code class=\"language-text\">Ctrl + 클릭</code> (<code class=\"language-text\">Command + 클릭</code>)</li>\n</ul>\n<h5 id=\"-cs-1\" style=\"position:relative;\"><a href=\"#-cs-1\" aria-label=\" cs 1 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👨‍💻 CS</h5>\n<p><em>2021.08.25</em></p>\n<p>FLOPS<sup id=\"fnref-9\"><a href=\"#fn-9\" class=\"footnote-ref\">9</a></sup> (FLoating point Operations Per Second)는 '1초 당 부동소수점 연산량'을 의미합니다. 컴퓨터의 성능을 나타낼 때 주로 사용됩니다. 슈퍼 컴퓨터의 성능을 나타낼 경우에는 테라플롭스 TFLOPS(1×1012 플롭스)가 주로 쓰이며 PFLOPS는 페타플롭스를 의미합니다.</p>\n<p>FLOPS와 FLOPs의 의미는 다릅니다. FLOPs는 FLoating point Operations의 약자인데, 이는 '부동소수점 연산량'을 의미합니다. FLOPs 같은 경우에는 딥러닝 커뮤니티에서 모델의 크기, 모델의 연산량을 나타내는데 사용됩니다.</p>\n<h5 id=\"🧩-ml-library-1\" style=\"position:relative;\"><a href=\"#%F0%9F%A7%A9-ml-library-1\" aria-label=\"🧩 ml library 1 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🧩 ML library</h5>\n<p><em>2021.09.20</em></p>\n<p><a href=\"https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze\">PyTorch 공식 문서</a>를 참고하여 가장 기본적인 torch Tensor 기능들을 정리합니다.</p>\n<ul>\n<li>squeeze: 차원이 1인 차원을 제거하는 함수입니다. 따로 옵션을 주지 않으면 차원이 1인 모든 차원을 제거합니다.</li>\n<li>unsqueeze: 특정 위치에 1인 차원을 추가하는 함수힙니다.</li>\n<li>view: 텐서의 shape을 변경해주는 함수입니다.</li>\n</ul>\n<h5 id=\"🥧-python-3\" style=\"position:relative;\"><a href=\"#%F0%9F%A5%A7-python-3\" aria-label=\"🥧 python 3 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🥧 Python</h5>\n<p><em>2021.09.30</em></p>\n<p><a href=\"https://docs.python.org/ko/3/tutorial/modules.html\">Python 공식 문서</a>를 참고하여 모듈과 모듈성에 대해 정리합니다.</p>\n<p>프로그램의 유지/보수를 위해 여러 개의 파일로 나누고 싶거나, 함수를 여러 프로그램에 복사하지 않고도 사용하고 싶은 경우에, 파이썬은 정의들을 파일에 넣고 사용할 수 있는 방법을 제공합니다. 그런 파일을 모듈<sup id=\"fnref-10\"><a href=\"#fn-10\" class=\"footnote-ref\">10</a></sup>이라고 부릅니다. 즉, 다른 파이썬 프로그램에서 불러와 사용할 수 있도록 만든 또 다른 파이썬 파일을 모듈이라고 합니다.</p>\n<h5 id=\"-ml--dl-2\" style=\"position:relative;\"><a href=\"#-ml--dl-2\" aria-label=\" ml  dl 2 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🤖 ML &#x26; DL</h5>\n<p><em>2021.11.13</em></p>\n<p>위키피디아의 Signed Distance Function(SDF)<sup id=\"fnref-12\"><a href=\"#fn-12\" class=\"footnote-ref\">12</a></sup>에 대한 설명을 읽었습니다. 먼저, SDF는 다음과 같이 정의됩니다.</p>\n<ul>\n<li>If <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Ω</mi></mrow><annotation encoding=\"application/x-tex\">\\Omega</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Ω</span></span></span></span> is a subset of a metric space and <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>b</mi></mrow><annotation encoding=\"application/x-tex\">b</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.69444em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">b</span></span></span></span> is the boundary of <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Ω</mi></mrow><annotation encoding=\"application/x-tex\">\\Omega</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.68333em;vertical-align:0em;\"></span><span class=\"mord\">Ω</span></span></span></span> the signed distance function <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>f</mi></mrow><annotation encoding=\"application/x-tex\">f</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span></span></span></span> is defined by</li>\n</ul>\n<span class=\"katex-display\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\"><semantics><mrow><mi>f</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo stretchy=\"false\">)</mo><mo>=</mo><mrow><mo fence=\"true\">{</mo><mtable rowspacing=\"0.3599999999999999em\" columnalign=\"left left\" columnspacing=\"1em\"><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mi>d</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">∂</mi><mi mathvariant=\"normal\">Ω</mi><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if </mtext><mi>x</mi><mo>∈</mo><mi mathvariant=\"normal\">Ω</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mo>−</mo><mi>d</mi><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi mathvariant=\"normal\">∂</mi><mi mathvariant=\"normal\">Ω</mi><mo stretchy=\"false\">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel=\"0\" displaystyle=\"false\"><mrow><mtext>if </mtext><mi>x</mi><mo>∈</mo><msup><mi mathvariant=\"normal\">Ω</mi><mi>c</mi></msup></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding=\"application/x-tex\">f(x)=\n\\begin{cases}\nd(x, \\partial \\Omega) &amp; \\text{if } x \\in \\Omega \\\\\n-d(x, \\partial \\Omega) &amp; \\text{if } x \\in \\Omega^c\n\\end{cases}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:3.0000299999999998em;vertical-align:-1.25003em;\"></span><span class=\"minner\"><span class=\"mopen delimcenter\" style=\"top:0em;\"><span class=\"delimsizing size4\">{</span></span><span class=\"mord\"><span class=\"mtable\"><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.69em;\"><span style=\"top:-3.69em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\">Ω</span><span class=\"mclose\">)</span></span></span><span style=\"top:-2.25em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord\">−</span><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\" style=\"margin-right:0.05556em;\">∂</span><span class=\"mord\">Ω</span><span class=\"mclose\">)</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.19em;\"><span></span></span></span></span></span><span class=\"arraycolsep\" style=\"width:1em;\"></span><span class=\"col-align-l\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.69em;\"><span style=\"top:-3.69em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">if </span></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\">Ω</span></span></span><span style=\"top:-2.25em;\"><span class=\"pstrut\" style=\"height:3.008em;\"></span><span class=\"mord\"><span class=\"mord text\"><span class=\"mord\">if </span></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mord\"><span class=\"mord\">Ω</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.664392em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">c</span></span></span></span></span></span></span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.19em;\"><span></span></span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>\n<p>SDF는 어떤 boundary까지의 거리를 표현하는 함수입니다. 만약 어떤 점 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span>가 boundary 안 쪽에 위치하게 되면 function 값은 양수를 갖게 되며, 이 점이 boundary와 점점 가깝게 이동할 수록 function 값은 0에 가까워 지다가, boundary에 위치하는 경우에는 0이 됩니다. 반대로 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi></mrow><annotation encoding=\"application/x-tex\">x</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">x</span></span></span></span>가 boundary 바깥 쪽에 위치하는 경우에는 function 값이 음수를 갖습니다.</p>\n<p>위에서는 SDF 함수의 식에 대해서 boundary 안 쪽인 경우에 양수라고 표기하였지만 boundary 안 쪽을 음수로 두어 반대로 사용하는 경우도 존재합니다. 아래 사진은 DeepSDF<sup id=\"fnref-13\"><a href=\"#fn-13\" class=\"footnote-ref\">13</a></sup>라는 논문에서 가져온 SDF의 예시이며 해당 논문에서는 boundary 안 쪽을 음수로 두었습니다.</p>\n<p><span\n      class=\"gatsby-resp-image-wrapper\"\n      style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 760px; \"\n    >\n      <a\n    class=\"gatsby-resp-image-link\"\n    href=\"/static/efcd25394a6bbab0c8413cad7da84c0d/9d5da/21-11-14-2.png\"\n    style=\"display: block\"\n    target=\"_blank\"\n    rel=\"noopener\"\n  >\n    <span\n    class=\"gatsby-resp-image-background-image\"\n    style=\"padding-bottom: 26.842105263157894%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAzElEQVQY042PTQsBYRSF/Te/wtJaNtYKWUhvoRQLSgpZMUlh2M1OwpDCDvkaHzNoDObjmGZB9CqnbmdxT8+9x4YvGYZhua7reNxVCKs1zqczdssZhhwLcbt87Wmy/QSafjGBe0WFoumoJMOopQjm4yEkYf2R/QuoyTKubB1KguDQ7SEb8YOJ+VA2p5WJQj4dqVAq0CojbVANtUHsRahsEzzfQdzjBHE5kA+4sRjz1OpUoHVTFDCNMygF+7iNJtauUUgjR7wYcM139uvDJ8FzdBj6U+nvAAAAAElFTkSuQmCC'); background-size: cover; display: block;\"\n  ></span>\n  <picture>\n          <source\n              srcset=\"/static/efcd25394a6bbab0c8413cad7da84c0d/15813/21-11-14-2.webp 190w,\n/static/efcd25394a6bbab0c8413cad7da84c0d/1cdb2/21-11-14-2.webp 380w,\n/static/efcd25394a6bbab0c8413cad7da84c0d/9046c/21-11-14-2.webp 760w,\n/static/efcd25394a6bbab0c8413cad7da84c0d/81b74/21-11-14-2.webp 871w\"\n              sizes=\"(max-width: 760px) 100vw, 760px\"\n              type=\"image/webp\"\n            />\n          <source\n            srcset=\"/static/efcd25394a6bbab0c8413cad7da84c0d/a2d4f/21-11-14-2.png 190w,\n/static/efcd25394a6bbab0c8413cad7da84c0d/3f520/21-11-14-2.png 380w,\n/static/efcd25394a6bbab0c8413cad7da84c0d/3c051/21-11-14-2.png 760w,\n/static/efcd25394a6bbab0c8413cad7da84c0d/9d5da/21-11-14-2.png 871w\"\n            sizes=\"(max-width: 760px) 100vw, 760px\"\n            type=\"image/png\"\n          />\n          <img\n            class=\"gatsby-resp-image-image\"\n            src=\"/static/efcd25394a6bbab0c8413cad7da84c0d/3c051/21-11-14-2.png\"\n            alt=\"img\"\n            title=\"img\"\n            loading=\"lazy\"\n            style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\"\n          />\n        </picture>\n  </a>\n    </span></p>\n<p>과거의 surface 추정이나 3D reconstruction 같은 task에서는 주로 voxel, point, mesh를 사용하는 방식으로 접근했다면, 최근에는 SDF 사용하려는 시도가 늘어나고 있는 것 같습니다. 특히 Implicit Neural Representation 연구와 SDF를 결합한 연구 결과들이 흥미로워 보였습니다.</p>\n<p>Implicit Neural Representation은 이미지나 3D 데이터를 pixel, voxel 단위의 matrix 형태로 표현하는 것이 아니라, (x, y) 값을 받았을 때 (r, g, b) 값을 출력하는 어떤 함수 하나로써 표현하려는 연구입니다(함수 1개는 데이터 1개를 의미하고, 따라서 학습 입력 1개는 픽셀 값 1개로 주어지게 될 듯 합니다). 데이터를 연속적인 함수의 형태로 표현하기 때문에 자연스럽게 super resolution이 가능하다는 장점이 있는데, 최근에 이 방식과 SDF를 결합하여 최종 output을 매우 매끄럽게 만들어내고자 하는 연구가 많이 진행되고 있습니다.</p>\n<h5 id=\"-ml--dl-3\" style=\"position:relative;\"><a href=\"#-ml--dl-3\" aria-label=\" ml  dl 3 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🤖 ML &#x26; DL</h5>\n<p><em>2021.12.02</em></p>\n<p>지금까지는 아무 생각 없이 continuous distribution에서도 single point에 특정 확률이 존재한다고 생각했습니다. 예를 들어 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"script\">N</mi><mo stretchy=\"false\">(</mo><mn>0</mn><mo separator=\"true\">,</mo><mn>1</mn><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\mathcal N (0, 1)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathcal\" style=\"margin-right:0.14736em;\">N</span><span class=\"mopen\">(</span><span class=\"mord\">0</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord\">1</span><span class=\"mclose\">)</span></span></span></span>에 대해서 point <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>x</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding=\"application/x-tex\">x=1</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.43056em;vertical-align:0em;\"></span><span class=\"mord mathnormal\">x</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">1</span></span></span></span>이 관측될 확률이 특정 값으로 존재한다고 잘못 생각하고 있었습니다.</p>\n<p><a href=\"https://www.itl.nist.gov/div898/handbook/eda/section3/eda361.htm\">이 곳</a><sup id=\"fnref-14\"><a href=\"#fn-14\" class=\"footnote-ref\">14</a></sup>을 참고하니 continuous probability function은 continuous interval의 무한 points에 대해 정의되기 때문에 single point의 확률은 언제나 0이며, 따라서 continuous probability function에서 확률은 특정 interval에 대해서 측정하고 single point에 대해선 측정하지 않는다고 합니다.</p>\n<p>어찌보면 간단한 것이었지만 자세히 생각해보지는 않아서 헷갈렸던 듯 합니다. 추가적으로, 그러면 어떻게 0이 모여 1이 되는 것 인지까지 궁금해지면서 수학을 당장 근본부터 다시 공부해야하나 싶었지만, 시간은 한정되어 있고 할 일은 많으니 길게 보고 천천히 공부하자는 결론으로 돌아왔습니다 🥲</p>\n<h5 id=\"🧩-ml-library-2\" style=\"position:relative;\"><a href=\"#%F0%9F%A7%A9-ml-library-2\" aria-label=\"🧩 ml library 2 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🧩 ML library</h5>\n<p><em>2021.12.08</em></p>\n<p>PyTorch에 특정 weight만 freeze하는 기능이 구현되어 있는지 살펴보았습니다.</p>\n<p>Layer 단위로 freezing 하는 경우에는 <code class=\"language-text\">required_grad=False</code>를 사용해서 구현했었는데, layer 내 특정 weight만 골라서 freeze하는 기능은 따로 본 적이 없는 것 같아 찾아보다가 <a href=\"https://discuss.pytorch.org/t/how-do-i-freeze-the-specific-weights-in-a-layer/104722/2\">해당 링크</a>를 읽게 되었습니다. 작성자 분이 설명하기로는 아래와 같은 두 가지 임시방편이 있다고 합니다.</p>\n<ul>\n<li><code class=\"language-text\">.step()</code>를 호출하기 전에 freeze 하고자하는 weight에 대해서 <code class=\"language-text\">grad=0</code> 할당. 다만 momentum, weight decay를 사용하는 optimizer의 경우엔 <code class=\"language-text\">grad=0</code>이더라도 <code class=\"language-text\">.step()</code> 호출 시 weight을 변형하기 때문에 원하는대로 동작하지 않을 수 있음</li>\n<li>Freeze하고 싶은 weight을 미리 copy 해두고 <code class=\"language-text\">.step()</code> 을 호출하여 weight을 업데이트한 뒤에, 복사했던 weight을 업데이트된 weight에 덮어씌우기</li>\n</ul>\n<h5 id=\"-ml--dl-4\" style=\"position:relative;\"><a href=\"#-ml--dl-4\" aria-label=\" ml  dl 4 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🤖 ML &#x26; DL</h5>\n<p><em>2022.01.15</em></p>\n<p><a href=\"https://omoindrot.github.io/triplet-loss\">링크</a><sup id=\"fnref-15\"><a href=\"#fn-15\" class=\"footnote-ref\">15</a></sup>를 참고하여 triplet loss 관련 용어를 숙지하였습니다. </p>\n<ul>\n<li>Easy triplets: <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo separator=\"true\">,</mo><mi>p</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mtext>margin</mtext><mo>&lt;</mo><mi>d</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo separator=\"true\">,</mo><mi>n</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">d(a, p) + \\text{margin} &lt; d(a, n)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8623000000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">margin</span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mclose\">)</span></span></span></span></li>\n<li>Hard triplets: <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo separator=\"true\">,</mo><mi>n</mi><mo stretchy=\"false\">)</mo><mo>&lt;</mo><mi>d</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo separator=\"true\">,</mo><mi>p</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">d(a,n) &lt; d(a, p)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mclose\">)</span></span></span></span></li>\n<li>Semi-hard triplets: <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo separator=\"true\">,</mo><mi>p</mi><mo stretchy=\"false\">)</mo><mo>&lt;</mo><mi>d</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo separator=\"true\">,</mo><mi>n</mi><mo stretchy=\"false\">)</mo><mo>&lt;</mo><mi>d</mi><mo stretchy=\"false\">(</mo><mi>a</mi><mo separator=\"true\">,</mo><mi>p</mi><mo stretchy=\"false\">)</mo><mo>+</mo><mtext>margin</mtext></mrow><annotation encoding=\"application/x-tex\">d(a, p) &lt; d(a, n) &lt; d(a,p) + \\text{margin}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">n</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">&lt;</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">d</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">a</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.16666666666666666em;\"></span><span class=\"mord mathnormal\">p</span><span class=\"mclose\">)</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8623000000000001em;vertical-align:-0.19444em;\"></span><span class=\"mord text\"><span class=\"mord\">margin</span></span></span></span></span></li>\n</ul>\n<h5 id=\"🧩-ml-library-3\" style=\"position:relative;\"><a href=\"#%F0%9F%A7%A9-ml-library-3\" aria-label=\"🧩 ml library 3 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🧩 ML library</h5>\n<p><em>2022.02.28</em></p>\n<p>Random seed를 고정할 때 가장 먼저 고려하면 좋을 것들을 기록하였습니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">random<span class=\"token punctuation\">.</span>seed<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>seed<span class=\"token punctuation\">)</span>\nnp<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>seed<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>seed<span class=\"token punctuation\">)</span>\ntorch<span class=\"token punctuation\">.</span>manual_seed<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>seed<span class=\"token punctuation\">)</span>\ntorch<span class=\"token punctuation\">.</span>cuda<span class=\"token punctuation\">.</span>manual_seed_all<span class=\"token punctuation\">(</span>args<span class=\"token punctuation\">.</span>seed<span class=\"token punctuation\">)</span></code></pre></div>\n<h5 id=\"-cs-2\" style=\"position:relative;\"><a href=\"#-cs-2\" aria-label=\" cs 2 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👨‍💻 CS</h5>\n<p><em>2022.03.30</em></p>\n<p>친구(<a href=\"https://github.com/jiun0\">@jiun0</a>, <a href=\"https://github.com/bwmelon97\">@bwmelon97</a>)들을 통해 알게 된 네이밍 스타일에 대해서 간단히 기록합니다.</p>\n<ul>\n<li>네이밍 스타일 종류: <code class=\"language-text\">lowerCamelCase</code>, <code class=\"language-text\">UpperCamelCase (PascalCase)</code>, <code class=\"language-text\">snake_case</code>, <code class=\"language-text\">Train_Case</code>, <code class=\"language-text\">spinal_case</code>, <code class=\"language-text\">UPPER_SNAKE_CASE</code>, ...</li>\n<li>자바스크립트는 주로: 변수, 함수, 메서드는 lowerCamelCase / 클래스명은 PascalCase / 상수명은 UPPER<em>SNAKE</em>CASE</li>\n<li>파이썬(PEP8)은 주로: 변수, 함수는 snake_case / 클래스는 CamelCase</li>\n</ul>\n<h5 id=\"-ml--dl-5\" style=\"position:relative;\"><a href=\"#-ml--dl-5\" aria-label=\" ml  dl 5 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>🤖 ML &#x26; DL</h5>\n<p><em>2022.04.10</em></p>\n<p>연구를 하며, 모델 학습의 안정성에 있어서 residual connection이 유용하다는 경험적인 팁을 얻었습니다. ResNet과 같이 모델 구조에서 residual connection을 활용하는 것 뿐만 아니라, 어떤 값을 조심스럽게 바꾸고 싶을 때 residual connection을 가진 구조가 비교적 높은 성능을 보이는 것을 확인하였습니다.</p>\n<p>예를 들어 GNN을 통해 embedding vector를 업데이트하고 싶을 때 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>V</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><mi>G</mi><mo stretchy=\"false\">(</mo><msub><mi>V</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">V_{t+1} = G(V_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.891661em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">G</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>의 형태를 사용하는 것 보다 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>V</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>=</mo><msub><mi>V</mi><mi>t</mi></msub><mo>+</mo><mi>G</mi><mo stretchy=\"false\">(</mo><msub><mi>V</mi><mi>t</mi></msub><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">V_{t+1} = V_t + G(V_t)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.891661em;vertical-align:-0.208331em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.301108em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">1</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.208331em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.83333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">G</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.22222em;\">V</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2805559999999999em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span>의 형태를 사용하는 것이 좋으며, 현재 실험 중인 것 중에서는 few-shot으로 distribution의 mean을 잘 추정해보려는 내용이 있는데, 이 경우에도 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>μ</mi><mo>^</mo></mover><mo>=</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mtext>few-shot</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\hat \\mu = f_\\theta(\\text{few-shot})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\">μ</span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">few-shot</span></span><span class=\"mclose\">)</span></span></span></span> 보다는 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mover accent=\"true\"><mi>μ</mi><mo>^</mo></mover><mo>=</mo><mtext>mean of few-shot</mtext><mo>+</mo><msub><mi>f</mi><mi>θ</mi></msub><mo stretchy=\"false\">(</mo><mtext>few-shot</mtext><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\hat \\mu = \\text{mean of few-shot} +  f_\\theta(\\text{few-shot})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8888799999999999em;vertical-align:-0.19444em;\"></span><span class=\"mord accent\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.69444em;\"><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"mord mathnormal\">μ</span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"accent-body\" style=\"left:-0.22222em;\"><span class=\"mord\">^</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.19444em;\"><span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2777777777777778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.77777em;vertical-align:-0.08333em;\"></span><span class=\"mord text\"><span class=\"mord\">mean of few-shot</span></span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10764em;\">f</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.33610799999999996em;\"><span style=\"top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">θ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord text\"><span class=\"mord\">few-shot</span></span><span class=\"mclose\">)</span></span></span></span> 형태에서 더 좋은 결과를 얻었습니다.</p>\n<p>아무래도 일반적으로 parameter가 0에 가까운 가우시안으로 초기화되기 때문에, residual connection을 사용한 경우에 초기 loss가 더 작아져 비교적 학습이 안정적인 것이 아닐까 싶습니다. (<em>정말로 그런 것인지 찾아보고 내용 추가하기</em>)</p>\n<h5 id=\"-cs-3\" style=\"position:relative;\"><a href=\"#-cs-3\" aria-label=\" cs 3 permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>👨‍💻 CS</h5>\n<p><em>2022.05.11</em></p>\n<ul>\n<li>인터프리터<sup id=\"fnref-18\"><a href=\"#fn-18\" class=\"footnote-ref\">18</a></sup> 언어: Python과 같이, 프로그래밍 언어의 소스 코드를 바로 실행. 빌드 시간이 없지만, runtime에서는 컴파일 언어에 비해 속도가 느림</li>\n<li>컴파일 언어<sup id=\"fnref-19\"><a href=\"#fn-19\" class=\"footnote-ref\">19</a></sup>: C/C++과 같이, 특정 프로그래밍 언어로 쓰여 있는 문서를 다른 프로그래밍 언어(혹은 기계어)로 번역하여 실행. 빌드 시간이 소요되지만, runtime에서 빠르게 실행 가능. 원래의 문서를 소스 코드(혹은 원시 코드)라고 부르고, 출력된 문서를 목적 코드라고 부름. 목적 코드는 주로 하드웨어가 처리하기에 용이한 형태로 출력되지만 사람이 읽을 수 있는 문서 파일이나 그림 파일 등으로 옮기는 경우도 있음</li>\n<li>현대에 들어 많은 인터프리터가 JIT(just-in-time) 컴파일 등의 기술로 실시간 컴파일을 수행하므로, 컴파일러와 인터프리터 사이의 기술적 구분은 사라져 가는 추세. Java가 JIT 컴파일을 지원하기 때문에 컴파일 언어인 동시에 인터프리터 언어라고 할 수 있음.</li>\n</ul>\n<h3 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h3>\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\">\n<p>Wikipedia contributors. (2021, April 12). Moment (mathematics). In Wikipedia, The Free Encyclopedia. Retrieved 12:08, May 24, 2021, from <a href=\"https://en.wikipedia.org/w/index.php?title=Moment_(mathematics)&#x26;oldid=1017468752\">https://en.wikipedia.org/w/index.php?title=Moment_(mathematics)&#x26;oldid=1017468752</a></p>\n<a href=\"#fnref-1\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-2\">\n<p>API. (2021년 3월 2일). 위키백과, . 04:58, 2021년 5월 24일에 확인 <a href=\"https://ko.wikipedia.org/w/index.php?title=API&#x26;oldid=28891731\">https://ko.wikipedia.org/w/index.php?title=API&#x26;oldid=28891731</a> </p>\n<a href=\"#fnref-2\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-3\">\n<p>REST. (2021년 4월 28일). 위키백과, . 04:57, 2021년 5월 24일에 확인 <a href=\"https://ko.wikipedia.org/w/index.php?title=REST&#x26;oldid=29220143\">https://ko.wikipedia.org/w/index.php?title=REST&#x26;oldid=29220143</a></p>\n<a href=\"#fnref-3\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-4\">\n<p>통합 자원 식별자. (2021년 3월 14일). 위키백과, . 05:02, 2021년 5월 24일에 확인 <a href=\"https://ko.wikipedia.org/w/index.php?title=%ED%86%B5%ED%95%A9%EC%9E%90%EC%9B%90%EC%8B%9D%EB%B3%84%EC%9E%90&#x26;oldid=28963926\">https://ko.wikipedia.org/w/index.php?title=%ED%86%B5%ED%95%A9%EC%9E%90%EC%9B%90%EC%8B%9D%EB%B3%84%EC%9E%90&#x26;oldid=28963926</a></p>\n<a href=\"#fnref-4\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn- 5\">\n<p>mutable vs immutable. (2019년 5월 24일). 공학자를 위한 Python, WikiDocs. 2021년 5월 24일에 확인 <a href=\"https://wikidocs.net/32277\">https://wikidocs.net/32277</a></p>\n<a href=\"#fnref-%205\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn- 6\">\n<p>얕은 복사(shallow copy)와 깊은 복사(deep copy). (2018년 3월 13일). 파이썬 - 기본을 갈고 닦자!, WikiDocs. 2021년 5월 24일에 확인 <a href=\"https://wikidocs.net/16038\">https://wikidocs.net/16038</a></p>\n<a href=\"#fnref-%206\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-7\">\n<p>JinWon Lee - PR-317: MLP-Mixer: An all-MLP Architecture for Vision. <a href=\"https://www.youtube.com/watch?v=KQmZlxdnnuY\">https://www.youtube.com/watch?v=KQmZlxdnnuY</a></p>\n<a href=\"#fnref-7\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-8\">\n<p>JoonYoung Yi - Slideshare, Dynamically Expandable Network (DEN). <a href=\"https://www.slideshare.net/ssuser62b35f/180808-dynamically-expandable-network\">https://www.slideshare.net/ssuser62b35f/180808-dynamically-expandable-network</a></p>\n<a href=\"#fnref-8\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-9\">\n<p>플롭스. (2021년 2월 3일). <em>위키백과,</em> . 13:21, 2021년 8월 25일에 확인 <a href=\"https://ko.wikipedia.org/w/index.php?title=%ED%94%8C%EB%A1%AD%EC%8A%A4&#x26;oldid=28682165\">https://ko.wikipedia.org/w/index.php?title=%ED%94%8C%EB%A1%AD%EC%8A%A4&#x26;oldid=28682165</a></p>\n<a href=\"#fnref-9\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-10\">\n<ol start=\"6\">\n<li>모듈. (2021년 9월 30일). Python 3.9.7 문서, <a href=\"https://docs.python.org/ko/3/tutorial/modules.html\">https://docs.python.org/ko/3/tutorial/modules.html</a></li>\n</ol>\n<a href=\"#fnref-10\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-11\">\n<p>모듈성 (프로그래밍). (2019년 4월 16일). 위키백과, . 15:08, 2021년 9월 30일에 확인 <a href=\"https://ko.wikipedia.org/w/index.php?title=%EB%AA%A8%EB%93%88%EC%84%B1_(%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D)&#x26;oldid=24041546\">https://ko.wikipedia.org/w/index.php?title=%EB%AA%A8%EB%93%88%EC%84%B1_(%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%B0%8D)&#x26;oldid=24041546</a></p>\n<a href=\"#fnref-11\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-12\">\n<p>Wikipedia contributors. (2021, August 1). Signed distance function. In <em>Wikipedia, The Free Encyclopedia</em>. Retrieved 00:41, November 14, 2021, from <a href=\"https://en.wikipedia.org/w/index.php?title=Signed_distance_function&#x26;oldid=1036639454\">https://en.wikipedia.org/w/index.php?title=Signed_distance_function&#x26;oldid=1036639454</a></p>\n<a href=\"#fnref-12\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-13\">\n<p>Park, Jeong Joon, et al. \"Deepsdf: Learning continuous signed distance functions for shape representation.\" <em>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>. 2019.</p>\n<a href=\"#fnref-13\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-14\">\n<p>1.3.6.1.What is a Probability Distribution., <em>NIST/SEMATECH e-Handbook of Statistical Methods</em>, <a href=\"http://www.itl.nist.gov/div898/handbook/\">http://www.itl.nist.gov/div898/handbook/</a>, December 2, 2021.</p>\n<a href=\"#fnref-14\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn- 15\">\n<p>Olivier Moindrot. \"Triplet Loss and Online Triplet Mining in TensorFlow\". <a href=\"https://omoindrot.github.io/triplet-loss\">https://omoindrot.github.io/triplet-loss</a>, Mar 19, 2018.</p>\n<a href=\"#fnref-%2015\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn- 16\">\n<p>Wikipedia contributors. (2021, December 22). Web query. In <em>Wikipedia, The Free Encyclopedia</em>. Retrieved 06:04, February 6, 2022, from <a href=\"https://en.wikipedia.org/w/index.php?title=Web_query&#x26;oldid=1061542579\">https://en.wikipedia.org/w/index.php?title=Web_query&#x26;oldid=1061542579</a></p>\n<a href=\"#fnref-%2016\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn- 17\">\n<p>Christina Kopecky. \"What is a database query? SQL and NoSQL queries explained\". <a href=\"https://www.educative.io/blog/what-is-database-query-sql-nosql#what-is\">https://www.educative.io/blog/what-is-database-query-sql-nosql#what-is</a>, Aug 31, 2020.</p>\n<a href=\"#fnref-%2017\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-18\">\n<p>인터프리터. (2022년 3월 3일). <em>위키백과,</em> . 14:47, 2022년 5월 10일에 확인 <a href=\"https://ko.wikipedia.org/w/index.php?title=%EC%9D%B8%ED%84%B0%ED%94%84%EB%A6%AC%ED%84%B0&#x26;oldid=32006110\">https://ko.wikipedia.org/w/index.php?title=%EC%9D%B8%ED%84%B0%ED%94%84%EB%A6%AC%ED%84%B0&#x26;oldid=32006110</a> 에서 찾아볼 수 있음.</p>\n<a href=\"#fnref-18\" class=\"footnote-backref\">↩</a>\n</li>\n<li id=\"fn-19\">\n<p>컴파일러. (2022년 3월 15일). <em>위키백과,</em> . 15:23, 2022년 5월 10일에 확인 <a href=\"https://ko.wikipedia.org/w/index.php?title=%EC%BB%B4%ED%8C%8C%EC%9D%BC%EB%9F%AC&#x26;oldid=32228964\">https://ko.wikipedia.org/w/index.php?title=%EC%BB%B4%ED%8C%8C%EC%9D%BC%EB%9F%AC&#x26;oldid=32228964</a> 에서 찾아볼 수 있음.</p>\n<a href=\"#fnref-19\" class=\"footnote-backref\">↩</a>\n</li>\n</ol>\n</div>","tableOfContents":"<ul>\n<li>\n<ul>\n<li>\n<ul>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A5%A7-python\">🥧 Python</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A7%A9-ml-library\">🧩 ML library</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A7%A9--ml-library\">🧩  ML library</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A7%A9--ml-library-1\">🧩  ML library</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-ml--dl\">🤖 ML &#x26; DL</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A7%A9--ml-library-2\">🧩  ML library</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-ml--dl-1\">🤖 ML &#x26; DL</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-cs\">👨‍💻 CS</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A5%A7-python-1\">🥧 Python</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A5%A7-python-2\">🥧 Python</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-cs-1\">👨‍💻 CS</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A7%A9-ml-library-1\">🧩 ML library</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A5%A7-python-3\">🥧 Python</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-ml--dl-2\">🤖 ML &#x26; DL</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-ml--dl-3\">🤖 ML &#x26; DL</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A7%A9-ml-library-2\">🧩 ML library</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-ml--dl-4\">🤖 ML &#x26; DL</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#%F0%9F%A7%A9-ml-library-3\">🧩 ML library</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-cs-2\">👨‍💻 CS</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-ml--dl-5\">🤖 ML &#x26; DL</a></li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#-cs-3\">👨‍💻 CS</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li><a href=\"/Archive/21-04-25-Today%20I%20Learned/#references\">References</a></li>\n</ul>","frontmatter":{"path":"/cheatsheet/21-04-25/","title":"Today I Learned","category":"Cheat Sheet","date":"2021-04-25"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}