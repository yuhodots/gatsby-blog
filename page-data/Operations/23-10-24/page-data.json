{"componentChunkName":"component---src-templates-post-js","path":"/Operations/23-10-24/","result":{"data":{"markdownRemark":{"html":"<blockquote>\n<p>NVIDIA Triton Inference Server 사용을 위한 기본 개념들과 샘플 코드를 기록합니다. 모든 내용은 NVIDIA Triton Inference Server의 github document를 기반으로 작성되었습니다. </p>\n</blockquote>\n<h3 id=\"ai-inference-pipeline\" style=\"position:relative;\"><a href=\"#ai-inference-pipeline\" aria-label=\"ai inference pipeline permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>AI Inference Pipeline</h3>\n<p>AI inference 파이프라인을 만들고 inference 요청을 주고 받는 과정은 일반적으로 다음과 같습니다.</p>\n<ol>\n<li>AI model 학습 후 model weights 저장 (e.g., onnx/jit model을 cloud/local storage에 저장)</li>\n<li>Inference server 실행: 필요한 모든 model weights을 inference server로 import(copy/download)하고, 각 model을 config에 따라 세팅</li>\n<li>Client에서 inference server로 요청 (gRPC request)</li>\n<li>Inference server에서 내부적으로 pre-processing / model inference / post-processing를 실시 (물론 client에서 전후처리 해도 됨)</li>\n<li>Inference server의 결과를 client로 응답 (response)</li>\n</ol>\n<p>전체 파이프라인 구축을 위해서 data storage, client server, inference server가 기본적으로 필요하며, client와 inference server를 한 번에 실행하기 위해서 docker compose와 같은 멀티 컨테이너 툴을 활용하는 것이 좋습니다.</p>\n<p>위 과정에서 inference server의 역할은 '필요한 AI model 관리', 'config에 따른 model 세팅', 'client의 요청에 대한 적절한 inference 결과 응답'정도로 생각해볼 수 있습니다. NVIDIA Triton에서는 이를 위한 여러 feature 들을 제공하고 있는데, 본 포스팅에서는 해당 feature들의 기본 개념이나 구조에 대한 설명과 간단한 예제 구현까지 수행해보도록 하겠습니다. </p>\n<h3 id=\"model-repository\" style=\"position:relative;\"><a href=\"#model-repository\" aria-label=\"model repository permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model Repository</h3>\n<p><strong>Model repositroy</strong>는, triton이 AI model을 불러오기 위해 필요한 weights, 메타데이터 등의 정보들을 보관하는 곳입니다. Model repository의 경로는 triton server를 시작할 때 <code class=\"language-text\">--model-repository</code> 옵션으로 명시할 수 있으며, model repository는 local에 직접 존재해도 되고 <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#s3\">AWS S3</a>나 <a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/user_guide/model_repository.html#cloud-storage-with-environment-variables\">google cloud service</a> 같은 cloud 상에 존재하는 것도 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"># local file sysytem\ntritonserver --model-repository=/path/to/model/repository ...\n\n# google cloud storage\ntritonserver --model-repository=gs://bucket/path/to/model/repository ...\n\n# private s3 instance\ntritonserver --model-repository=gs://bucket/path/to/model/repository ...</code></pre></div>\n<p>Cloud storage의 여러 credential file을 하나의 json format으로 저장하는 <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#cloud-storage-with-credential-file-beta\">방법</a>도 제공하고 있지만 아직 beta 기능이라서 지켜봐야할 듯 싶습니다. </p>\n<p>Model repository의 폴더 구조는 반드시 NVIDIA에서 제공하고 있는 아래의 레이아웃을 따라야 합니다. </p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;model-repository-path&gt;/\n    &lt;model-name&gt;/\n      [config.pbtxt]\n      [&lt;output-labels-file&gt; ...]\n      &lt;version&gt;/\n        &lt;model-definition-file&gt;\n      &lt;version&gt;/\n        &lt;model-definition-file&gt;\n      ...\n    &lt;model-name&gt;/\n      [config.pbtxt]\n      [&lt;output-labels-file&gt; ...]\n      &lt;version&gt;/\n        &lt;model-definition-file&gt;\n      &lt;version&gt;/\n        &lt;model-definition-file&gt;\n      ...\n    ...</code></pre></div>\n<p><strong>Model name</strong>은 여러 모델을 구분하기 위해 존재하고, <strong>config.pbtxt</strong> 파일은 model configuration을 제공합니다.</p>\n<p><strong>Model Version</strong>은 최소 하나 이상을 가지고 있어야 하며 numerical name을 갖습니다. Numerical name을 갖지 않거나 0으로 시작하는 폴더 명을 가진다면 무시됩니다. Version policy는 <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#version-policy\">이곳</a>에서 확인 가능합니다. 단순히 하나의 version을 가지고 있는 모델이라면 위의 구조 상에서 &#x3C;version>위치의 폴더 명을 그냥 <code class=\"language-text\">1</code>로 이름 붙이는게 무난할 것 같습니다.</p>\n<p><strong>Model File</strong>은 &#x3C;version> 폴더 안에 존재하며, triton backend에서 지원하는 모델의 종류만 가능하고 모델에 따라 조금씩 레이아웃도 달라집니다. <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#model-files\">이곳</a>에서 자세한 내용을 확인 가능하며, 가장 간단한 예시로 TorchScript 모델의 경우에는 아래의 구조를 가집니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">&lt;model-repository-path&gt;/\n    &lt;model-name&gt;/\n      config.pbtxt\n      1/\n        model.pt</code></pre></div>\n<h3 id=\"model-configuration\" style=\"position:relative;\"><a href=\"#model-configuration\" aria-label=\"model configuration permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Model Configuration</h3>\n<p>아래는 NVIDIA에서 제공하는 config.pbtxt의 예시입니다. </p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">name: &quot;text_detection&quot;\nbackend: &quot;onnxruntime&quot;\nmax_batch_size : 256\ninput [\n  {\n    name: &quot;input_images:0&quot;\n    data_type: TYPE_FP32\n    dims: [ -1, -1, -1, 3 ]\n  }\n]\noutput [\n  {\n    name: &quot;feature_fusion/Conv_7/Sigmoid:0&quot;\n    data_type: TYPE_FP32\n    dims: [ -1, -1, -1, 1 ]\n  }\n]\noutput [\n  {\n    name: &quot;feature_fusion/concat_3:0&quot;\n    data_type: TYPE_FP32\n    dims: [ -1, -1, -1, 5 ]\n  }\n]</code></pre></div>\n<ul>\n<li><code class=\"language-text\">name</code>: optional field이고, 모델의 폴더 이름과 매칭되어야 함</li>\n<li><code class=\"language-text\">backend</code>: 모델 실행을 위해 사용되는 backend 정의 (e.g., pytorch, onnxruntime, tensorflow)</li>\n<li><code class=\"language-text\">max_batch_size</code>: 모델이 지원하는 maximum batch size</li>\n<li><code class=\"language-text\">input</code> and <code class=\"language-text\">output</code>: shape, <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#datatypes\">datatype</a>, <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#reshape\">reshaping</a>, <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/ragged_batching.md#ragged-batching\">ragged batches</a> 등</li>\n</ul>\n<p>일반적으로 config.pbtxt 파일이 필요하긴 하나 해당 파일이 optional인 경우도 있습니다. 예를 들어, model configuration 설정을 따로 하지 않더라도  <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#auto-generated-model-configuration\">auto-generated model configurate</a>에 따라서 triton이 <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md#minimal-model-configuration\">minimal model configuration</a>을 만드는 작업을 수행하기 때문입니다. 물론, <code class=\"language-text\">--disable-auto-complete-config</code>  옵션으로 이 과정을 자동으로 수행하지 않도록 설정하는 것도 가능합니다. </p>\n<p>Triton에 의해 생성된 model configuration은 <a href=\"https://github.com/triton-inference-server/server/blob/main/docs/protocol/extension_model_configuration.md#model-configuration-extension\">model configuration endpoint</a>에 요청을 보내 확인 가능합니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">curl localhost:8000/v2/models/&lt;model name&gt;/config</code></pre></div>\n<h3 id=\"python-backend\" style=\"position:relative;\"><a href=\"#python-backend\" aria-label=\"python backend permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Python Backend</h3>\n<p>Trition backend는 model 실행을 위한 구현체들을 의미하며 기본적으로는 PyTorch, TensorFlow, TensorRT, ONNX Runtime 등 딥러닝 프레임워크의 wrapper로 볼 수 있습니다. 특히 딥러닝 프레임워크에 대한 backend 뿐만 아니라 <a href=\"https://github.com/triton-inference-server/python_backend\">python backend</a>도 지원을 해서, 모델 입력에 대한 전처리나 모델 출력에 대한 후처리를 python backend 내에서 수행할 수 있습니다.</p>\n<p>Python backend를 사용하기 위해서는 아래의 구조를 가진 <strong>TritonPythonModel</strong> class를 정의해야 하고, TritonPythonModel 내에는 4개의 함수가 존재합니다. </p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> triton_python_backend_utils <span class=\"token keyword\">as</span> pb_utils\n\n<span class=\"token keyword\">class</span> <span class=\"token class-name\">TritonPythonModel</span><span class=\"token punctuation\">:</span>\n    <span class=\"token decorator annotation punctuation\">@staticmethod</span>\n    <span class=\"token keyword\">def</span> <span class=\"token function\">auto_complete_config</span><span class=\"token punctuation\">(</span>auto_complete_model_config<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n       \t<span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span><span class=\"token punctuation\">.</span>\n        <span class=\"token keyword\">return</span> auto_complete_model_config\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">initialize</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> args<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Initialized...'</span><span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">execute</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> requests<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        responses <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n        <span class=\"token keyword\">for</span> request <span class=\"token keyword\">in</span> requests<span class=\"token punctuation\">:</span>\n            <span class=\"token comment\"># Perform inference on the request and append it to responses</span>\n            <span class=\"token comment\"># list...</span>\n        <span class=\"token keyword\">return</span> responses\n\n    <span class=\"token keyword\">def</span> <span class=\"token function\">finalize</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string\">'Cleaning up...'</span><span class=\"token punctuation\">)</span></code></pre></div>\n<ul>\n<li><code class=\"language-text\">auto_complete_config</code>: <code class=\"language-text\">--disable-auto-complete-config</code> 옵션을 사용하지 않은 경우에 모델을 로드할 때 한 번 호출. set<em>max</em>batch<em>size, set</em>dynamic<em>batching, add</em>input, add_output 프로퍼티를 사용해서 config를 설정하는데에 사용</li>\n<li><code class=\"language-text\">initialize</code>: 모델이 로드되고 난 후에 한 번 호출. 전달되는 <code class=\"language-text\">args</code>는 python dictionary인데 <a href=\"https://github.com/triton-inference-server/python_backend#initialize\">이곳</a>에서 사용 가능한 key를 확인 가능</li>\n<li><code class=\"language-text\">execute</code>: inference request가 올 때 마다 호출되는 함수</li>\n<li><code class=\"language-text\">finalize</code>: 모델을 unload하기 전에 필요한 모든 정리 작업을 수행</li>\n</ul>\n<p>이 중에서 제일 중요한 함수는 <code class=\"language-text\">execute</code>이고 나머지 함수들은 선택적(optional)으로 구현해도 상관이 없습니다. Error handling이나 Logger 등과 같은 추가적인 기능 사용을 위해서는 <code class=\"language-text\">pb_utils</code>을 살펴보시면 좋습니다. 모든 함수를 구현하고 난 뒤에는 해당 파일 명을 반드시 <strong>model.py</strong>로 설정해야 합니다. </p>\n<h3 id=\"other-features\" style=\"position:relative;\"><a href=\"#other-features\" aria-label=\"other features permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Other Features</h3>\n<ul>\n<li><a href=\"https://github.com/triton-inference-server/model_analyzer\">Model Analyzer</a>: 주어진 하드웨어나 문제 상황에 대해 최적의 config를 찾기 것을 돕는 툴</li>\n<li>\n<p><a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md\">Model Management</a>: 모델 load를 위해 아래 세 가지 mode 중 하나를 선택하게 됨</p>\n<ul>\n<li>NONE: model repository의 모든 모델을 로드</li>\n<li>EXPLICIT: <code class=\"language-text\">--load-model</code> 옵션에 명시된 모델만 로드. 모든 모델 로드는 <code class=\"language-text\">--load-model=*</code>. 해당 옵션을 적지 않는 경우엔 아무 모델도 로드되지 않음</li>\n<li>POLL: 처음에는 모든 모델 로드 후에, 모델 변경에 따라 주기적으로 모델 다시 로드하거나 내림. 제대로된 sync 처리 따로 없기 때문에 배포 환경에서 사용하지 말기</li>\n</ul>\n</li>\n<li><a href=\"https://github.com/triton-inference-server/server/blob/main/docs/user_guide/metrics.md\">Metrics</a>, Dynamic batching(여러 요청 배치 처리), Ensemble model(여러 모델의 종합), Concurrent Model Execution(같은 모델 병렬 실행) 등 기능 존재</li>\n</ul>\n<h3 id=\"example\" style=\"position:relative;\"><a href=\"#example\" aria-label=\"example permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Example</h3>\n<p>예제로는 공식 예제에 존재하는 Falcon-7B model을 사용해보겠습니다. 모델이 너무 큰 경우에는 다른 모델로 실험해보셔도 좋을 것 같습니다. 전체 코드는 <a href=\"https://github.com/yuhodots/triton\">개인 저장소</a>에 업로드 해두었습니다. </p>\n<p>해당 예제에서는 모델을 직접 학습시키고 export 할 필요 없이, Python BackEnd 기반으로 huggingface 모델의 weight을 다운 받아서 사용합니다.</p>\n<p>사실 내용은 별 게 없습니다. 이미 triton server와 client container가 만들어지도록 docekr compose 파일을 만들어 두었고, triton server 내 falcon7b 모델 또한 initialize 과정에서 필요한 weight이나 토크나이저를 모두 알아서 불러옵니다. 따라서 아래 명령어만 실행시켜주시면 됩니다.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\">docker compose up</code></pre></div>\n<p>서버 잘 실행되었다면, client container 내에서 curl을 통해 아래의 요청을 보내봅시다.</p>\n<div class=\"gatsby-highlight\" data-language=\"shell\"><pre class=\"language-shell\"><code class=\"language-shell\"><span class=\"token function\">curl</span> -X POST tritonserver:8000/v2/models/falcon7b/infer -d <span class=\"token string\">'{\"inputs\": [{\"name\":\"text_input\",\"datatype\":\"BYTES\",\"shape\":[1],\"data\":[\"How can you be\"]}]}'</span></code></pre></div>\n<p>요청에 대해서 아래와 같이 응답이 잘 오는 것을 확인하면 끝입니다! </p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">{\n    &quot;model_name&quot;: &quot;falcon7b&quot;,\n    &quot;model_version&quot;: &quot;1&quot;,\n    &quot;outputs&quot;: [\n        {\n            &quot;name&quot;: &quot;text_output&quot;,\n            &quot;datatype&quot;: &quot;BYTES&quot;,\n            &quot;shape&quot;: [1],\n            &quot;data&quot;: [&quot;How can you be sure that you are getting the best deal on your car&quot;]\n        }\n    ]\n}</code></pre></div>\n<h3 id=\"references\" style=\"position:relative;\"><a href=\"#references\" aria-label=\"references permalink\" class=\"anchor-header before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>References</h3>\n<p>좋은 깃헙 예제들을 발견하여 아래에 공유합니다.</p>\n<ul>\n<li><a href=\"https://github.com/Curt-Park/triton-inference-server-practice\">https://github.com/Curt-Park/triton-inference-server-practice</a></li>\n<li><a href=\"https://github.com/fegler/triton_server_example\">https://github.com/fegler/triton_server_example</a></li>\n<li><a href=\"https://github.com/triton-inference-server/tutorials\">https://github.com/triton-inference-server/tutorials</a></li>\n</ul>\n<p>공부를 위한 문서들을 아래에 공유합니다.</p>\n<ul>\n<li><a href=\"https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html\">https://docs.nvidia.com/deeplearning/triton-inference-server/user-guide/docs/getting_started/quickstart.html</a></li>\n<li><a href=\"https://developer.nvidia.com/blog/nvidia-serves-deep-learning-inference/\">https://developer.nvidia.com/blog/nvidia-serves-deep-learning-inference/</a></li>\n</ul>\n<p>더 쉬운 사용을 위한 wrapper 오픈소스도 존재하네요!</p>\n<ul>\n<li><a href=\"https://blog.rtzr.ai/tritony-tiny-configuration-for-triton-inference-server/\">https://blog.rtzr.ai/tritony-tiny-configuration-for-triton-inference-server/</a></li>\n</ul>","tableOfContents":"<ul>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#ai-inference-pipeline\">AI Inference Pipeline</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#model-repository\">Model Repository</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#model-configuration\">Model Configuration</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#python-backend\">Python Backend</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#other-features\">Other Features</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#example\">Example</a></li>\n<li><a href=\"/MachineLearning/23-10-24-NVIDIA%20triton/#references\">References</a></li>\n</ul>","frontmatter":{"path":"/Operations/23-10-24/","title":"NVIDIA Triton Inference Server","category":"Operations","date":"2023-10-24"}}},"pageContext":{}},"staticQueryHashes":["2390655019","256249292","63159454"]}